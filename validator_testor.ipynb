{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "from cortexsubsetloader import CortexSubsetLoader\n",
    "\n",
    "# model i is always pre-finetune, and thus gets the buffer\n",
    "# model j is post our finetune\n",
    "\n",
    "def iswin(loss_i, loss_j, epsilon=0.01):\n",
    "    # Adjust loss based on timestamp and pretrain epsilon\n",
    "    loss_i = (1 - epsilon) * loss_i\n",
    "    return loss_i > loss_j\n",
    "\n",
    "def compute_losses(model, batches, device):\n",
    "    # Iterate over each page and corresponding batches\n",
    "    losses = []\n",
    "    print()\n",
    "    with torch.inference_mode():\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        steps = 0\n",
    "        for inputs, prompt_len in batches:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = inputs.clone()\n",
    "            labels[:, :prompt_len] = -100 # Only calculate loss on response\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs.loss.item()  # Extract scalar loss value\n",
    "            losses.append(loss)\n",
    "            steps += 1\n",
    "            if steps % (len(batches) // 16) == 0:\n",
    "                print(\".\", end=\"\")\n",
    "    return losses\n",
    "\n",
    "\n",
    "def norm_model_weights(model):\n",
    "    last_q = None\n",
    "    lqb = None\n",
    "    lqkm = None\n",
    "    last_v = None\n",
    "    lvb = None\n",
    "    lvom = None\n",
    "    last_up = None\n",
    "    bias = False\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"q_proj\" in name:\n",
    "            if \"bias\" in name:\n",
    "                bias = True\n",
    "                lqb = param\n",
    "            else:\n",
    "                last_q = param\n",
    "        if \"k_proj\" in name:\n",
    "            if \"bias\" in name and lqkm is not None:\n",
    "                param.data = param.data * lqkm\n",
    "            else:\n",
    "                mult = torch.sqrt(torch.mean(torch.abs(last_q.data), dim=1, keepdim=True).transpose(0, 1) / \n",
    "                                torch.mean(torch.abs(param.data), dim=0, keepdim=True))\n",
    "                mult = torch.mean(mult)\n",
    "                last_q.data = last_q.data / mult#.transpose(0, 1)\n",
    "                if bias:\n",
    "                    lqb.data = lqb.data / mult#.transpose(0, 1).flatten()\n",
    "                param.data = param.data * mult # \n",
    "                lqkm = mult\n",
    "                \n",
    "        if \"v_proj\" in name:\n",
    "            if \"bias\" in name:\n",
    "                lvb = param\n",
    "            else:\n",
    "                last_v = param\n",
    "        if \"o_proj\" in name:\n",
    "            if \"bias\" in name:\n",
    "                param.data = param.data * lvom\n",
    "            else:\n",
    "                mult = torch.sqrt(torch.mean(torch.abs(last_v.data), dim=1, keepdim=True).transpose(0, 1).repeat(1, \n",
    "                                                                            int(param.data.shape[0] / last_v.data.shape[0])) / \n",
    "                                torch.mean(torch.abs(param.data), dim=0, keepdim=True))\n",
    "                last_v.data = last_v.data / mult.transpose(0, 1)[:last_v.data.shape[0]]\n",
    "                if bias:\n",
    "                    lvb.data = lvb.data / mult.transpose(0, 1).flatten()\n",
    "                param.data = param.data * mult\n",
    "                lvom = mult\n",
    "\n",
    "        if \"up_proj\" in name:\n",
    "            last_up = param\n",
    "        if \"down_proj\" in name:\n",
    "            # print(last_up.data.shape, param.data.shape)\n",
    "            mult = torch.sqrt(torch.mean(torch.abs(last_up.data), dim=1, keepdim=True).transpose(0, 1) / \n",
    "                            torch.mean(torch.abs(param.data), dim=0, keepdim=True))\n",
    "            last_up.data = last_up.data / mult.transpose(0, 1)\n",
    "            param.data = param.data * mult\n",
    "            # print(mult, mult.shape)\n",
    "    return model\n",
    "\n",
    "\n",
    "def validate_improvement(test_model_name, lora_name, lora=True, rescale_for_lora=False, n_runs=10, gpu=True, samples=400, dedup=True,\n",
    "                         mistral=False):\n",
    "    if type(test_model_name) == str and type(lora_name) == str:\n",
    "        print(\"Testing\", test_model_name, \"against\", lora_name)\n",
    "    win_count = 0; prev_data = []\n",
    "    win_count_0eps = 0\n",
    "    avg_loss_diff = 0\n",
    "    avg_loss_i = 0\n",
    "    avg_loss_j = 0\n",
    "\n",
    "    n_runs_done = 0\n",
    "    for i in range(n_runs):\n",
    "        # try:\n",
    "            if type(test_model_name) is str:\n",
    "                test_model = AutoModelForCausalLM.from_pretrained(test_model_name, **params, cache_dir=\"Models\")\n",
    "            else:\n",
    "                test_model = test_model_name\n",
    "            for name, param in test_model.named_parameters():\n",
    "                param.requires_grad = False\n",
    "            if not mistral:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-2-zephyr-1_6b\", cache_dir=\"Models\")\n",
    "            else:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", cache_dir=\"Models\")\n",
    "\n",
    "            cortex_data = CortexSubsetLoader(\n",
    "                latest=True, running=True,\n",
    "                random_seed=random.randint(0, sys.maxsize),\n",
    "                max_samples=samples, page_size=samples,\n",
    "                steps=1,\n",
    "            )\n",
    "            \n",
    "            batches = cortex_data.tokenize(tokenizer)\n",
    "            dedup_batches = []\n",
    "            for batch in batches: \n",
    "                any_same = False\n",
    "                for prev in prev_data:\n",
    "                    if batch[0].shape == prev[0].shape:\n",
    "                        if (batch[0][0] == prev[0][0]).all():\n",
    "                            any_same = True\n",
    "                            break\n",
    "                if not any_same:\n",
    "                    dedup_batches.append(batch)\n",
    "            if len(dedup_batches) < len(batches):\n",
    "                if dedup:\n",
    "                    batches = dedup_batches\n",
    "                    print(\"Removed\", len(batches) - len(dedup_batches), \"duplicate batches\")\n",
    "                else:\n",
    "                    print(\"Found\", len(batches) - len(dedup_batches), \"duplicate batches, not deduplicating\")\n",
    "            elif len(dedup_batches) == len(batches):\n",
    "                print(\"No duplicates found\")\n",
    "\n",
    "            device = \"cuda:0\" if gpu else \"cpu\"\n",
    "            base_loss = compute_losses(test_model, batches, device)\n",
    "\n",
    "            if lora:\n",
    "                if rescale_for_lora:\n",
    "                    test_model = norm_model_weights(test_model)\n",
    "                lora_model = PeftModel.from_pretrained(test_model, lora_name, adapter_name=lora_name)\n",
    "                lora_model = lora_model.merge_and_unload()\n",
    "            else:\n",
    "                test_model = None; gc.collect(); torch.cuda.empty_cache()\n",
    "                if type(lora_name) is str:\n",
    "                    lora_model = AutoModelForCausalLM.from_pretrained(lora_name, **params, cache_dir=\"Models\")\n",
    "                else:\n",
    "                    lora_model = lora_name\n",
    "                for name, param in lora_model.named_parameters():\n",
    "                    param.requires_grad = False\n",
    "            lora_loss = compute_losses(lora_model, batches, device)\n",
    "\n",
    "            per_loss_win = np.mean([iswin(base, lora) for base, lora in zip(base_loss, lora_loss)])\n",
    "            per_loss_win_b = np.mean([iswin(base, lora, epsilon=0.0) for base, lora in zip(base_loss, lora_loss)])\n",
    "            win_count += per_loss_win\n",
    "            win_count_0eps += per_loss_win_b\n",
    "\n",
    "            avg_loss_i += np.mean(base_loss)\n",
    "            avg_loss_j += np.mean(lora_loss) \n",
    "            loss_diff = np.mean(np.array(lora_loss) - np.array(base_loss))\n",
    "            avg_loss_diff += loss_diff\n",
    "\n",
    "            print(\"Base: \", np.mean(base_loss), \"\\tNew: \", np.mean(lora_loss), \"\\tDiff: \", loss_diff)\n",
    "            print(\"\\tWinRate: \", per_loss_win, \"\\t0Eps WinRate: \", per_loss_win_b)\n",
    "\n",
    "            n_runs_done += 1\n",
    "            prev_data = prev_data + batches\n",
    "            del test_model, lora_model; gc.collect(); torch.cuda.empty_cache()\n",
    "        # except:\n",
    "        #     continue\n",
    "\n",
    "    avg_loss_diff /= n_runs_done\n",
    "    avg_loss_i /= n_runs_done\n",
    "    avg_loss_j /= n_runs_done\n",
    "\n",
    "    print()\n",
    "    print(\"Avg Loss Base: \", avg_loss_i)\n",
    "    print(\"Avg Loss New: \", avg_loss_j)\n",
    "    print(\"Avg Loss Diff: \", avg_loss_diff)\n",
    "    print(\"Final Win Rate: \", win_count / n_runs_done)\n",
    "    print(\"Final 0Eps Win Rate: \", win_count_0eps / n_runs_done)\n",
    "\n",
    "params = {\n",
    "    'low_cpu_mem_usage': True,\n",
    "    'trust_remote_code': False,\n",
    "    'torch_dtype': torch.bfloat16,\n",
    "    'use_safetensors': True,\n",
    "    'attn_implementation': \"flash_attention_2\"\n",
    "}\n",
    "\n",
    "def print_model_params(model, norm=False):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model, **params, cache_dir=\"Models\")\n",
    "    if norm:\n",
    "        model = norm_model_weights(model)\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param)\n",
    "    del model; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "def check_matching_weights(model0, model1):\n",
    "    model0 = AutoModelForCausalLM.from_pretrained(model0, **params, cache_dir=\"Models\")\n",
    "    model1 = AutoModelForCausalLM.from_pretrained(model1, **params, cache_dir=\"Models\")\n",
    "    mismatch_diffs = []\n",
    "    any_mismatch = False\n",
    "    for (name0, param0), (name1, param1) in zip(model0.named_parameters(), model1.named_parameters()):\n",
    "        if not (param0.data == param1.data).all():\n",
    "            any_mismatch = True\n",
    "            diff = torch.sum(torch.abs(param0.data - param1.data)).item()\n",
    "            mismatch_diffs.append(diff)\n",
    "            print(\"Mismatched weights\", name0, name1, diff)\n",
    "    if not any_mismatch:\n",
    "        print(\"No mismatched weights\")\n",
    "    else:\n",
    "        print(\"Mean abs mismatched\", np.mean(mismatch_diffs), \"Std abs mismatched\", np.std(mismatch_diffs))\n",
    "    del model0, model1; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "def validate_parameters(base_model, eps_soft=500, eps_soft_percent_threshold=0.3, eps_hard=2500, norm=False):\n",
    "    if type(base_model) is str:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(base_model, **params, cache_dir=\"Models\")\n",
    "    else:\n",
    "        base_model = base_model\n",
    "    if norm:\n",
    "        base_model = norm_model_weights(base_model)\n",
    "\n",
    "    exceed_counts = {'q_proj': 0, 'k_proj': 0, 'v_proj': 0, 'o_proj': 0, 'up_proj': 0, 'down_proj': 0}\n",
    "    avg_norms = {'q_proj': 0.0, 'k_proj': 0.0, 'v_proj': 0.0, 'o_proj': 0.0, 'up_proj': 0.0, 'down_proj': 0.0}\n",
    "    total_counts = {'q_proj': 0, 'k_proj': 0, 'v_proj': 0, 'o_proj': 0, 'up_proj': 0, 'down_proj': 0}\n",
    "\n",
    "    for layer in base_model.model.layers:\n",
    "        for proj in ['k_proj', 'v_proj', 'q_proj', 'o_proj']:\n",
    "            weight_norm = getattr(layer.self_attn, proj).weight.norm().item()\n",
    "            if weight_norm > eps_hard:\n",
    "                return False\n",
    "            elif weight_norm > eps_soft:\n",
    "                exceed_counts[proj] += 1\n",
    "            total_counts[proj] += 1\n",
    "            avg_norms[proj] += weight_norm\n",
    "\n",
    "        # up_proj and down_proj are in the mlp layer\n",
    "        for proj in ['up_proj', 'down_proj']:\n",
    "            weight_norm = getattr(layer.mlp, proj).weight.norm().item()\n",
    "            if weight_norm > eps_hard:\n",
    "                return False\n",
    "            elif weight_norm > eps_soft:\n",
    "                exceed_counts[proj] += 1\n",
    "            total_counts[proj] += 1\n",
    "            avg_norms[proj] += weight_norm\n",
    "\n",
    "    # Calculating and printing percentages\n",
    "    percentages = [exceed_counts[proj] / total_counts[proj] if total_counts[proj] > 0 else 0.0 for proj in exceed_counts]\n",
    "    for key, value in total_counts.items():\n",
    "        avg_norms[key] = avg_norms[key] / value\n",
    "    print(percentages, avg_norms)\n",
    "    del base_model; gc.collect(); torch.cuda.empty_cache()\n",
    "    return np.mean(np.array(percentages)[[1, 2, 4]]) <= eps_soft_percent_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing tomaszki/stablelm-1 against MesozoicMetallurgist/zeta-Ladinian\n",
      "No duplicates found\n",
      "\n",
      "................"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032a39cae7ec46eba855f71f7f30473c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/744 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8175d4ea09243489daca1a889c35042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.29G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1ed7700b9e4bc4b39ccbddf833771a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "................Base:  0.7294731140136719 \tNew:  0.6798674265543619 \tDiff:  -0.0496056874593099\n",
      "\tWinRate:  0.8138020833333334 \t0Eps WinRate:  0.8515625\n",
      "\n",
      "Avg Loss Base:  0.7294731140136719\n",
      "Avg Loss New:  0.6798674265543619\n",
      "Avg Loss Diff:  -0.0496056874593099\n",
      "Final Win Rate:  0.8138020833333334\n",
      "Final 0Eps Win Rate:  0.8515625\n"
     ]
    }
   ],
   "source": [
    "validate_improvement(\"tomaszki/stablelm-1\", \"MesozoicMetallurgist/zeta-Ladinian\", False, n_runs=1, samples=768, dedup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MesozoicMetallurgist/zeta-Anisian against 0x0dad0/beta_s03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found\n",
      "\n",
      "................\n",
      "................Base:  0.7434326807657877 \tNew:  0.7078625361124674 \tDiff:  -0.03557014465332031\n",
      "\tWinRate:  0.5989583333333334 \t0Eps WinRate:  0.69140625\n",
      "\n",
      "Avg Loss Base:  0.7434326807657877\n",
      "Avg Loss New:  0.7078625361124674\n",
      "Avg Loss Diff:  -0.03557014465332031\n",
      "Final Win Rate:  0.5989583333333334\n",
      "Final 0Eps Win Rate:  0.69140625\n"
     ]
    }
   ],
   "source": [
    "validate_improvement(\"MesozoicMetallurgist/zeta-Anisian\", \"0x0dad0/beta_s03\", False, n_runs=1, samples=768, dedup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MesozoicMetallurgist/zeta-Anisian against tomaszki/stablelm-0\n",
      "No duplicates found\n",
      "\n",
      "................"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c8dc4fa3ab4ae3bc7eca2b5e42ba5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/719 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfec8ebc656438aae29809b6a0790b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc950ec505444564a15d945b788ffe1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35c45ce5e3d48e884c168a7cca348ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00001.safetensors:   0%|          | 0.00/3.29G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "................Base:  0.6679238080978394 \tNew:  0.6688983043034872 \tDiff:  0.0009744962056477865\n",
      "\tWinRate:  0.13671875 \t0Eps WinRate:  0.265625\n",
      "\n",
      "Avg Loss Base:  0.6679238080978394\n",
      "Avg Loss New:  0.6688983043034872\n",
      "Avg Loss Diff:  0.0009744962056477865\n",
      "Final Win Rate:  0.13671875\n",
      "Final 0Eps Win Rate:  0.265625\n"
     ]
    }
   ],
   "source": [
    "validate_improvement(\"MesozoicMetallurgist/zeta-Anisian\", \"tomaszki/stablelm-0\", False, n_runs=1, samples=768, dedup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MesozoicMetallurgist/zeta-Induan against MesozoicMetallurgist/zeta-Anisian\n",
      "No duplicates found\n",
      "\n",
      "................\n",
      "................Base:  0.749762217203776 \tNew:  0.6579113006591797 \tDiff:  -0.09185091654459636\n",
      "\tWinRate:  0.9375 \t0Eps WinRate:  0.9453125\n",
      "\n",
      "Avg Loss Base:  0.749762217203776\n",
      "Avg Loss New:  0.6579113006591797\n",
      "Avg Loss Diff:  -0.09185091654459636\n",
      "Final Win Rate:  0.9375\n",
      "Final 0Eps Win Rate:  0.9453125\n"
     ]
    }
   ],
   "source": [
    "validate_improvement(\"MesozoicMetallurgist/zeta-Induan\", \"MesozoicMetallurgist/zeta-Anisian\", False, n_runs=1, samples=768, dedup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing coffiee/s11 against MesozoicMetallurgist/zeta-Anisian\n",
      "No duplicates found\n",
      "\n",
      "................\n",
      "................Base:  0.6664326985677084 \tNew:  0.6242529551188151 \tDiff:  -0.04217974344889323\n",
      "\tWinRate:  0.9921875 \t0Eps WinRate:  0.9986979166666666\n",
      "\n",
      "Avg Loss Base:  0.6664326985677084\n",
      "Avg Loss New:  0.6242529551188151\n",
      "Avg Loss Diff:  -0.04217974344889323\n",
      "Final Win Rate:  0.9921875\n",
      "Final 0Eps Win Rate:  0.9986979166666666\n"
     ]
    }
   ],
   "source": [
    "validate_improvement(\"coffiee/s11\", \"MesozoicMetallurgist/zeta-Anisian\", False, n_runs=1, samples=768, dedup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MesozoicMetallurgist/zeta-Anisian against coffiee/s11\n",
      "No duplicates found\n",
      "\n",
      "................\n",
      "................Base:  0.8217595418294271 \tNew:  0.8159128824869791 \tDiff:  -0.005846659342447917\n",
      "\tWinRate:  0.5963541666666666 \t0Eps WinRate:  0.6197916666666666\n",
      "\n",
      "Avg Loss Base:  0.8217595418294271\n",
      "Avg Loss New:  0.8159128824869791\n",
      "Avg Loss Diff:  -0.005846659342447917\n",
      "Final Win Rate:  0.5963541666666666\n",
      "Final 0Eps Win Rate:  0.6197916666666666\n"
     ]
    }
   ],
   "source": [
    "validate_improvement(\"MesozoicMetallurgist/zeta-Anisian\", \"coffiee/s11\", False, n_runs=1, samples=768, dedup=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
