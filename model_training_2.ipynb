{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from peft import (\n",
    "    # AdaLoraConfig,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel,\n",
    "    LoHaConfig,\n",
    ")\n",
    "\n",
    "from cortexsubsetloader import tokenize\n",
    "from pytorch_optimizer import Ranger21, Lamb, DAdaptLion, SAM, DAdaptAdam, LOMO, SophiaH\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(train, eval):\n",
    "    evalf = []\n",
    "    for f in eval:\n",
    "        if f not in train:\n",
    "            evalf.append(f)\n",
    "    if len(evalf) < len(eval): print(f\"Removed {len(eval)-len(evalf)} duplicates from eval\")\n",
    "    else: print(\"No duplicates found in eval\")\n",
    "    return evalf\n",
    "\n",
    "\n",
    "def data_collator(features):\n",
    "    batches = []\n",
    "    for feature in features:\n",
    "        inputs, prompt_len = feature\n",
    "        data = [inputs]\n",
    "        b_labels = inputs.clone()\n",
    "        b_labels[:, :prompt_len] = -100\n",
    "        labels = [b_labels]\n",
    "            \n",
    "        batch = {}\n",
    "        batch['input_ids'] = torch.concat(data)\n",
    "        batch['labels'] = torch.concat(labels)\n",
    "        batches.append(batch)\n",
    "    return batches\n",
    "\n",
    "\n",
    "def get_data(train_name, eval_name, tokenizer, train_subset=None, eval_subset=None, shuffle=True, extend_train_length=0):\n",
    "    if type(train_name) == str:\n",
    "        with open(train_name, \"r\") as f:\n",
    "            train_data = np.array(json.load(f))\n",
    "    else: # list of str\n",
    "        train_data = []\n",
    "        for name in train_name:\n",
    "            with open(name, \"r\") as f:\n",
    "                train_data.extend(json.load(f))\n",
    "        train_data = np.array(train_data)\n",
    "    with open(eval_name, \"r\") as f:\n",
    "        eval_data = np.array(json.load(f))\n",
    "\n",
    "    if train_subset is not None:\n",
    "        train_data = train_data[train_subset]\n",
    "    if eval_subset is not None:\n",
    "        eval_data = eval_data[eval_subset]\n",
    "    \n",
    "    eval_data = filter_data(train_data, eval_data)\n",
    "\n",
    "    if shuffle:\n",
    "        p = np.random.permutation(len(train_data))\n",
    "        train_data = train_data[p]\n",
    "\n",
    "    train_data = tokenize(tokenizer, train_data, 2048+extend_train_length)\n",
    "    eval_data = tokenize(tokenizer, eval_data, 2048)\n",
    "\n",
    "    train_data = data_collator(train_data)\n",
    "    eval_data = data_collator(eval_data)\n",
    "\n",
    "    return train_data, eval_data\n",
    "\n",
    "\n",
    "true_eps = 0.01\n",
    "# initial_loss_eps = 0.0001\n",
    "intermed_check_step_split = 8\n",
    "\n",
    "def evaluate(model, eval_d, return_to_cpu=False, return_stats=False, print_stats=True, \n",
    "             base_model=None, precompute_base_loss=True, device=\"cuda\", instruction_finetuning=True):\n",
    "    print(\"Evaluating\", end=\" \")\n",
    "    model = model.to(\"cuda\")\n",
    "    eval_base_loss = 0\n",
    "    lora_diff = 0\n",
    "    eval_loss = 0\n",
    "    head_to_head = 0\n",
    "    eps0_head_to_head = 0\n",
    "    overshoot = 0\n",
    "    model.eval()\n",
    "    steps_so_far = 1\n",
    "\n",
    "    LORA = True\n",
    "    if base_model is not None:\n",
    "        LORA = False\n",
    "\n",
    "    precomputed_base_losses = []\n",
    "    if precompute_base_loss:\n",
    "\n",
    "        if LORA:\n",
    "            model.disable_adapter_layers()\n",
    "        else:\n",
    "            base_model = base_model.to(device)\n",
    "\n",
    "        for batch in eval_d:\n",
    "            if instruction_finetuning:\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "            else:\n",
    "                inputs = batch.to(device)\n",
    "                labels = inputs.clone()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if LORA:\n",
    "                    base_outputs_loss = model(inputs, labels=labels).loss\n",
    "                else:\n",
    "                    base_outputs_loss = base_model(inputs, labels=labels).loss\n",
    "                precomputed_base_losses.append(base_outputs_loss)\n",
    "        \n",
    "        if LORA:\n",
    "            model.enable_adapter_layers()\n",
    "        else:\n",
    "            base_model = base_model.to(\"cpu\")\n",
    "    else:\n",
    "        base_model = base_model.to(device)\n",
    "\n",
    "\n",
    "    for batch in eval_d:\n",
    "        if instruction_finetuning:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "        else:\n",
    "            inputs = batch.to(device)\n",
    "            labels = inputs.clone()\n",
    "        with torch.no_grad():\n",
    "            if precompute_base_loss:\n",
    "                base_outputs_loss = precomputed_base_losses.pop(0)\n",
    "            else:\n",
    "                if LORA:\n",
    "                    model.disable_adapter_layers()\n",
    "                    base_outputs_loss = model(inputs, labels=labels).loss\n",
    "                    model.enable_adapter_layers()\n",
    "                else:\n",
    "                    base_outputs_loss = base_model(inputs, labels=labels).loss\n",
    "            outputs_loss = model(inputs, labels=labels).loss\n",
    "\n",
    "            base_loss = base_outputs_loss\n",
    "            partial_loss = torch.nn.functional.relu(outputs_loss - (base_loss * (1.0 - true_eps)))\n",
    "            overshoot_penalty = torch.nn.functional.relu(-(outputs_loss - (base_loss * (1.0 - true_eps))))\n",
    "            loss = partial_loss / base_loss\n",
    "\n",
    "            eval_loss += loss.item() / len(eval_d)\n",
    "            eval_base_loss += base_outputs_loss.item() / len(eval_d)\n",
    "            lora_diff += (outputs_loss.item() - base_outputs_loss.item()) / len(eval_d)\n",
    "            head_to_head += 100.0 / len(eval_d) if outputs_loss < (base_outputs_loss * (1.0 - true_eps)) else 0.0\n",
    "            head_to_head += 50.0 / len(eval_d) if outputs_loss == (base_outputs_loss * (1.0 - true_eps)) else 0.0\n",
    "            eps0_head_to_head += 100.0 / len(eval_d) if outputs_loss < base_outputs_loss else 0.0\n",
    "            eps0_head_to_head += 50.0 / len(eval_d) if outputs_loss == base_outputs_loss else 0.0\n",
    "            overshoot += overshoot_penalty.item() / len(eval_d)\n",
    "\n",
    "        if steps_so_far % (len(eval_d) // intermed_check_step_split) == 0:\n",
    "            print(\".\", end=\"\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "        steps_so_far += 1\n",
    "\n",
    "    if return_to_cpu:\n",
    "        model = model.to(\"cpu\")\n",
    "\n",
    "    if not LORA:\n",
    "        base_model = base_model.to(\"cpu\")\n",
    "\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    data = {\n",
    "        \"loss\": eval_loss,\n",
    "        \"base_loss\": eval_base_loss,\n",
    "        \"lora_diff\": lora_diff,\n",
    "        \"head_to_head\": head_to_head,\n",
    "        \"eps0_head_to_head\": eps0_head_to_head,\n",
    "        \"overshoot\": overshoot\n",
    "    }\n",
    "\n",
    "    if print_stats:\n",
    "        print(f\" Loss: {eval_loss:.8f}, Base Loss: {eval_base_loss:.6f}, Lora Diff: {lora_diff:.8f},\",\n",
    "            f\"WR: {head_to_head:.2f}%, 0epsWR: {eps0_head_to_head:.2f}%, OShL: {overshoot:.8f}\")\n",
    "    if return_stats:\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_d, eval_d, base_model=None, inf_training=False, training_device=\"cuda\",\n",
    "            acc_batch_size=32, instruction_finetuing=True, precalculate_base_loss=True, precalculate_batch_mult=1.5,\n",
    "            lr=1e-4, weight_decay=0.001, lr_scheduler=\"constant\", warmup_steps=0, betas=(0.9, 0.99), squared_loss=False,\n",
    "            use_sam=False, sam_rho=0.05, do_dadapt=False, use_LOMO=False, opt=\"adamw\",\n",
    "            manual_grad_clip_norm=0.0, manual_grad_clip_value=0.0, wait_for_full_batch=True, sam_reuse_base_outputs=False,\n",
    "            do_base_gradient=True, add_overshoot_penalty=True, ignore_overshot_samples=True, bad_sample_mult=1.0,\n",
    "            loss_eps = 0.015, overshoot_buffer = 0.01,\n",
    "            prompt_dropout=0.0,\n",
    "            eval_steps=1024, save_steps=1024, save_name=\"lora\", do_save=True,\n",
    "            average_stats=False,\n",
    "            partial_eval_steps=0, partial_eval_size=128, save_n_start=0,\n",
    "            gradient_checkpointing=False):\n",
    "    if warmup_steps is None:\n",
    "        warmup_steps = (eval_steps // acc_batch_size) // 2\n",
    "    LORA = True\n",
    "    if base_model is not None:\n",
    "        LORA = False\n",
    "        # base_model = base_model.to(\"cuda\")\n",
    "\n",
    "    if inf_training:\n",
    "        # from subsetfalconloader import SubsetFalconLoader\n",
    "        pass\n",
    "\n",
    "    if precalculate_base_loss and do_base_gradient:\n",
    "        raise ValueError(\"Precalculating base loss will disconnect base gradients\")\n",
    "\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    model = model.to(\"cuda\")\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "    if gradient_checkpointing:\n",
    "        model.config.use_cache = False\n",
    "        grad_check_kwargs = {\"use_reentrant\": False}\n",
    "        if do_base_gradient:\n",
    "            grad_check_kwargs[\"use_reentrant\"] = True\n",
    "        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=grad_check_kwargs)\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    if not use_sam:\n",
    "        if opt == \"dadapt_adam\":\n",
    "            optimizer = DAdaptAdam(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay, fixed_decay=True)\n",
    "        elif opt == \"sophia\":\n",
    "            optimizer = SophiaH(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        elif opt == \"ranger\":\n",
    "            optimizer = Ranger21(model.parameters(), num_iterations=1, lr=lr, betas=betas, weight_decay=weight_decay,\n",
    "                                 num_warm_up_iterations=0, num_warm_down_iterations=0)\n",
    "        elif opt == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        elif opt == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=betas[0], weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer {opt}\")\n",
    "    else:\n",
    "        base_optimizer_args = {\"lr\": lr, \"weight_decay\": weight_decay, \"betas\": betas, \"eps\": 1e-8}\n",
    "\n",
    "        if opt == \"dadapt_adam\":\n",
    "            base_optimizer = DAdaptAdam\n",
    "        elif opt == \"sophia\":\n",
    "            base_optimizer = SophiaH\n",
    "        elif opt == \"ranger\":\n",
    "            base_optimizer = Ranger21\n",
    "            base_optimizer_args[\"num_iterations\"] = 1\n",
    "            base_optimizer_args[\"num_warm_up_iterations\"] = 0\n",
    "            base_optimizer_args[\"num_warm_down_iterations\"] = 0\n",
    "        elif opt == \"adamw\":\n",
    "            base_optimizer = torch.optim.AdamW\n",
    "        elif opt == \"sgd\":\n",
    "            base_optimizer = torch.optim.SGD\n",
    "            del base_optimizer_args[\"betas\"], base_optimizer_args[\"eps\"]\n",
    "            base_optimizer_args[\"momentum\"] = betas[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer {opt}\")\n",
    "\n",
    "        optimizer = SAM(model.parameters(), base_optimizer=base_optimizer, rho=sam_rho, adaptive=True, **base_optimizer_args)\n",
    "        sam_optimizer = optimizer\n",
    "\n",
    "    if use_LOMO:\n",
    "        if use_sam:\n",
    "            raise ValueError(\"LOMO and SAM are not compatible\")\n",
    "        if do_dadapt:\n",
    "            raise ValueError(\"LOMO and DAdapt are not compatible\")\n",
    "        # check for lora layers\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"lora\" in name:\n",
    "                print(\"training lora using LOMO?\")\n",
    "                break\n",
    "        optimizer = LOMO(model, lr=lr)\n",
    "        lomo_optimizer = optimizer\n",
    "\n",
    "    if lr_scheduler == \"cosine\":\n",
    "        lr_scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, warmup_steps, (len(train_d)//acc_batch_size)+warmup_steps)\n",
    "    else:\n",
    "        lr_scheduler = transformers.get_constant_schedule_with_warmup(optimizer, warmup_steps)\n",
    "    lr_scheduler.step() # don't want to start at 0\n",
    "\n",
    "    @torch.jit.script\n",
    "    def combined_loss_bgrad_os(outputs_loss, base_loss, loss_eps:float=loss_eps, overshoot_buffer:float=overshoot_buffer):\n",
    "        partial_loss = torch.nn.functional.relu(outputs_loss - (base_loss * (1.0 - loss_eps)))\n",
    "        loss = partial_loss / base_loss\n",
    "        overshoot_penalty = torch.nn.functional.relu(-(loss - (loss_eps + overshoot_buffer)))\n",
    "        loss = loss + overshoot_penalty\n",
    "        return loss, overshoot_penalty.item()\n",
    "    \n",
    "    @torch.jit.script\n",
    "    def combined_loss_os(outputs_loss, base_loss_in, loss_eps:float=loss_eps, overshoot_buffer:float=overshoot_buffer):\n",
    "        base_loss = base_loss_in.item()\n",
    "        partial_loss = torch.nn.functional.relu(outputs_loss - (base_loss * (1.0 - loss_eps)))\n",
    "        loss = partial_loss / base_loss\n",
    "        overshoot_penalty = torch.nn.functional.relu(-(loss - (loss_eps + overshoot_buffer)))\n",
    "        loss = loss + overshoot_penalty\n",
    "        return loss, overshoot_penalty.item()\n",
    "    \n",
    "    @torch.jit.script\n",
    "    def combined_loss_os_noshot(outputs_loss, base_loss_in, loss_eps:float=loss_eps, overshoot_buffer:float=overshoot_buffer):\n",
    "        base_loss = base_loss_in.item()\n",
    "        partial_loss = torch.nn.functional.relu(outputs_loss - (base_loss * (1.0 - loss_eps)))\n",
    "        loss = partial_loss / base_loss\n",
    "        return loss, torch.nn.functional.relu(-(loss - (loss_eps + overshoot_buffer))).detach().item()\n",
    "    \n",
    "    @torch.jit.script\n",
    "    def combined_loss_bgrad_os_noshot(outputs_loss, base_loss, loss_eps:float=loss_eps, overshoot_buffer:float=overshoot_buffer):\n",
    "        partial_loss = torch.nn.functional.relu(outputs_loss - (base_loss * (1.0 - loss_eps)))\n",
    "        loss = partial_loss / base_loss\n",
    "        return loss, torch.nn.functional.relu(-(loss - (loss_eps + overshoot_buffer))).detach().item()\n",
    "    \n",
    "    if add_overshoot_penalty:\n",
    "        if do_base_gradient:\n",
    "            combined_loss = combined_loss_bgrad_os\n",
    "        else:\n",
    "            combined_loss = combined_loss_os\n",
    "    else:\n",
    "        if do_base_gradient:\n",
    "            combined_loss = combined_loss_bgrad_os_noshot\n",
    "        else:\n",
    "            combined_loss = combined_loss_os_noshot\n",
    "\n",
    "\n",
    "    steps_so_far = 1 # start at one to avoid all the modulo checks\n",
    "    epoch_loss = 0; epoch_overshoot = 0; epoch_base_loss = 0; lora_diff = 0\n",
    "    epoch_wr = 0; epoch_0eps_wr = 0\n",
    "    fit_samples = 0; unfit_samples = 0\n",
    "    sam_batch = []\n",
    "    # lomo_batch_loss = []\n",
    "    accum_steps = 0\n",
    "    true_steps_taken = 0; prev_dot_step = -1\n",
    "    last_tst = true_steps_taken\n",
    "    sam_saved_base_outputs = []\n",
    "    precalculated_base_outputs = []\n",
    "    while len(train_d) > 0:\n",
    "\n",
    "        if precalculate_base_loss and len(precalculated_base_outputs) == 0:\n",
    "            batches = train_d[:int(acc_batch_size * precalculate_batch_mult)]\n",
    "\n",
    "            if LORA:\n",
    "                model.disable_adapter_layers()\n",
    "            else:\n",
    "                base_model = base_model.to(training_device)\n",
    "            \n",
    "            for batch in batches:\n",
    "                if instruction_finetuing:\n",
    "                    inputs = batch['input_ids'].to(training_device)\n",
    "                    labels = batch['labels'].to(training_device)\n",
    "                else:\n",
    "                    inputs = batch.to(training_device)\n",
    "                    labels = inputs.clone()\n",
    "\n",
    "                if prompt_dropout > 0.0:\n",
    "                    labels = labels.clone()\n",
    "                    labels[:, torch.randperm(labels.size(1))[:int(labels.size(1) * prompt_dropout)]] = -100\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if LORA:\n",
    "                        base_outputs = model(inputs, labels=labels)\n",
    "                    else:\n",
    "                        base_outputs = base_model(inputs, labels=labels)\n",
    "                    precalculated_base_outputs.append(base_outputs.loss)\n",
    "\n",
    "            if LORA:\n",
    "                model.enable_adapter_layers()\n",
    "            else:\n",
    "                base_model = base_model.to(\"cpu\")\n",
    "\n",
    "\n",
    "        batch = train_d.pop(0)\n",
    "        if instruction_finetuing:\n",
    "            inputs = batch['input_ids'].to(training_device)\n",
    "            labels = batch['labels'].to(training_device)\n",
    "        else:\n",
    "            inputs = batch.to(training_device)\n",
    "            labels = inputs.clone()\n",
    "\n",
    "        if prompt_dropout > 0.0:\n",
    "            labels = labels.clone()\n",
    "            labels[:, torch.randperm(labels.size(1))[:int(labels.size(1) * prompt_dropout)]] = -100\n",
    "        \n",
    "        if not precalculated_base_outputs:\n",
    "            if LORA:\n",
    "                model.disable_adapter_layers()\n",
    "                base_outputs_loss = model(inputs, labels=labels).loss\n",
    "                model.enable_adapter_layers()\n",
    "                outputs_loss = model(inputs, labels=labels).loss\n",
    "            else:\n",
    "                base_outputs_loss = base_model(inputs, labels=labels).loss\n",
    "                outputs_loss = model(inputs, labels=labels).loss\n",
    "        else:\n",
    "            base_outputs_loss = precalculated_base_outputs.pop(0)\n",
    "            outputs_loss = model(inputs, labels=labels).loss\n",
    "\n",
    "        # current_loss_eps = final_loss_eps \n",
    "        current_loss_eps = loss_eps # ((final_loss_eps * (min(steps_so_far * 8 / len(train_d), 1.0))) + initial_loss_eps) / (1.0 + initial_loss_eps)\n",
    "        # current_loss_eps = 0.011 # lr_scheduler.get_last_lr()[0] * 100.0 # * 3.33\n",
    "\n",
    "        loss, overshoot_penalty = combined_loss(outputs_loss, base_outputs_loss)\n",
    "        loss = loss / acc_batch_size\n",
    "\n",
    "        if not ignore_overshot_samples or overshoot_penalty == 0.0:\n",
    "            if loss.item() > ((loss_eps / acc_batch_size)+1e-8):\n",
    "                unfit_samples += -1\n",
    "                if bad_sample_mult is not None and bad_sample_mult != 1.0:\n",
    "                    loss = loss * bad_sample_mult\n",
    "            elif loss.item() < ((loss_eps - (loss_eps + overshoot_buffer)) / acc_batch_size):\n",
    "                fit_samples += 1\n",
    "\n",
    "            # if not use_LOMO:\n",
    "            if squared_loss:\n",
    "                floss = loss ** 2\n",
    "            else:\n",
    "                floss = loss\n",
    "            if opt == \"sophia\":\n",
    "                floss.backward(create_graph=True, retain_graph=False)\n",
    "            else:\n",
    "                floss.backward()\n",
    "            \n",
    "            # else:\n",
    "            #     lomo_batch_loss.append(loss)\n",
    "            accum_steps += 1\n",
    "            true_steps_taken += 1\n",
    "            if use_sam:\n",
    "                sam_batch.append((inputs, labels))\n",
    "                if sam_reuse_base_outputs:\n",
    "                    sam_saved_base_outputs.append(base_outputs_loss.detach())\n",
    "        else:\n",
    "            fit_samples += 1\n",
    "\n",
    "        outputs_loss_item = outputs_loss.detach().item()\n",
    "        base_loss_item = base_outputs_loss.detach().item()\n",
    "        epoch_base_loss += base_loss_item\n",
    "        lora_diff += (outputs_loss_item - base_loss_item)\n",
    "        epoch_loss += loss.detach().item() * acc_batch_size\n",
    "        epoch_wr += 100.0 if outputs_loss_item < (base_loss_item * (1.0 - true_eps)) else 0.0\n",
    "        epoch_wr += 50.0 if outputs_loss_item == (base_loss_item * (1.0 - true_eps)) else 0.0\n",
    "        epoch_0eps_wr += 100.0 if outputs_loss_item < base_loss_item else 0.0\n",
    "        epoch_0eps_wr += 50.0 if outputs_loss_item == base_loss_item else 0.0\n",
    "        epoch_overshoot += overshoot_penalty\n",
    "\n",
    "        if (true_steps_taken % (acc_batch_size // intermed_check_step_split) == 0) and accum_steps != prev_dot_step:\n",
    "            prev_dot_step = accum_steps\n",
    "            print(\".\", end=\"\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "        if (((steps_so_far % acc_batch_size == 0 or steps_so_far == len(train_d)) and not wait_for_full_batch) or \n",
    "                                                                            (wait_for_full_batch and accum_steps == acc_batch_size)):\n",
    "            if manual_grad_clip_norm > 0.0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), manual_grad_clip_norm)\n",
    "            if manual_grad_clip_value > 0.0:\n",
    "                torch.nn.utils.clip_grad_value_(model.parameters(), manual_grad_clip_value)\n",
    "\n",
    "            if use_LOMO:\n",
    "                lomo_optimizer.fuse_update()\n",
    "                # lomo_optimizer.fused_backward(loss, lr_scheduler.get_last_lr()[0])\n",
    "                # lomo_optimizer.para\n",
    "\n",
    "            if not use_sam and not use_LOMO:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                sub_steps = 1\n",
    "                sam_optimizer.first_step(zero_grad=True)\n",
    "                \n",
    "                for inputs, labels in sam_batch:\n",
    "                    if LORA:\n",
    "                        if sam_reuse_base_outputs:\n",
    "                            base_outputs_loss = sam_saved_base_outputs.pop(0)\n",
    "                        else:\n",
    "                            model.disable_adapter_layers()\n",
    "                            base_outputs_loss = model(inputs, labels=labels).loss\n",
    "                            model.enable_adapter_layers()\n",
    "                        outputs = model(inputs, labels=labels)\n",
    "                    else:\n",
    "                        if sam_reuse_base_outputs:\n",
    "                            base_outputs_loss = sam_saved_base_outputs.pop(0)\n",
    "                        else:\n",
    "                            base_outputs_loss = base_model(inputs, labels=labels).loss\n",
    "                        outputs = model(inputs, labels=labels)\n",
    "\n",
    "                    loss, overshoot_penalty = combined_loss(outputs.loss, base_outputs_loss)\n",
    "                    if loss.item() > ((loss_eps / acc_batch_size)+1e-7):\n",
    "                        if bad_sample_mult is not None and bad_sample_mult != 1.0:\n",
    "                            loss = loss * bad_sample_mult\n",
    "                    loss = loss / accum_steps\n",
    "                    \n",
    "                    if squared_loss:\n",
    "                        floss = loss ** 2\n",
    "                    else:\n",
    "                        floss = loss\n",
    "                    if opt == \"sophia\":\n",
    "                        floss.backward(create_graph=True, retain_graph=False)\n",
    "                    else:\n",
    "                        floss.backward()\n",
    "\n",
    "                    sub_steps += 1\n",
    "                    if sub_steps % (acc_batch_size // intermed_check_step_split) == 0:\n",
    "                        print(\".\", end=\"\")\n",
    "                    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "                sam_optimizer.second_step(zero_grad=True)\n",
    "                sam_batch = []\n",
    "\n",
    "            if average_stats:\n",
    "                stat_steps = steps_so_far\n",
    "            else:\n",
    "                stat_steps = accum_steps\n",
    "                if wait_for_full_batch:\n",
    "                    stat_steps += fit_samples\n",
    "            print(f\"Step {steps_so_far}/{len(train_d)}\\tLoss: {epoch_loss/stat_steps:.6f}\",\n",
    "                                                    f\"OShL: {epoch_overshoot/stat_steps:.3e}\"\n",
    "                                                    f\"\\tBase: {epoch_base_loss/stat_steps:.4f}\",\n",
    "                                                    f\"Diff: {lora_diff/stat_steps:.4e}\",\n",
    "                                                    f\"\\tWR: {epoch_wr/stat_steps:2.2f}%\",\n",
    "                                                    f\"0eps: {epoch_0eps_wr/stat_steps:2.2f}% \",\n",
    "                                                    f\"\\tLR: {lr_scheduler.get_last_lr()[0]:.2e}\",\n",
    "                                                    f\"eps: {loss_eps:.2e}\",\n",
    "                                                    f\"fit: {fit_samples}/{unfit_samples}\"\n",
    "                                                    )\n",
    "\n",
    "            if inf_training:\n",
    "                # pages = [random.randint(1, SubsetFalconLoader.max_pages) for _ in range(accum_steps+fit_samples)]\n",
    "                # batches = list(SubsetFalconLoader(pages=pages, rows_per_page=100, sample_total=accum_steps+fit_samples, silent=True))\n",
    "                # random.shuffle(batches)\n",
    "                # train_d.extend(batches[:accum_steps+fit_samples])\n",
    "                # random.shuffle(train_d) # reshuffling may be bad if precomputing base loss\n",
    "                pass\n",
    "\n",
    "            if not average_stats:\n",
    "                epoch_overshoot = 0\n",
    "                epoch_loss = 0\n",
    "                epoch_base_loss = 0\n",
    "                lora_diff = 0\n",
    "                epoch_wr = 0\n",
    "                epoch_0eps_wr = 0\n",
    "            unfit_samples = 0\n",
    "            fit_samples = 0\n",
    "            accum_steps = 0\n",
    "            if sam_saved_base_outputs != []:\n",
    "                raise ValueError(\"SAM saved base outputs didn't get used?\")\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        if do_save and true_steps_taken % save_steps == 0 and len(train_d) > 0 and true_steps_taken != last_tst:\n",
    "            model.save_pretrained(save_name + '_' + str((true_steps_taken // save_steps) + save_n_start).format(\"02d\"))\n",
    "            \n",
    "        do_full_eval = true_steps_taken % eval_steps == 0 and len(train_d) > 0 and true_steps_taken != last_tst\n",
    "        if do_full_eval:\n",
    "            evaluate(model, eval_d, base_model=base_model, device=training_device, instruction_finetuning=instruction_finetuing)\n",
    "            model.train()\n",
    "        if ((partial_eval_steps > 0 and true_steps_taken % partial_eval_steps == 0) and not do_full_eval \n",
    "                                            and len(train_d) > 0 and true_steps_taken != last_tst):\n",
    "            evaluate(model, eval_d[:partial_eval_size], base_model=base_model, device=training_device, instruction_finetuning=instruction_finetuing)\n",
    "            model.train()\n",
    "        \n",
    "        steps_so_far += 1\n",
    "        last_tst = true_steps_taken\n",
    "\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    if do_save:\n",
    "        if save_n_start > 0:\n",
    "            model.save_pretrained(save_name+\"_X\"+str(save_n_start))\n",
    "        else:\n",
    "            model.save_pretrained(save_name)\n",
    "\n",
    "    model.eval()\n",
    "    final_eval_stats = evaluate(model, eval_d, return_stats=True, base_model=base_model, device=training_device, instruction_finetuning=instruction_finetuing)\n",
    "\n",
    "    model = model.to(\"cpu\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    return final_eval_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca9fd1238bd4767ad7ca5206bbd3611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 436,207,616 || all params: 7,677,939,712 || trainable%: 5.681310772970029\n"
     ]
    }
   ],
   "source": [
    "lora_name = \"Hydrogen\"\n",
    "model_name = \"\"\n",
    "\n",
    "neft_noise = 0.0 # bad actually?\n",
    "\n",
    "\n",
    "rank = 1024\n",
    "config = LoraConfig(\n",
    "    r=rank, lora_alpha=16,\n",
    "    target_modules=['q_proj','v_proj', \n",
    "                    # \"k_proj\", \"o_proj\", \n",
    "                    # \"gate_proj\", \n",
    "                    # \"up_proj\", \n",
    "                    # \"down_proj\"\n",
    "                    ],  #   , \n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "    use_rslora=True,\n",
    "    # init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "# config = LoHaConfig(\n",
    "#     r=rank, \n",
    "#     alpha=rank,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"], #,  , \"up_proj\" # , \"o_proj\" , \"k_proj\", \"down_proj\"\n",
    "#     rank_dropout=0.0,\n",
    "#     module_dropout=0.0,\n",
    "#     init_weights=True,\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# config = AdaLoraConfig(\n",
    "#     peft_type=\"ADALORA\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     r=rank,\n",
    "#     init_r=int(rank * 1.5),\n",
    "#     target_r=rank, \n",
    "#     lora_alpha=rank,\n",
    "#     orth_reg_weight=0.5,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"], #, , \"down_proj\" , \"up_proj\", \"o_proj\" # , \"k_proj\"\n",
    "#     lora_dropout=0.0,\n",
    "#     tinit=8, tfinal=32,\n",
    "#     deltaT=1,\n",
    "#     use_rslora=True,\n",
    "#     total_step = 8192 // 64,\n",
    "# )\n",
    "\n",
    "params = {\n",
    "    'low_cpu_mem_usage': True,\n",
    "    'trust_remote_code': False,\n",
    "    'torch_dtype': torch.bfloat16,\n",
    "    'use_safetensors': True,\n",
    "    'attn_implementation': \"flash_attention_2\"\n",
    "}\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, **params, cache_dir=\"Models\")\n",
    "lora_model = get_peft_model(model, config)\n",
    "# lora_model = PeftModel.from_pretrained(model, model_id=\"lora_0x0\", is_trainable=True)\n",
    "# lora_model.to(torch.bfloat16)\n",
    "lora_model.print_trainable_parameters()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=False, use_fast=True, cache_dir=\"Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0x0 2_16_e512 {'eval_loss': 0.34912705421447754, 'eval_runtime': 99.3432, 'eval_samples_per_second': 5.154, 'eval_steps_per_second': 5.154}\n",
    "#t27 2_16_e512 {'eval_loss': 0.3499789834022522, 'eval_runtime': 103.1445, 'eval_samples_per_second': 4.964, 'eval_steps_per_second': 4.964}\n",
    "\n",
    "#0x0 2_e26_512 {'eval_loss': 0.4443358778953552, 'eval_runtime': 92.638, 'eval_samples_per_second': 5.527, 'eval_steps_per_second': 5.527}\n",
    "#t27 2_e26_512 {'eval_loss': 0.4439195692539215, 'eval_runtime': 92.953, 'eval_samples_per_second': 5.508, 'eval_steps_per_second': 5.508}\n",
    "\n",
    "#aes e1_512 {'eval_loss': 0.3197530210018158, 'eval_runtime': 100.1814, 'eval_samples_per_second': 5.111, 'eval_steps_per_second': 5.111}\n",
    "#0x0 e1_512 {'eval_loss': 0.3169699013233185, 'eval_runtime': 98.9367, 'eval_samples_per_second': 5.175, 'eval_steps_per_second': 5.175}\n",
    "#t27 e1_512 {'eval_loss': 0.31162574887275696, 'eval_runtime': 96.6104, 'eval_samples_per_second': 5.3, 'eval_steps_per_second': 5.3}\n",
    "#m00 e1_512 {'eval_loss': 0.30996233224868774, 'eval_runtime': 91.7931, 'eval_samples_per_second': 5.578, 'eval_steps_per_second': 5.578}\n",
    "#sn6 e1_512 {'eval_loss': 0.3129960298538208, 'eval_runtime': 94.8393, 'eval_samples_per_second': 5.399, 'eval_steps_per_second': 5.399}\n",
    "\n",
    "#m01 e18_512 {'eval_loss': 0.29920494556427, 'eval_runtime': 103.3506, 'eval_samples_per_second': 4.954, 'eval_steps_per_second': 4.954}\n",
    "#sn6 e18_512 {'eval_loss': 0.30003952980041504, 'eval_runtime': 92.8474, 'eval_samples_per_second': 5.514, 'eval_steps_per_second': 5.514}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found in eval\n"
     ]
    }
   ],
   "source": [
    "# train_data, eval_data = get_data(train_name=['data/perf_43_16896_256.json',], #  # 'data/atk_43_2048_2048.json'\n",
    "#                                  train_subset=np.arange(16384), eval_subset=np.arange(16384, 16384+512),\n",
    "#                                  eval_name='data/perf_43_16896_256.json', tokenizer=tokenizer, shuffle=False)\n",
    "train_data, eval_data = get_data(train_name=['data/cortex_52_4608.json',],\n",
    "                                 train_subset=np.arange(4096), eval_subset=np.arange(4096,4096+512),\n",
    "                                 eval_name='data/cortex_52_4608.json', tokenizer=tokenizer, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minf_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43macc_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction_finetuing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecalculate_base_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecalculate_batch_mult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.55\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconstant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msam_rho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msgd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_eps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43movershoot_buffer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmanual_grad_clip_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanual_grad_clip_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_full_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msam_reuse_base_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_base_gradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_overshoot_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_overshot_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbad_sample_mult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial_eval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial_eval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_n_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 337\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_d, eval_d, base_model, inf_training, training_device, acc_batch_size, instruction_finetuing, precalculate_base_loss, precalculate_batch_mult, lr, weight_decay, lr_scheduler, warmup_steps, betas, squared_loss, use_sam, sam_rho, do_dadapt, use_LOMO, opt, manual_grad_clip_norm, manual_grad_clip_value, wait_for_full_batch, sam_reuse_base_outputs, do_base_gradient, add_overshoot_penalty, ignore_overshot_samples, bad_sample_mult, loss_eps, overshoot_buffer, prompt_dropout, eval_steps, save_steps, save_name, do_save, average_stats, partial_eval_steps, partial_eval_size, save_n_start, gradient_checkpointing)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instruction_finetuing:\n\u001b[1;32m    336\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(training_device)\n\u001b[0;32m--> 337\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    339\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(training_device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(lora_model, train_data, eval_data, base_model=None, inf_training=False, training_device=\"cuda\",\n",
    "        acc_batch_size=1024, instruction_finetuing=True, precalculate_base_loss=True, precalculate_batch_mult=1.55,\n",
    "        lr=4e-5, weight_decay=0.0, lr_scheduler=\"constant\", warmup_steps=0, betas=(0.8, 0.95), squared_loss=False,\n",
    "        use_sam=False, sam_rho=0.2, opt=\"adamw\",\n",
    "        loss_eps = 0.02, overshoot_buffer = -0.01,\n",
    "        manual_grad_clip_norm=0.0, manual_grad_clip_value=0.0, wait_for_full_batch=True, sam_reuse_base_outputs=True,\n",
    "        do_base_gradient=False, add_overshoot_penalty=False, ignore_overshot_samples=True, bad_sample_mult=1.0,\n",
    "        prompt_dropout=0.0,\n",
    "        eval_steps=2048, save_steps=2048, do_save=True, save_name=lora_name,\n",
    "        average_stats=False,\n",
    "        partial_eval_steps=0, partial_eval_size=128, save_n_start=0,\n",
    "        gradient_checkpointing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ........ Loss: 0.01634118, Base Loss: 0.394893, Lora Diff: 0.00121789, WR: 5.08%, 0epsWR: 38.09%, OShL: 0.00017984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.016341184635450645,\n",
       " 'base_loss': 0.39489315043670103,\n",
       " 'lora_diff': 0.0012178865729310928,\n",
       " 'head_to_head': 5.078125,\n",
       " 'eps0_head_to_head': 38.0859375,\n",
       " 'overshoot': 0.00017984330224862788}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect(); torch.cuda.empty_cache()\n",
    "evaluate(lora_model, eval_data, return_stats=True, base_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 13 duplicates from eval\n",
      "................Step 67/8192\tLoss: 0.021026 OShL: 8.122e-04\tBase: 0.4503 Diff: -1.3390e-03 \tWR: 4.48% 0eps: 59.70%  \tLR: 3.12e-09 eps: 2.00e-02 fit: 3/-27\n",
      "................Step 134/8192\tLoss: 0.021912 OShL: 6.013e-05\tBase: 0.3729 Diff: -3.9632e-04 \tWR: 4.48% 0eps: 49.25%  \tLR: 6.25e-09 eps: 2.00e-02 fit: 3/-34\n",
      "................Step 202/8192\tLoss: 0.023419 OShL: 9.164e-04\tBase: 0.4080 Diff: -1.3260e-03 \tWR: 5.88% 0eps: 54.41%  \tLR: 9.37e-09 eps: 2.00e-02 fit: 4/-31\n",
      "................Step 271/8192\tLoss: 0.022529 OShL: 2.105e-04\tBase: 0.3744 Diff: -1.9587e-04 \tWR: 7.25% 0eps: 49.28%  \tLR: 1.25e-08 eps: 2.00e-02 fit: 5/-35\n",
      "................Step 341/8192\tLoss: 0.020482 OShL: 1.952e-03\tBase: 0.4646 Diff: -2.7062e-03 \tWR: 8.57% 0eps: 52.86%  \tLR: 1.56e-08 eps: 2.00e-02 fit: 6/-33\n",
      "................Step 408/8192\tLoss: 0.021326 OShL: 6.891e-05\tBase: 0.4062 Diff: -3.0093e-04 \tWR: 4.48% 0eps: 55.22%  \tLR: 1.87e-08 eps: 2.00e-02 fit: 3/-30\n",
      "................Step 480/8192\tLoss: 0.020705 OShL: 5.998e-04\tBase: 0.3770 Diff: -9.1319e-04 \tWR: 11.11% 0eps: 51.39%  \tLR: 2.19e-08 eps: 2.00e-02 fit: 8/-35\n",
      "................Step 554/8192\tLoss: 0.019583 OShL: 7.609e-04\tBase: 0.3918 Diff: -1.2686e-03 \tWR: 13.51% 0eps: 62.16%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 10/-28\n",
      "Evaluating ........ Loss: 0.00863659, Base Loss: 0.366165, Lora Diff: -0.00107774, WR: 11.72%, 0epsWR: 62.50%, OShL: 0.00052821\n",
      "................Step 622/8192\tLoss: 0.021733 OShL: 2.445e-04\tBase: 0.3954 Diff: -5.4929e-04 \tWR: 5.88% 0eps: 55.88%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-30\n",
      "................Step 695/8192\tLoss: 0.019805 OShL: 3.165e-04\tBase: 0.4387 Diff: -5.6851e-04 \tWR: 12.33% 0eps: 58.90%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 9/-30\n",
      "................Step 763/8192\tLoss: 0.020358 OShL: 1.115e-04\tBase: 0.4238 Diff: -4.5875e-04 \tWR: 5.88% 0eps: 64.71%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-24\n",
      "................Step 829/8192\tLoss: 0.021202 OShL: 5.511e-05\tBase: 0.4070 Diff: -3.1505e-04 \tWR: 3.03% 0eps: 57.58%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 2/-28\n",
      "................Step 899/8192\tLoss: 0.021640 OShL: 4.515e-04\tBase: 0.4192 Diff: -9.0135e-04 \tWR: 8.57% 0eps: 52.86%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 6/-33\n",
      "................Step 965/8192\tLoss: 0.023099 OShL: 3.119e-05\tBase: 0.3691 Diff: -5.3787e-05 \tWR: 3.03% 0eps: 45.45%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 2/-36\n",
      "................Step 1031/8192\tLoss: 0.021989 OShL: 4.453e-05\tBase: 0.3612 Diff: -1.8172e-04 \tWR: 3.03% 0eps: 54.55%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 2/-30\n",
      "................Step 1097/8192\tLoss: 0.022480 OShL: 4.375e-05\tBase: 0.3694 Diff: 4.5302e-06 \tWR: 3.03% 0eps: 53.03%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 2/-31\n",
      "Evaluating ........ Loss: 0.00923102, Base Loss: 0.366165, Lora Diff: -0.00114925, WR: 12.50%, 0epsWR: 57.81%, OShL: 0.00063269\n",
      "................Step 1163/8192\tLoss: 0.021152 OShL: 1.223e-05\tBase: 0.3601 Diff: -2.4292e-04 \tWR: 3.03% 0eps: 62.12%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 2/-25\n",
      "................Step 1231/8192\tLoss: 0.021217 OShL: 1.976e-04\tBase: 0.3502 Diff: -4.4181e-04 \tWR: 5.88% 0eps: 57.35%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-29\n",
      "................Step 1299/8192\tLoss: 0.023729 OShL: 8.226e-04\tBase: 0.3976 Diff: -1.3423e-03 \tWR: 5.88% 0eps: 66.18%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-23\n",
      "................Step 1370/8192\tLoss: 0.020754 OShL: 1.431e-03\tBase: 0.4520 Diff: -2.0453e-03 \tWR: 9.86% 0eps: 57.75%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 7/-30\n",
      "................Step 1438/8192\tLoss: 0.021866 OShL: 3.461e-04\tBase: 0.3982 Diff: -6.9194e-04 \tWR: 5.88% 0eps: 55.88%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-30\n",
      "................Step 1508/8192\tLoss: 0.024774 OShL: 3.378e-04\tBase: 0.3720 Diff: -5.0908e-04 \tWR: 8.57% 0eps: 57.14%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 6/-30\n",
      "................Step 1576/8192\tLoss: 0.022593 OShL: 6.664e-05\tBase: 0.3585 Diff: -1.1548e-04 \tWR: 5.88% 0eps: 54.41%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-31\n",
      "................Step 1643/8192\tLoss: 0.022223 OShL: 3.879e-05\tBase: 0.3840 Diff: -6.7668e-05 \tWR: 4.48% 0eps: 50.75%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 3/-33\n",
      "Evaluating ........ Loss: 0.00893380, Base Loss: 0.366165, Lora Diff: -0.00103252, WR: 10.94%, 0epsWR: 57.81%, OShL: 0.00047874\n",
      "................Step 1711/8192\tLoss: 0.021291 OShL: 1.166e-04\tBase: 0.3588 Diff: -3.4613e-04 \tWR: 5.88% 0eps: 55.88%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-30\n",
      ".................Step 1782/8192\tLoss: 0.022141 OShL: 8.828e-05\tBase: 0.4075 Diff: -2.9169e-04 \tWR: 9.86% 0eps: 47.89%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 7/-37\n",
      "................Step 1851/8192\tLoss: 0.021419 OShL: 1.173e-03\tBase: 0.4238 Diff: -1.6073e-03 \tWR: 7.25% 0eps: 52.17%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 5/-33\n",
      "................Step 1924/8192\tLoss: 0.020230 OShL: 1.709e-03\tBase: 0.4299 Diff: -2.3332e-03 \tWR: 12.33% 0eps: 53.42%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 9/-34\n",
      "................Step 1995/8192\tLoss: 0.021562 OShL: 8.254e-05\tBase: 0.3810 Diff: -3.0655e-04 \tWR: 9.86% 0eps: 52.11%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 7/-34\n",
      "................Step 2070/8192\tLoss: 0.018449 OShL: 1.053e-03\tBase: 0.4377 Diff: -1.7739e-03 \tWR: 14.67% 0eps: 66.67%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 11/-25\n",
      "................Step 2142/8192\tLoss: 0.022157 OShL: 3.976e-04\tBase: 0.3783 Diff: -4.4412e-04 \tWR: 11.11% 0eps: 51.39%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 8/-35\n",
      "................Step 2216/8192\tLoss: 0.020783 OShL: 3.405e-04\tBase: 0.3662 Diff: -6.3239e-04 \tWR: 13.51% 0eps: 52.70%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 10/-35\n",
      "Evaluating ........ Loss: 0.00942210, Base Loss: 0.368491, Lora Diff: -0.00112090, WR: 11.13%, 0epsWR: 58.79%, OShL: 0.00072228\n",
      "................Step 2283/8192\tLoss: 0.041783 OShL: 7.000e-04\tBase: 0.3902 Diff: -9.2181e-04 \tWR: 4.48% 0eps: 52.24%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 3/-32\n",
      "................Step 2351/8192\tLoss: 0.020997 OShL: 2.141e-04\tBase: 0.4450 Diff: -7.3955e-04 \tWR: 5.88% 0eps: 54.41%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-31\n",
      "................Step 2421/8192\tLoss: 0.021208 OShL: 1.186e-04\tBase: 0.4185 Diff: -4.5864e-04 \tWR: 8.57% 0eps: 51.43%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 6/-34\n",
      "................Step 2491/8192\tLoss: 0.019656 OShL: 4.366e-04\tBase: 0.3685 Diff: -7.7039e-04 \tWR: 8.57% 0eps: 64.29%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 6/-25\n",
      "................Step 2558/8192\tLoss: 0.023717 OShL: 1.022e-04\tBase: 0.3293 Diff: -1.4100e-04 \tWR: 4.48% 0eps: 49.25%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 3/-34\n",
      "................Step 2629/8192\tLoss: 0.020545 OShL: 4.020e-04\tBase: 0.3545 Diff: -6.6875e-04 \tWR: 9.86% 0eps: 56.34%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 7/-31\n",
      "................Step 2697/8192\tLoss: 0.021296 OShL: 1.313e-04\tBase: 0.3998 Diff: -3.6911e-04 \tWR: 5.88% 0eps: 58.82%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-28\n",
      "................Step 2766/8192\tLoss: 0.021465 OShL: 4.808e-04\tBase: 0.4292 Diff: -8.6921e-04 \tWR: 7.25% 0eps: 56.52%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 5/-30\n",
      "Evaluating ........ Loss: 0.00899150, Base Loss: 0.366165, Lora Diff: -0.00144137, WR: 10.94%, 0epsWR: 57.81%, OShL: 0.00088979\n",
      "................Step 2840/8192\tLoss: 0.018327 OShL: 2.396e-04\tBase: 0.3457 Diff: -7.9937e-04 \tWR: 13.51% 0eps: 67.57%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 10/-24\n",
      "................Step 2912/8192\tLoss: 0.021165 OShL: 1.979e-04\tBase: 0.4012 Diff: -2.8989e-04 \tWR: 11.11% 0eps: 50.00%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 8/-36\n",
      "................Step 2988/8192\tLoss: 0.019981 OShL: 3.389e-04\tBase: 0.3550 Diff: -5.2914e-04 \tWR: 15.79% 0eps: 55.26%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 12/-34\n",
      ".................Step 3059/8192\tLoss: 0.020822 OShL: 1.356e-04\tBase: 0.3836 Diff: -4.3730e-04 \tWR: 9.86% 0eps: 54.93%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 7/-32\n",
      "................Step 3132/8192\tLoss: 0.020072 OShL: 9.153e-04\tBase: 0.3999 Diff: -1.3345e-03 \tWR: 12.33% 0eps: 58.90%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 9/-30\n",
      "................Step 3200/8192\tLoss: 0.021355 OShL: 1.891e-05\tBase: 0.3941 Diff: -1.8339e-04 \tWR: 5.88% 0eps: 57.35%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-29\n",
      "................Step 3271/8192\tLoss: 0.021262 OShL: 1.102e-03\tBase: 0.4289 Diff: -1.6499e-03 \tWR: 9.86% 0eps: 54.93%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 7/-32\n",
      "................Step 3338/8192\tLoss: 0.022065 OShL: 6.056e-05\tBase: 0.3754 Diff: -2.0227e-04 \tWR: 4.48% 0eps: 52.24%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 3/-32\n",
      "Evaluating ........ Loss: 0.00831227, Base Loss: 0.366165, Lora Diff: -0.00172337, WR: 14.06%, 0epsWR: 63.28%, OShL: 0.00103502\n",
      "................Step 3411/8192\tLoss: 0.020695 OShL: 9.386e-04\tBase: 0.3489 Diff: -1.4697e-03 \tWR: 12.33% 0eps: 57.53%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 9/-31\n",
      "................Step 3478/8192\tLoss: 0.022127 OShL: 3.780e-04\tBase: 0.4010 Diff: -7.1745e-04 \tWR: 4.48% 0eps: 59.70%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 3/-27\n",
      "................Step 3549/8192\tLoss: 0.022201 OShL: 8.577e-05\tBase: 0.3935 Diff: -3.2229e-04 \tWR: 9.86% 0eps: 57.75%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 7/-30\n",
      "................Step 3615/8192\tLoss: 0.021841 OShL: 3.116e-05\tBase: 0.4477 Diff: -3.5097e-04 \tWR: 3.03% 0eps: 66.67%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 2/-22\n",
      ".................Step 3688/8192\tLoss: 0.021224 OShL: 6.701e-04\tBase: 0.3830 Diff: -1.0553e-03 \tWR: 12.33% 0eps: 60.27%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 9/-29\n",
      ".................Step 3757/8192\tLoss: 0.020084 OShL: 4.616e-04\tBase: 0.3940 Diff: -9.5677e-04 \tWR: 7.25% 0eps: 68.12%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 5/-22\n",
      ".................Step 3827/8192\tLoss: 0.021090 OShL: 2.073e-04\tBase: 0.3831 Diff: -4.2833e-04 \tWR: 8.57% 0eps: 50.00%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 6/-35\n",
      "................Step 3899/8192\tLoss: 0.020922 OShL: 1.389e-04\tBase: 0.3523 Diff: -3.8928e-04 \tWR: 11.11% 0eps: 59.72%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 8/-29\n",
      "Evaluating ........ Loss: 0.00850342, Base Loss: 0.366165, Lora Diff: -0.00160084, WR: 13.28%, 0epsWR: 62.50%, OShL: 0.00086429\n",
      ".Evaluating ........ Loss: 0.00850342, Base Loss: 0.366165, Lora Diff: -0.00160084, WR: 13.28%, 0epsWR: 62.50%, OShL: 0.00086429\n",
      "Evaluating ........ Loss: 0.00850342, Base Loss: 0.366165, Lora Diff: -0.00160084, WR: 13.28%, 0epsWR: 62.50%, OShL: 0.00086429\n",
      "Evaluating ........ Loss: 0.00850342, Base Loss: 0.366165, Lora Diff: -0.00160084, WR: 13.28%, 0epsWR: 62.50%, OShL: 0.00086429\n",
      "................Step 3967/8192\tLoss: 0.020261 OShL: 8.430e-04\tBase: 0.3923 Diff: -1.4133e-03 \tWR: 5.88% 0eps: 60.29%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-27\n",
      "................Step 4034/8192\tLoss: 0.021426 OShL: 1.574e-04\tBase: 0.4148 Diff: -4.0893e-04 \tWR: 4.48% 0eps: 56.72%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 3/-29\n",
      "................Step 4099/8192\tLoss: 0.020076 OShL: 2.922e-05\tBase: 0.4265 Diff: -4.5031e-04 \tWR: 1.54% 0eps: 67.69%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 1/-21\n",
      "................Step 4179/8192\tLoss: 0.018205 OShL: 7.311e-04\tBase: 0.3674 Diff: -1.3761e-03 \tWR: 20.00% 0eps: 65.00%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 16/-28\n",
      "................Step 4251/8192\tLoss: 0.021038 OShL: 1.413e-03\tBase: 0.3938 Diff: -1.9259e-03 \tWR: 11.11% 0eps: 52.78%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 8/-34\n",
      "................Step 4326/8192\tLoss: 0.020002 OShL: 1.461e-03\tBase: 0.3974 Diff: -2.0229e-03 \tWR: 14.67% 0eps: 58.67%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 11/-31\n",
      "................Step 4393/8192\tLoss: 0.021749 OShL: 2.877e-04\tBase: 0.4298 Diff: -4.7835e-04 \tWR: 4.48% 0eps: 53.73%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 3/-31\n",
      "................Step 4468/8192\tLoss: 0.020015 OShL: 7.738e-04\tBase: 0.3817 Diff: -1.2081e-03 \tWR: 14.67% 0eps: 57.33%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 11/-32\n",
      "Evaluating ........ Loss: 0.00968728, Base Loss: 0.368491, Lora Diff: -0.00131116, WR: 11.33%, 0epsWR: 58.79%, OShL: 0.00088463\n",
      "................Step 4535/8192\tLoss: 0.023102 OShL: 6.875e-05\tBase: 0.3818 Diff: -1.1625e-04 \tWR: 4.48% 0eps: 55.22%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 3/-30\n",
      ".................Step 4608/8192\tLoss: 0.019777 OShL: 9.935e-04\tBase: 0.3560 Diff: -1.4875e-03 \tWR: 12.33% 0eps: 58.90%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 9/-30\n",
      "................Step 4678/8192\tLoss: 0.019478 OShL: 5.717e-04\tBase: 0.4275 Diff: -1.3502e-03 \tWR: 8.57% 0eps: 62.86%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 6/-26\n",
      "................Step 4748/8192\tLoss: 0.023459 OShL: 2.525e-04\tBase: 0.3695 Diff: -6.4861e-04 \tWR: 8.57% 0eps: 57.14%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 6/-29\n",
      "................Step 4816/8192\tLoss: 0.022339 OShL: 1.343e-03\tBase: 0.3859 Diff: -2.1239e-03 \tWR: 5.88% 0eps: 67.65%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-22\n",
      ".................Step 4887/8192\tLoss: 0.020527 OShL: 1.867e-04\tBase: 0.4120 Diff: -4.0489e-04 \tWR: 9.86% 0eps: 56.34%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 7/-31\n",
      "................Step 4958/8192\tLoss: 0.019736 OShL: 4.003e-04\tBase: 0.3526 Diff: -7.2780e-04 \tWR: 9.86% 0eps: 61.97%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 7/-27\n",
      "................Step 5032/8192\tLoss: 0.021564 OShL: 4.459e-04\tBase: 0.3771 Diff: -6.7787e-04 \tWR: 13.51% 0eps: 60.81%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 10/-29\n",
      "Evaluating ........ Loss: 0.00843563, Base Loss: 0.366165, Lora Diff: -0.00164030, WR: 14.06%, 0epsWR: 63.28%, OShL: 0.00104108\n",
      "................Step 5101/8192\tLoss: 0.021091 OShL: 2.663e-04\tBase: 0.4470 Diff: -5.7782e-04 \tWR: 7.25% 0eps: 52.17%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 5/-33\n",
      "................Step 5170/8192\tLoss: 0.020087 OShL: 1.686e-04\tBase: 0.4083 Diff: -6.8633e-04 \tWR: 7.25% 0eps: 65.22%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 5/-24\n",
      "................Step 5236/8192\tLoss: 0.022240 OShL: 1.341e-03\tBase: 0.4305 Diff: -1.5357e-03 \tWR: 3.03% 0eps: 54.55%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 2/-30\n",
      "................Step 5307/8192\tLoss: 0.020058 OShL: 3.844e-04\tBase: 0.4160 Diff: -8.0504e-04 \tWR: 9.86% 0eps: 66.20%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 7/-24\n",
      "................Step 5381/8192\tLoss: 0.020175 OShL: 6.351e-04\tBase: 0.3981 Diff: -9.4056e-04 \tWR: 13.51% 0eps: 55.41%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 10/-33\n",
      "................Step 5451/8192\tLoss: 0.020969 OShL: 2.671e-04\tBase: 0.4173 Diff: -5.6462e-04 \tWR: 8.57% 0eps: 58.57%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 6/-28\n",
      "................Step 5516/8192\tLoss: 0.023586 OShL: 6.430e-05\tBase: 0.4180 Diff: -6.3937e-05 \tWR: 1.54% 0eps: 44.62%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 1/-36\n",
      "................Step 5586/8192\tLoss: 0.020298 OShL: 6.606e-05\tBase: 0.3869 Diff: -4.6108e-04 \tWR: 8.57% 0eps: 58.57%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 6/-29\n",
      "Evaluating ........ Loss: 0.00888760, Base Loss: 0.366165, Lora Diff: -0.00165314, WR: 14.06%, 0epsWR: 57.81%, OShL: 0.00106693\n",
      "................Step 5656/8192\tLoss: 0.019543 OShL: 3.198e-04\tBase: 0.3927 Diff: -7.8783e-04 \tWR: 8.57% 0eps: 68.57%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 6/-22\n",
      "................Step 5724/8192\tLoss: 0.020544 OShL: 4.059e-04\tBase: 0.4216 Diff: -7.8855e-04 \tWR: 5.88% 0eps: 58.82%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-28\n",
      "................Step 5795/8192\tLoss: 0.019905 OShL: 2.218e-04\tBase: 0.3933 Diff: -4.6253e-04 \tWR: 9.86% 0eps: 59.15%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 7/-29\n",
      "................Step 5866/8192\tLoss: 0.019238 OShL: 5.166e-04\tBase: 0.4536 Diff: -1.2072e-03 \tWR: 9.86% 0eps: 66.20%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 7/-24\n",
      "................Step 5935/8192\tLoss: 0.022224 OShL: 1.058e-04\tBase: 0.3699 Diff: -1.3996e-04 \tWR: 7.25% 0eps: 49.28%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 5/-35\n",
      ".................Step 6005/8192\tLoss: 0.022490 OShL: 1.054e-03\tBase: 0.4136 Diff: -1.5388e-03 \tWR: 8.57% 0eps: 55.71%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 6/-31\n",
      "................Step 6075/8192\tLoss: 0.019810 OShL: 3.679e-04\tBase: 0.3614 Diff: -8.0278e-04 \tWR: 8.57% 0eps: 64.29%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 6/-25\n",
      "................Step 6146/8192\tLoss: 0.020511 OShL: 1.686e-04\tBase: 0.4415 Diff: -6.1033e-04 \tWR: 9.86% 0eps: 63.38%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 7/-26\n",
      "Evaluating ........ Loss: 0.00872464, Base Loss: 0.366165, Lora Diff: -0.00185629, WR: 12.50%, 0epsWR: 57.81%, OShL: 0.00115078\n",
      "................Step 6218/8192\tLoss: 0.020590 OShL: 1.428e-04\tBase: 0.3690 Diff: -3.5722e-04 \tWR: 11.11% 0eps: 56.94%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 8/-30\n",
      "................Step 6287/8192\tLoss: 0.021001 OShL: 4.019e-04\tBase: 0.3746 Diff: -6.6331e-04 \tWR: 7.25% 0eps: 50.72%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 5/-34\n",
      "................Step 6356/8192\tLoss: 0.020686 OShL: 2.814e-05\tBase: 0.4073 Diff: -3.5256e-04 \tWR: 7.25% 0eps: 57.97%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 5/-29\n",
      ".................Step 6426/8192\tLoss: 0.020566 OShL: 3.818e-05\tBase: 0.3623 Diff: -4.8210e-04 \tWR: 8.57% 0eps: 58.57%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 6/-29\n",
      "................Step 6494/8192\tLoss: 0.020918 OShL: 1.867e-04\tBase: 0.4014 Diff: -4.7244e-04 \tWR: 5.88% 0eps: 57.35%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-29\n",
      "................Step 6563/8192\tLoss: 0.020486 OShL: 1.177e-04\tBase: 0.4442 Diff: -4.3193e-04 \tWR: 7.25% 0eps: 66.67%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 5/-23\n",
      "................Step 6635/8192\tLoss: 0.029292 OShL: 5.782e-04\tBase: 0.4052 Diff: -9.8816e-04 \tWR: 11.11% 0eps: 66.67%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 8/-24\n",
      "................Step 6708/8192\tLoss: 0.019370 OShL: 3.117e-04\tBase: 0.4024 Diff: -7.7272e-04 \tWR: 12.33% 0eps: 63.01%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 9/-27\n",
      "Evaluating ........ Loss: 0.01021797, Base Loss: 0.368491, Lora Diff: -0.00161032, WR: 12.50%, 0epsWR: 55.27%, OShL: 0.00111305\n",
      "................Step 6776/8192\tLoss: 0.021582 OShL: 5.781e-05\tBase: 0.3804 Diff: -2.0344e-04 \tWR: 5.88% 0eps: 51.47%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-33\n",
      "................Step 6848/8192\tLoss: 0.020465 OShL: 1.273e-04\tBase: 0.4055 Diff: -4.5834e-04 \tWR: 11.11% 0eps: 55.56%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 8/-32\n",
      "................Step 6918/8192\tLoss: 0.021048 OShL: 1.054e-04\tBase: 0.3730 Diff: -2.7037e-04 \tWR: 8.57% 0eps: 55.71%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 6/-31\n",
      "................Step 6985/8192\tLoss: 0.021441 OShL: 5.852e-05\tBase: 0.4164 Diff: -3.5602e-04 \tWR: 4.48% 0eps: 61.19%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 3/-26\n",
      "................Step 7057/8192\tLoss: 0.021281 OShL: 8.919e-04\tBase: 0.3912 Diff: -1.2072e-03 \tWR: 11.11% 0eps: 54.17%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 8/-33\n",
      "................Step 7129/8192\tLoss: 0.020588 OShL: 5.293e-04\tBase: 0.4029 Diff: -7.7980e-04 \tWR: 11.11% 0eps: 55.56%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 8/-32\n",
      "................Step 7196/8192\tLoss: 0.023162 OShL: 1.367e-04\tBase: 0.4155 Diff: -2.8429e-04 \tWR: 4.48% 0eps: 50.75%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 3/-33\n",
      "................Step 7264/8192\tLoss: 0.035204 OShL: 4.187e-04\tBase: 0.3822 Diff: -7.0540e-04 \tWR: 5.88% 0eps: 54.41%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-31\n",
      "Evaluating ........ Loss: 0.00835627, Base Loss: 0.366165, Lora Diff: -0.00180442, WR: 12.50%, 0epsWR: 59.38%, OShL: 0.00117797\n",
      "................Step 7334/8192\tLoss: 0.020835 OShL: 5.655e-04\tBase: 0.4030 Diff: -8.0709e-04 \tWR: 8.57% 0eps: 62.86%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 6/-26\n",
      "................Step 7408/8192\tLoss: 0.020086 OShL: 6.752e-04\tBase: 0.3705 Diff: -1.1387e-03 \tWR: 13.51% 0eps: 59.46%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 10/-30\n",
      "................Step 7475/8192\tLoss: 0.020574 OShL: 9.841e-05\tBase: 0.4001 Diff: -6.2249e-04 \tWR: 4.48% 0eps: 62.69%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 3/-25\n",
      "................Step 7546/8192\tLoss: 0.019116 OShL: 8.110e-04\tBase: 0.4276 Diff: -1.9095e-03 \tWR: 9.86% 0eps: 71.83%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 7/-20\n",
      "................Step 7615/8192\tLoss: 0.019972 OShL: 3.102e-04\tBase: 0.3945 Diff: -8.0322e-04 \tWR: 7.25% 0eps: 66.67%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 5/-23\n",
      "................Step 7686/8192\tLoss: 0.021758 OShL: 8.955e-04\tBase: 0.4272 Diff: -1.3987e-03 \tWR: 9.86% 0eps: 57.75%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 7/-30\n",
      "................Step 7759/8192\tLoss: 0.020209 OShL: 2.676e-04\tBase: 0.3645 Diff: -6.3064e-04 \tWR: 12.33% 0eps: 57.53%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 9/-31\n",
      "................Step 7836/8192\tLoss: 0.019310 OShL: 1.705e-03\tBase: 0.4413 Diff: -2.7155e-03 \tWR: 16.88% 0eps: 58.44%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 13/-32\n",
      "Evaluating ........ Loss: 0.00887330, Base Loss: 0.366165, Lora Diff: -0.00176127, WR: 11.72%, 0epsWR: 55.47%, OShL: 0.00118942\n",
      ".Evaluating ........ Loss: 0.00887330, Base Loss: 0.366165, Lora Diff: -0.00176127, WR: 11.72%, 0epsWR: 55.47%, OShL: 0.00118942\n",
      "................Step 7908/8192\tLoss: 0.019607 OShL: 2.531e-04\tBase: 0.4035 Diff: -8.5039e-04 \tWR: 11.11% 0eps: 65.28%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 8/-25\n",
      "................Step 7976/8192\tLoss: 0.020252 OShL: 3.583e-04\tBase: 0.3778 Diff: -7.9208e-04 \tWR: 5.88% 0eps: 64.71%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 4/-24\n",
      "................Step 8045/8192\tLoss: 0.022833 OShL: 6.720e-04\tBase: 0.4001 Diff: -1.1028e-03 \tWR: 7.25% 0eps: 47.83%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 5/-36\n",
      ".................Step 8122/8192\tLoss: 0.018815 OShL: 1.641e-03\tBase: 0.4100 Diff: -2.7725e-03 \tWR: 16.88% 0eps: 62.34%  \tLR: 2.50e-08 eps: 2.00e-02 fit: 13/-29\n",
      ".......Evaluating ........ Loss: 0.00983609, Base Loss: 0.368491, Lora Diff: -0.00157710, WR: 12.50%, 0epsWR: 57.62%, OShL: 0.00105254\n"
     ]
    }
   ],
   "source": [
    "train_data, _ = get_data(train_name=['data/cortex_40_8704.json','data/cortex_39_8704.json','data/cortex_38_8704.json'],\n",
    "                                 eval_subset=np.arange(8192, 8192+512),\n",
    "                                 eval_name='data/cortex_41_8704.json', tokenizer=tokenizer)\n",
    "train(lora_model, train_data, eval_data, \n",
    "        acc_batch_size=64,\n",
    "        lr=2.5e-8, weight_decay=0.0, lr_scheduler=\"constant\", warmup_steps=8, betas=(0.88, 0.97), squared_loss=False,\n",
    "        use_sam=True, sam_rho=0.25, do_dadapt=False,\n",
    "        loss_eps = 0.02, overshoot_buffer = -0.01,\n",
    "        manual_grad_clip_norm=0.0, manual_grad_clip_value=0.0, wait_for_full_batch=True,\n",
    "        do_base_gradient=False, add_overshoot_penalty=False, ignore_overshot_samples=True, bad_sample_mult=1.25,\n",
    "        prompt_dropout=0.0,\n",
    "        eval_steps=2048, save_steps=512, do_save=True, save_name=lora_name,\n",
    "        average_stats=False,\n",
    "        partial_eval_steps=512, partial_eval_size=128, save_n_start=0)\n",
    "gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found in eval\n",
      "................Step 146/17408\tLoss: 0.019014 OShL: 1.533e-03\tBase: 0.4047 Diff: -2.1964e-03 \tWR: 12.33% 0eps: 59.59%  \tLR: 7.81e-09 eps: 2.00e-02 fit: 18/-59\n",
      "................Step 286/17408\tLoss: 0.020845 OShL: 9.163e-04\tBase: 0.4167 Diff: -1.0542e-03 \tWR: 8.57% 0eps: 57.14%  \tLR: 1.56e-08 eps: 2.00e-02 fit: 12/-60\n",
      "................Step 425/17408\tLoss: 0.020790 OShL: 9.871e-05\tBase: 0.4187 Diff: -3.1786e-04 \tWR: 7.91% 0eps: 53.24%  \tLR: 2.34e-08 eps: 2.00e-02 fit: 11/-65\n",
      "................Step 573/17408\tLoss: 0.020117 OShL: 1.626e-03\tBase: 0.4422 Diff: -2.2701e-03 \tWR: 13.51% 0eps: 60.81%  \tLR: 3.12e-08 eps: 2.00e-02 fit: 20/-58\n",
      "Evaluating ........ Loss: 0.00872664, Base Loss: 0.366165, Lora Diff: -0.00173172, WR: 13.28%, 0epsWR: 63.28%, OShL: 0.00098392\n",
      "................Step 712/17408\tLoss: 0.020651 OShL: 1.554e-03\tBase: 0.3986 Diff: -2.0033e-03 \tWR: 7.91% 0eps: 56.12%  \tLR: 3.91e-08 eps: 2.00e-02 fit: 11/-61\n",
      "................Step 864/17408\tLoss: 0.020669 OShL: 7.522e-04\tBase: 0.3674 Diff: -9.9806e-04 \tWR: 15.79% 0eps: 58.55%  \tLR: 4.69e-08 eps: 2.00e-02 fit: 24/-62\n",
      "................Step 1017/17408\tLoss: 0.018233 OShL: 1.627e-03\tBase: 0.3974 Diff: -2.3723e-03 \tWR: 16.34% 0eps: 62.75%  \tLR: 5.47e-08 eps: 2.00e-02 fit: 25/-57\n",
      "................Step 1158/17408\tLoss: 0.019972 OShL: 1.222e-03\tBase: 0.3765 Diff: -1.7648e-03 \tWR: 9.22% 0eps: 56.74%  \tLR: 6.25e-08 eps: 2.00e-02 fit: 13/-61\n",
      "Evaluating ........ Loss: 0.00896283, Base Loss: 0.366165, Lora Diff: -0.00166888, WR: 13.28%, 0epsWR: 58.59%, OShL: 0.00100296\n",
      "................Step 1297/17408\tLoss: 0.020621 OShL: 6.305e-04\tBase: 0.4227 Diff: -1.0650e-03 \tWR: 7.91% 0eps: 49.64%  \tLR: 7.03e-08 eps: 2.00e-02 fit: 11/-70\n",
      "................Step 1436/17408\tLoss: 0.019477 OShL: 7.860e-05\tBase: 0.4123 Diff: -6.5360e-04 \tWR: 7.91% 0eps: 63.31%  \tLR: 7.81e-08 eps: 2.00e-02 fit: 11/-51\n",
      "................Step 1578/17408\tLoss: 0.019607 OShL: 5.085e-04\tBase: 0.3985 Diff: -8.4662e-04 \tWR: 9.86% 0eps: 57.75%  \tLR: 8.59e-08 eps: 2.00e-02 fit: 14/-59\n",
      "................Step 1727/17408\tLoss: 0.019568 OShL: 1.643e-03\tBase: 0.4251 Diff: -2.1700e-03 \tWR: 14.09% 0eps: 58.39%  \tLR: 9.38e-08 eps: 2.00e-02 fit: 21/-62\n",
      "Evaluating ........ Loss: 0.01053110, Base Loss: 0.366165, Lora Diff: -0.00153031, WR: 12.50%, 0epsWR: 56.25%, OShL: 0.00107297\n",
      "................Step 1876/17408\tLoss: 0.018946 OShL: 1.256e-03\tBase: 0.3874 Diff: -1.8258e-03 \tWR: 14.09% 0eps: 62.42%  \tLR: 1.02e-07 eps: 2.00e-02 fit: 21/-56\n",
      ".................Step 2026/17408\tLoss: 0.018978 OShL: 8.644e-04\tBase: 0.4503 Diff: -1.5716e-03 \tWR: 14.67% 0eps: 62.00%  \tLR: 1.09e-07 eps: 2.00e-02 fit: 22/-57\n",
      "................Step 2169/17408\tLoss: 0.020437 OShL: 5.437e-04\tBase: 0.3829 Diff: -1.0842e-03 \tWR: 10.49% 0eps: 63.64%  \tLR: 1.17e-07 eps: 2.00e-02 fit: 15/-52\n",
      "................Step 2325/17408\tLoss: 0.018729 OShL: 1.030e-03\tBase: 0.4003 Diff: -1.8984e-03 \tWR: 17.95% 0eps: 62.82%  \tLR: 1.25e-07 eps: 2.00e-02 fit: 28/-58\n",
      "Evaluating ........ Loss: 0.01105384, Base Loss: 0.368491, Lora Diff: -0.00150975, WR: 13.87%, 0epsWR: 59.77%, OShL: 0.00099670\n",
      "................Step 2469/17408\tLoss: 0.021115 OShL: 7.651e-04\tBase: 0.4027 Diff: -1.1660e-03 \tWR: 11.11% 0eps: 58.33%  \tLR: 1.33e-07 eps: 2.00e-02 fit: 16/-60\n",
      "................Step 2623/17408\tLoss: 0.019827 OShL: 9.027e-04\tBase: 0.4438 Diff: -1.4906e-03 \tWR: 16.88% 0eps: 66.23%  \tLR: 1.41e-07 eps: 2.00e-02 fit: 26/-52\n",
      "................Step 2772/17408\tLoss: 0.018601 OShL: 1.365e-03\tBase: 0.4199 Diff: -2.0522e-03 \tWR: 14.09% 0eps: 63.09%  \tLR: 1.48e-07 eps: 2.00e-02 fit: 21/-55\n",
      "................Step 2919/17408\tLoss: 0.020105 OShL: 6.231e-04\tBase: 0.3893 Diff: -9.1344e-04 \tWR: 12.93% 0eps: 53.74%  \tLR: 1.56e-07 eps: 2.00e-02 fit: 19/-68\n",
      "Evaluating ........ Loss: 0.01212312, Base Loss: 0.366165, Lora Diff: -0.00128725, WR: 16.41%, 0epsWR: 59.38%, OShL: 0.00090180\n",
      "................Step 3062/17408\tLoss: 0.026688 OShL: 5.280e-04\tBase: 0.4085 Diff: -9.5322e-04 \tWR: 10.49% 0eps: 57.34%  \tLR: 1.64e-07 eps: 2.00e-02 fit: 15/-61\n",
      "................Step 3210/17408\tLoss: 0.021029 OShL: 9.402e-04\tBase: 0.4080 Diff: -1.3156e-03 \tWR: 13.51% 0eps: 57.43%  \tLR: 1.72e-07 eps: 2.00e-02 fit: 20/-63\n",
      ".................Step 3364/17408\tLoss: 0.021379 OShL: 1.843e-03\tBase: 0.4407 Diff: -2.5354e-03 \tWR: 16.88% 0eps: 59.09%  \tLR: 1.80e-07 eps: 2.00e-02 fit: 26/-63\n",
      "................Step 3506/17408\tLoss: 0.025176 OShL: 7.017e-04\tBase: 0.4196 Diff: -1.0718e-03 \tWR: 9.86% 0eps: 55.63%  \tLR: 1.88e-07 eps: 2.00e-02 fit: 14/-63\n",
      "Evaluating ........ Loss: 0.01264645, Base Loss: 0.366165, Lora Diff: -0.00131795, WR: 12.50%, 0epsWR: 60.94%, OShL: 0.00087099\n",
      "................Step 3652/17408\tLoss: 0.020663 OShL: 6.950e-04\tBase: 0.3858 Diff: -8.7926e-04 \tWR: 12.33% 0eps: 50.68%  \tLR: 1.95e-07 eps: 2.00e-02 fit: 18/-72\n",
      "................Step 3803/17408\tLoss: 0.024455 OShL: 1.946e-03\tBase: 0.4629 Diff: -2.5180e-03 \tWR: 15.23% 0eps: 52.98%  \tLR: 2.03e-07 eps: 2.00e-02 fit: 23/-70\n",
      "................Step 3942/17408\tLoss: 0.022478 OShL: 3.225e-04\tBase: 0.3865 Diff: -3.0757e-04 \tWR: 7.91% 0eps: 56.12%  \tLR: 2.11e-07 eps: 2.00e-02 fit: 11/-61\n",
      "................Step 4091/17408\tLoss: 0.031430 OShL: 9.725e-04\tBase: 0.3837 Diff: -9.6050e-04 \tWR: 14.09% 0eps: 50.34%  \tLR: 2.19e-07 eps: 2.00e-02 fit: 21/-74\n",
      "Evaluating ........ Loss: 0.01273463, Base Loss: 0.366165, Lora Diff: -0.00144524, WR: 16.41%, 0epsWR: 58.59%, OShL: 0.00099278\n",
      "................Step 4247/17408\tLoss: 0.019261 OShL: 1.468e-03\tBase: 0.4024 Diff: -2.1440e-03 \tWR: 17.95% 0eps: 61.54%  \tLR: 2.27e-07 eps: 2.00e-02 fit: 28/-60\n",
      "................Step 4398/17408\tLoss: 0.023376 OShL: 1.203e-03\tBase: 0.4069 Diff: -1.5572e-03 \tWR: 15.23% 0eps: 50.99%  \tLR: 2.34e-07 eps: 2.00e-02 fit: 23/-74\n",
      "................Step 4551/17408\tLoss: 0.019023 OShL: 8.445e-04\tBase: 0.4338 Diff: -1.4487e-03 \tWR: 16.34% 0eps: 60.13%  \tLR: 2.42e-07 eps: 2.00e-02 fit: 25/-60\n",
      "................Step 4701/17408\tLoss: 0.023135 OShL: 9.423e-04\tBase: 0.3793 Diff: -6.9592e-04 \tWR: 14.67% 0eps: 48.67%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 22/-77\n",
      "Evaluating ........ Loss: 0.02394285, Base Loss: 0.368491, Lora Diff: -0.00096006, WR: 16.02%, 0epsWR: 51.17%, OShL: 0.00098021\n",
      "................Step 4847/17408\tLoss: 0.051862 OShL: 5.801e-04\tBase: 0.4049 Diff: -4.6332e-04 \tWR: 12.33% 0eps: 47.26%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 18/-76\n",
      "................Step 4994/17408\tLoss: 0.033694 OShL: 1.051e-03\tBase: 0.3663 Diff: -9.0162e-04 \tWR: 12.93% 0eps: 48.98%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 19/-74\n",
      ".................Step 5144/17408\tLoss: 0.029350 OShL: 9.952e-04\tBase: 0.4070 Diff: -9.6647e-04 \tWR: 14.67% 0eps: 47.33%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 22/-78\n",
      "................Step 5293/17408\tLoss: 0.024276 OShL: 6.259e-04\tBase: 0.4198 Diff: -7.1683e-04 \tWR: 14.09% 0eps: 54.36%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 21/-68\n",
      "Evaluating ........ Loss: 0.01187604, Base Loss: 0.366165, Lora Diff: -0.00137759, WR: 17.19%, 0epsWR: 51.56%, OShL: 0.00118324\n",
      "................Step 5440/17408\tLoss: 0.043835 OShL: 5.907e-04\tBase: 0.3855 Diff: -5.6469e-05 \tWR: 12.93% 0eps: 53.74%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 19/-68\n",
      ".................Step 5594/17408\tLoss: 0.044663 OShL: 8.638e-04\tBase: 0.4007 Diff: -5.2159e-04 \tWR: 16.88% 0eps: 53.25%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 26/-72\n",
      "................Step 5755/17408\tLoss: 0.034701 OShL: 1.947e-03\tBase: 0.4082 Diff: -1.8386e-03 \tWR: 20.50% 0eps: 49.07%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 33/-82\n",
      "................Step 5903/17408\tLoss: 0.025950 OShL: 1.222e-03\tBase: 0.4125 Diff: -7.6548e-04 \tWR: 13.51% 0eps: 50.68%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 20/-73\n",
      "Evaluating ........ Loss: 0.01327111, Base Loss: 0.366165, Lora Diff: -0.00160795, WR: 15.62%, 0epsWR: 47.66%, OShL: 0.00159259\n",
      "................Step 6055/17408\tLoss: 0.051640 OShL: 1.563e-03\tBase: 0.4204 Diff: -1.0125e-03 \tWR: 15.79% 0eps: 49.34%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 24/-77\n",
      "................Step 6203/17408\tLoss: 0.020612 OShL: 8.798e-04\tBase: 0.3939 Diff: -1.2163e-03 \tWR: 13.51% 0eps: 57.43%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 20/-63\n",
      "................Step 6350/17408\tLoss: 0.085000 OShL: 8.931e-04\tBase: 0.4081 Diff: -2.9863e-04 \tWR: 12.93% 0eps: 48.98%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 19/-75\n",
      "................Step 6508/17408\tLoss: 0.048802 OShL: 9.952e-04\tBase: 0.4194 Diff: 1.8752e-04 \tWR: 18.99% 0eps: 45.57%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 30/-86\n",
      "Evaluating ........ Loss: 0.01288057, Base Loss: 0.366165, Lora Diff: -0.00164724, WR: 16.41%, 0epsWR: 53.91%, OShL: 0.00169839\n",
      "................Step 6654/17408\tLoss: 0.026707 OShL: 1.153e-03\tBase: 0.4029 Diff: -1.1941e-03 \tWR: 12.33% 0eps: 46.58%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 18/-77\n",
      ".................Step 6808/17408\tLoss: 0.046648 OShL: 1.566e-03\tBase: 0.4050 Diff: -1.0081e-03 \tWR: 16.88% 0eps: 48.05%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 26/-80\n",
      ".................Step 6956/17408\tLoss: 0.022857 OShL: 2.246e-03\tBase: 0.4572 Diff: -1.9807e-03 \tWR: 13.51% 0eps: 45.27%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 20/-81\n",
      "................Step 7109/17408\tLoss: 0.028648 OShL: 7.526e-04\tBase: 0.4135 Diff: -2.3560e-04 \tWR: 16.34% 0eps: 41.18%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 25/-90\n",
      "Evaluating ........ Loss: 0.03217764, Base Loss: 0.368491, Lora Diff: -0.00090434, WR: 18.36%, 0epsWR: 48.83%, OShL: 0.00146905\n",
      "................Step 7267/17408\tLoss: 0.023613 OShL: 1.598e-03\tBase: 0.4136 Diff: -1.1813e-03 \tWR: 18.99% 0eps: 45.57%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 30/-86\n",
      "................Step 7423/17408\tLoss: 0.092700 OShL: 1.610e-03\tBase: 0.4145 Diff: 3.8572e-04 \tWR: 17.95% 0eps: 48.08%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 28/-81\n",
      "................Step 7565/17408\tLoss: 0.027615 OShL: 4.758e-04\tBase: 0.4316 Diff: 4.9399e-04 \tWR: 9.86% 0eps: 40.14%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 14/-85\n",
      "................Step 7721/17408\tLoss: 0.028664 OShL: 9.926e-04\tBase: 0.3852 Diff: -2.2094e-04 \tWR: 17.95% 0eps: 44.87%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 28/-86\n",
      "Evaluating ........ Loss: 0.01481177, Base Loss: 0.366165, Lora Diff: -0.00155444, WR: 19.53%, 0epsWR: 52.34%, OShL: 0.00169584\n",
      "................Step 7873/17408\tLoss: 0.030829 OShL: 1.946e-03\tBase: 0.4119 Diff: -1.2883e-03 \tWR: 15.79% 0eps: 40.79%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 24/-90\n",
      "................Step 8021/17408\tLoss: 0.029068 OShL: 1.099e-03\tBase: 0.4118 Diff: -1.6716e-04 \tWR: 13.51% 0eps: 43.24%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 20/-84\n",
      "................Step 8185/17408\tLoss: 0.113443 OShL: 2.226e-03\tBase: 0.4072 Diff: -1.8040e-03 \tWR: 21.95% 0eps: 51.22%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 36/-79\n",
      "................Step 8335/17408\tLoss: 0.029864 OShL: 2.168e-03\tBase: 0.4368 Diff: -2.0992e-03 \tWR: 14.67% 0eps: 44.67%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 22/-82\n",
      "Evaluating ........ Loss: 0.01568829, Base Loss: 0.366165, Lora Diff: -0.00155179, WR: 17.97%, 0epsWR: 46.09%, OShL: 0.00183539\n",
      "................Step 8486/17408\tLoss: 0.049184 OShL: 1.080e-03\tBase: 0.4258 Diff: -7.3339e-04 \tWR: 15.23% 0eps: 45.03%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 23/-83\n",
      "................Step 8642/17408\tLoss: 0.030634 OShL: 2.192e-03\tBase: 0.3819 Diff: -2.0485e-03 \tWR: 17.95% 0eps: 48.08%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 28/-81\n",
      "................Step 8791/17408\tLoss: 0.028020 OShL: 1.289e-03\tBase: 0.3578 Diff: -1.0182e-03 \tWR: 14.09% 0eps: 49.66%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 21/-75\n",
      "................Step 8945/17408\tLoss: 0.037564 OShL: 2.815e-03\tBase: 0.4484 Diff: -2.6163e-03 \tWR: 16.88% 0eps: 48.70%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 26/-79\n",
      "Evaluating ........ Loss: 0.01539702, Base Loss: 0.366165, Lora Diff: -0.00161402, WR: 17.19%, 0epsWR: 48.44%, OShL: 0.00181897\n",
      "................Step 9102/17408\tLoss: 0.064567 OShL: 2.062e-03\tBase: 0.4048 Diff: -1.5811e-03 \tWR: 18.47% 0eps: 50.32%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 29/-78\n",
      "................Step 9257/17408\tLoss: 0.035792 OShL: 8.052e-04\tBase: 0.4131 Diff: -2.6398e-04 \tWR: 17.42% 0eps: 53.55%  \tLR: 2.50e-07 eps: 2.00e-02 fit: 27/-72\n",
      "......."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m train_data, _ \u001b[38;5;241m=\u001b[39m get_data(train_name\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/cortex_39_8704.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/cortex_38_8704.json\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      2\u001b[0m                                  eval_subset\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m8192\u001b[39m, \u001b[38;5;241m8192\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m512\u001b[39m),\n\u001b[1;32m      3\u001b[0m                                  eval_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/cortex_41_8704.json\u001b[39m\u001b[38;5;124m'\u001b[39m, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43macc_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.5e-7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconstant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msam_rho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_dadapt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_eps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43movershoot_buffer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmanual_grad_clip_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanual_grad_clip_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_full_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_base_gradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_overshoot_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_overshot_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbad_sample_mult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial_eval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial_eval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_n_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect(); torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[2], line 239\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_d, eval_d, acc_batch_size, lr, weight_decay, lr_scheduler, warmup_steps, betas, squared_loss, use_sam, sam_rho, do_dadapt, manual_grad_clip_norm, manual_grad_clip_value, wait_for_full_batch, do_base_gradient, add_overshoot_penalty, ignore_overshot_samples, bad_sample_mult, loss_eps, overshoot_buffer, prompt_dropout, eval_steps, save_steps, save_name, do_save, average_stats, partial_eval_steps, partial_eval_size, save_n_start)\u001b[0m\n\u001b[1;32m    236\u001b[0m current_loss_eps \u001b[38;5;241m=\u001b[39m loss_eps \u001b[38;5;66;03m# ((final_loss_eps * (min(steps_so_far * 8 / len(train_d), 1.0))) + initial_loss_eps) / (1.0 + initial_loss_eps)\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# current_loss_eps = 0.011 # lr_scheduler.get_last_lr()[0] * 100.0 # * 3.33\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m loss, overshoot_penalty \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m acc_batch_size\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_overshot_samples \u001b[38;5;129;01mor\u001b[39;00m overshoot_penalty \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data, _ = get_data(train_name=['data/cortex_39_8704.json','data/cortex_38_8704.json'],\n",
    "                                 eval_subset=np.arange(8192, 8192+512),\n",
    "                                 eval_name='data/cortex_41_8704.json', tokenizer=tokenizer)\n",
    "train(lora_model, train_data, eval_data, \n",
    "        acc_batch_size=128,\n",
    "        lr=2.5e-7, weight_decay=0.0, lr_scheduler=\"constant\", warmup_steps=32, betas=(0.9, 0.99), squared_loss=False,\n",
    "        use_sam=True, sam_rho=0.5, do_dadapt=False,\n",
    "        loss_eps = 0.02, overshoot_buffer = -0.01,\n",
    "        manual_grad_clip_norm=0.0, manual_grad_clip_value=0.0, wait_for_full_batch=True,\n",
    "        do_base_gradient=False, add_overshoot_penalty=False, ignore_overshot_samples=True, bad_sample_mult=1.15,\n",
    "        prompt_dropout=0.0,\n",
    "        eval_steps=2048, save_steps=512, do_save=True, save_name=lora_name,\n",
    "        average_stats=False,\n",
    "        partial_eval_steps=512, partial_eval_size=128, save_n_start=0)\n",
    "gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ........0.012056129032998797\n",
      " Loss: 0.01205613, Base Loss: 0.365259, Lora Diff: -0.00147909, WR: 13.87%, 0epsWR: 58.01%, OShL: 0.00109561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.012056129032998797,\n",
       " 'base_loss': 0.36525918503184585,\n",
       " 'lora_diff': -0.0014790902009735873,\n",
       " 'head_to_head': 13.8671875,\n",
       " 'eps0_head_to_head': 58.0078125,\n",
       " 'overshoot': 0.0010956073215311335}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(lora_model, eval_data, return_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model, lora_model, tokenizer, train_data, eval_data, trainer; gc.collect(); torch.cuda.empty_cache()\n",
    "# gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/alyx/finetuning-subnet/merged_model/tokenizer_config.json',\n",
       " '/home/alyx/finetuning-subnet/merged_model/special_tokens_map.json',\n",
       " '/home/alyx/finetuning-subnet/merged_model/tokenizer.model',\n",
       " '/home/alyx/finetuning-subnet/merged_model/added_tokens.json',\n",
       " '/home/alyx/finetuning-subnet/merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lora_model = lora_model.merge_and_unload()\n",
    "# lora_model.config.name_or_path = \"MesozoicMetallurgist/new_model\"\n",
    "# model_dir = \"Models/merged_model\"\n",
    "# model_dir = os.path.expanduser(\"~/finetuning-subnet/merged_model\")\n",
    "# if not os.path.exists(model_dir):\n",
    "#     os.makedirs(model_dir, exist_ok=True)\n",
    "# else:\n",
    "#     # wipe the directory\n",
    "#     for file in os.listdir(model_dir):\n",
    "#         os.remove(os.path.join(model_dir, file))\n",
    "# lora_model.save_pretrained(save_directory=model_dir, safe_serialization=True)\n",
    "# tokenizer.save_pretrained(save_directory=model_dir)\n",
    "\n",
    "# del lora_model, trainer, model, tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
