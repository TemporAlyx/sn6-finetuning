{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel,\n",
    "    LoHaConfig,\n",
    ")\n",
    "\n",
    "from cortexsubsetloader import tokenize\n",
    "from pytorch_optimizer import Ranger21, Lamb, DAdaptLion, SAM, DAdaptAdam, LOMO, SophiaH\n",
    "from pytorch_optimizer.optimizer.sam import WSAM\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.37.2\n",
    "# !pip install -U --force-reinstall transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(train, eval):\n",
    "    evalf = []\n",
    "    for f in eval:\n",
    "        if f not in train:\n",
    "            evalf.append(f)\n",
    "    if len(evalf) < len(eval): print(f\"Removed {len(eval)-len(evalf)} duplicates from eval\")\n",
    "    else: print(\"No duplicates found in eval\")\n",
    "    return evalf\n",
    "\n",
    "\n",
    "def data_collator(features):\n",
    "    batches = []\n",
    "    for feature in features:\n",
    "        inputs, prompt_len = feature\n",
    "        data = [inputs]\n",
    "        b_labels = inputs.clone()\n",
    "        b_labels[:, :prompt_len] = -100\n",
    "        labels = [b_labels]\n",
    "            \n",
    "        batch = {}\n",
    "        batch['input_ids'] = torch.concat(data)\n",
    "        batch['labels'] = torch.concat(labels)\n",
    "        batches.append(batch)\n",
    "    return batches\n",
    "\n",
    "\n",
    "def get_data(train_name, eval_name, tokenizer, train_subset=None, eval_subset=None, shuffle=True, extend_train_length=0):\n",
    "    if type(train_name) == str:\n",
    "        with open(train_name, \"r\") as f:\n",
    "            train_data = np.array(json.load(f))\n",
    "    else: # list of str\n",
    "        train_data = []\n",
    "        for name in train_name:\n",
    "            with open(name, \"r\") as f:\n",
    "                train_data.extend(json.load(f))\n",
    "        train_data = np.array(train_data)\n",
    "    with open(eval_name, \"r\") as f:\n",
    "        eval_data = np.array(json.load(f))\n",
    "\n",
    "    if train_subset is not None:\n",
    "        train_data = train_data[train_subset]\n",
    "    if eval_subset is not None:\n",
    "        eval_data = eval_data[-eval_subset:]\n",
    "    \n",
    "    eval_data = filter_data(train_data, eval_data)\n",
    "\n",
    "    if shuffle:\n",
    "        p = np.random.permutation(len(train_data))\n",
    "        train_data = train_data[p]\n",
    "\n",
    "    train_data = tokenize(tokenizer, train_data, 2048+extend_train_length)\n",
    "    eval_data = tokenize(tokenizer, eval_data, 2048)\n",
    "\n",
    "    train_data = data_collator(train_data)\n",
    "    eval_data = data_collator(eval_data)\n",
    "\n",
    "    return train_data, eval_data\n",
    "\n",
    "# initial_loss_eps = 0.0001\n",
    "intermed_check_step_split = 8\n",
    "\n",
    "\n",
    "def simple_eval(model, eval_d):\n",
    "    print(\"Evaluating\", end=\" \")\n",
    "    model = model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    steps_so_far = 1\n",
    "    for batch in eval_d:\n",
    "        inputs = batch['input_ids'].to(\"cuda\")\n",
    "        labels = batch['labels'].to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            eval_loss += outputs.loss.item() / len(eval_d)\n",
    "        if steps_so_far % (len(eval_d) // intermed_check_step_split) == 0:\n",
    "            print(\".\", end=\"\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "        steps_so_far += 1\n",
    "    model = model.to(\"cpu\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    print(f\" Loss: {eval_loss:.8f}\")\n",
    "\n",
    "def evaluate(model, eval_d, return_to_cpu=False, return_stats=False, print_stats=True, \n",
    "             base_model=None, precompute_base_loss=True, device=\"cuda\", instruction_finetuning=True, true_eps=0.01):\n",
    "    print(\"Evaluating\", end=\" \")\n",
    "    model = model.to(\"cuda\")\n",
    "    eval_loss = 0\n",
    "    model.eval()\n",
    "    steps_so_far = 1\n",
    "\n",
    "    for batch in eval_d:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        loss = model(inputs, labels=labels).loss\n",
    "\n",
    "        eval_loss += loss.item() / len(eval_d)\n",
    "\n",
    "        if steps_so_far % (len(eval_d) // intermed_check_step_split) == 0:\n",
    "            print(\".\", end=\"\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "        steps_so_far += 1\n",
    "\n",
    "    if return_to_cpu:\n",
    "        model = model.to(\"cpu\")\n",
    "\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    data = {\n",
    "        \"loss\": eval_loss,\n",
    "    }\n",
    "\n",
    "    if print_stats:\n",
    "        print(f\" Loss: {eval_loss:.8f}\")\n",
    "    if return_stats:\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_d, eval_d, base_model=None, inf_training=False, training_device=\"cuda\",\n",
    "            acc_batch_size=32, instruction_finetuing=True, precalculate_base_loss=True, precalculate_batch_mult=1.5,\n",
    "            lr=1e-4, weight_decay=0.001, lr_scheduler=\"constant\", warmup_steps=0, betas=(0.9, 0.99), \n",
    "            use_sam=False, sam_rho=0.05, opt=\"adamw\",\n",
    "            manual_grad_clip_norm=0.0, wait_for_full_batch=True, sam_reuse_base_outputs=False,\n",
    "            do_base_gradient=True, add_overshoot_penalty=True, ignore_overshot_samples=True, bad_sample_mult=1.0,\n",
    "            loss_eps = 0.015, overshoot_buffer = 0.01, true_eps=0.01,\n",
    "            prompt_dropout=0.0,\n",
    "            eval_steps=1024, save_steps=1024, save_name=\"lora\", do_save=True,\n",
    "            average_stats=False,\n",
    "            partial_eval_steps=0, partial_eval_size=128, save_n_start=0,\n",
    "            gradient_checkpointing=False, excessive_cache_clearing=False):\n",
    "    if warmup_steps is None:\n",
    "        warmup_steps = (eval_steps // acc_batch_size) // 2\n",
    "    LORA = True\n",
    "    if base_model is not None:\n",
    "        LORA = False\n",
    "        # base_model = base_model.to(\"cuda\")\n",
    "\n",
    "    if inf_training:\n",
    "        # from subsetfalconloader import SubsetFalconLoader\n",
    "        pass\n",
    "\n",
    "    if precalculate_base_loss and do_base_gradient:\n",
    "        raise ValueError(\"Precalculating base loss will disconnect base gradients\")\n",
    "\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    model = model.to(\"cuda\")\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "    if gradient_checkpointing:\n",
    "        model.config.use_cache = False\n",
    "        grad_check_kwargs = {\"use_reentrant\": False}\n",
    "        if do_base_gradient:\n",
    "            grad_check_kwargs[\"use_reentrant\"] = True\n",
    "        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=grad_check_kwargs)\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    if not use_sam:\n",
    "        if opt == \"dadapt_adam\":\n",
    "            optimizer = DAdaptAdam(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay, fixed_decay=True)\n",
    "        elif opt == \"sophia\":\n",
    "            optimizer = SophiaH(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        elif opt == \"ranger\":\n",
    "            optimizer = Ranger21(model.parameters(), num_iterations=1, lr=lr, betas=betas, weight_decay=weight_decay,\n",
    "                                 num_warm_up_iterations=0, num_warm_down_iterations=0)\n",
    "        elif opt == \"adamw\" or opt == \"adam\":\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        elif opt == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=betas[0], weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer {opt}\")\n",
    "    else:\n",
    "        base_optimizer_args = {\"lr\": lr, \"weight_decay\": weight_decay, \"betas\": betas, \"eps\": 1e-8}\n",
    "\n",
    "        if opt == \"dadapt_adam\":\n",
    "            base_optimizer = DAdaptAdam\n",
    "        elif opt == \"sophia\":\n",
    "            base_optimizer = SophiaH\n",
    "        elif opt == \"ranger\":\n",
    "            base_optimizer = Ranger21\n",
    "            base_optimizer_args[\"num_iterations\"] = 1\n",
    "            base_optimizer_args[\"num_warm_up_iterations\"] = 0\n",
    "            base_optimizer_args[\"num_warm_down_iterations\"] = 0\n",
    "        elif opt == \"adamw\" or opt == \"adam\":\n",
    "            base_optimizer = torch.optim.AdamW\n",
    "        elif opt == \"sgd\":\n",
    "            base_optimizer = torch.optim.SGD\n",
    "            del base_optimizer_args[\"betas\"], base_optimizer_args[\"eps\"]\n",
    "            base_optimizer_args[\"momentum\"] = betas[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer {opt}\")\n",
    "\n",
    "        optimizer = SAM(model.parameters(), base_optimizer=base_optimizer, rho=sam_rho, adaptive=True, **base_optimizer_args)\n",
    "        sam_optimizer = optimizer\n",
    "\n",
    "\n",
    "    if lr_scheduler == \"cosine\":\n",
    "        lr_scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, warmup_steps, (len(train_d)//acc_batch_size)+warmup_steps)\n",
    "    else:\n",
    "        lr_scheduler = transformers.get_constant_schedule_with_warmup(optimizer, warmup_steps)\n",
    "    lr_scheduler.step() # don't want to start at 0\n",
    "\n",
    "\n",
    "    steps_so_far = 1 # start at one to avoid all the modulo checks\n",
    "    epoch_loss = 0; epoch_overshoot = 0; epoch_base_loss = 0; lora_diff = 0\n",
    "    epoch_wr = 0; epoch_0eps_wr = 0\n",
    "    fit_samples = 0; unfit_samples = 0\n",
    "    sam_batch = []\n",
    "    # lomo_batch_loss = []\n",
    "    accum_steps = 0\n",
    "    true_steps_taken = 0; prev_dot_step = -1\n",
    "    last_tst = true_steps_taken\n",
    "    sam_saved_base_outputs = []\n",
    "    precalculated_base_outputs = []\n",
    "    while len(train_d) > 0:\n",
    "\n",
    "        batch = train_d.pop(0)\n",
    "        inputs = batch['input_ids'].to(training_device)\n",
    "        labels = batch['labels'].to(training_device)\n",
    "\n",
    "        outputs_loss = model(inputs, labels=labels).loss\n",
    "\n",
    "        # current_loss_eps = final_loss_eps \n",
    "        current_loss_eps = loss_eps # ((final_loss_eps * (min(steps_so_far * 8 / len(train_d), 1.0))) + initial_loss_eps) / (1.0 + initial_loss_eps)\n",
    "        # current_loss_eps = 0.011 # lr_scheduler.get_last_lr()[0] * 100.0 # * 3.33\n",
    "\n",
    "        loss = outputs_loss\n",
    "        loss = loss / acc_batch_size\n",
    "\n",
    "        loss.backward()\n",
    "            \n",
    "        accum_steps += 1\n",
    "        true_steps_taken += 1\n",
    "        if use_sam:\n",
    "            sam_batch.append((inputs, labels))\n",
    "\n",
    "        epoch_loss += loss.detach().item() * acc_batch_size\n",
    "\n",
    "        if (true_steps_taken % (acc_batch_size // intermed_check_step_split) == 0) and accum_steps != prev_dot_step:\n",
    "            prev_dot_step = accum_steps\n",
    "            print(\".\", end=\"\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "        if (((steps_so_far % acc_batch_size == 0 or steps_so_far == len(train_d)) and not wait_for_full_batch) or \n",
    "                                                                            (wait_for_full_batch and accum_steps == acc_batch_size)):\n",
    "            if manual_grad_clip_norm > 0.0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), manual_grad_clip_norm)\n",
    "\n",
    "            if not use_sam:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                sub_steps = 1\n",
    "                sam_optimizer.first_step(zero_grad=True)\n",
    "                \n",
    "                for inputs, labels in sam_batch:\n",
    "                    outputs = model(inputs, labels=labels)\n",
    "\n",
    "                    loss = outputs.loss\n",
    "                    loss = loss / accum_steps\n",
    "                    \n",
    "                    loss.backward()\n",
    "\n",
    "                    sub_steps += 1\n",
    "                    if sub_steps % (acc_batch_size // intermed_check_step_split) == 0:\n",
    "                        print(\".\", end=\"\")\n",
    "                    if excessive_cache_clearing:\n",
    "                        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "                sam_optimizer.second_step(zero_grad=True)\n",
    "                sam_batch = []\n",
    "\n",
    "            if average_stats:\n",
    "                stat_steps = steps_so_far\n",
    "            else:\n",
    "                stat_steps = accum_steps\n",
    "            print(f\"Step {steps_so_far}/{len(train_d)}\\tLoss: {epoch_loss/stat_steps:.6f}\\t\",\"LR:\", lr_scheduler.get_last_lr()[0])\n",
    "\n",
    "            if not average_stats:\n",
    "                epoch_loss = 0\n",
    "            accum_steps = 0\n",
    "            \n",
    "            lr_scheduler.step()\n",
    "\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        if do_save and true_steps_taken % save_steps == 0 and len(train_d) > 0 and true_steps_taken != last_tst:\n",
    "            model.save_pretrained(save_name + '_' + str((true_steps_taken // save_steps) + save_n_start).format(\"02d\"))\n",
    "            \n",
    "        do_full_eval = true_steps_taken % eval_steps == 0 and len(train_d) > 0 and true_steps_taken != last_tst\n",
    "        if do_full_eval:\n",
    "            evaluate(model, eval_d, base_model=base_model, device=training_device, instruction_finetuning=instruction_finetuing, true_eps=true_eps)\n",
    "            model.train()\n",
    "        if ((partial_eval_steps > 0 and true_steps_taken % partial_eval_steps == 0) and not do_full_eval \n",
    "                                            and len(train_d) > 0 and true_steps_taken != last_tst):\n",
    "            evaluate(model, eval_d[:partial_eval_size], base_model=base_model, device=training_device, \n",
    "                     instruction_finetuning=instruction_finetuing, true_eps=true_eps)\n",
    "            model.train()\n",
    "        \n",
    "        steps_so_far += 1\n",
    "        last_tst = true_steps_taken\n",
    "\n",
    "        if excessive_cache_clearing:\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    if do_save:\n",
    "        if save_n_start > 0:\n",
    "            model.save_pretrained(save_name+\"_X\"+str(save_n_start))\n",
    "        else:\n",
    "            model.save_pretrained(save_name)\n",
    "\n",
    "    model.eval()\n",
    "    final_eval_stats = evaluate(model, eval_d, return_stats=True, base_model=base_model, device=training_device, \n",
    "                                instruction_finetuning=instruction_finetuing, true_eps=true_eps)\n",
    "\n",
    "    model = model.to(\"cpu\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    return final_eval_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89041aeab7ec40d587c35e69729d53e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lora_name = \"fulltune5\"\n",
    "model_name = \"fulltune4\"\n",
    "\n",
    "neft_noise = 0.0 # bad actually?\n",
    "\n",
    "\n",
    "rank=256\n",
    "config = LoraConfig(\n",
    "    r=rank, lora_alpha=32,\n",
    "    target_modules=[\n",
    "                    'q_proj',\n",
    "                    'v_proj', \n",
    "                    \"k_proj\", \n",
    "                    \"o_proj\", \n",
    "                    \"gate_proj\", \n",
    "                    \"up_proj\", \n",
    "                    \"down_proj\"\n",
    "                    ],  #   , \n",
    "    lora_dropout=0.0,\n",
    "    bias=\"all\", \n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    use_rslora=True,\n",
    "    # init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "# config = LoHaConfig(\n",
    "#     r=rank, \n",
    "#     alpha=rank,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"], #,  , \"up_proj\" # , \"o_proj\" , \"k_proj\", \"down_proj\"\n",
    "#     rank_dropout=0.0,\n",
    "#     module_dropout=0.0,\n",
    "#     init_weights=True,\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "params = {\n",
    "    'low_cpu_mem_usage': True,\n",
    "    'trust_remote_code': False,\n",
    "    'torch_dtype': torch.bfloat16,\n",
    "    'use_safetensors': True,\n",
    "    'attn_implementation': \"flash_attention_2\"\n",
    "}\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, **params, cache_dir=\"Models\")\n",
    "\n",
    "last_q = None\n",
    "last_v = None\n",
    "last_up = None\n",
    "for name, param in model.named_parameters():\n",
    "    if \"q_proj\" in name:\n",
    "        last_q = param\n",
    "    elif \"k_proj\" in name:\n",
    "        # print(last_q.data.shape, param.data.shape)\n",
    "        mult = torch.sqrt(torch.mean(torch.abs(last_q.data), dim=1, keepdim=True).transpose(0, 1) / torch.mean(torch.abs(param.data), dim=0))\n",
    "        mult = torch.mean(mult)\n",
    "        # mult = torch.sqrt(torch.mean(torch.abs(last_q.data)) / torch.mean(torch.abs(param.data)))\n",
    "        # print(mult.shape)\n",
    "        last_q.data = last_q.data / mult\n",
    "        param.data = param.data * mult\n",
    "        last_q = None\n",
    "        # print(mult)\n",
    "    if \"v_proj\" in name:\n",
    "        last_v = param\n",
    "    elif \"o_proj\" in name:\n",
    "        # print(last_v.data.shape, param.data.shape)\n",
    "        mult = torch.sqrt(torch.mean(torch.abs(last_v.data), dim=0, keepdim=True) / torch.mean(torch.abs(param.data), dim=1))\n",
    "        mult = torch.mean(mult)\n",
    "        # mult = torch.sqrt(torch.mean(torch.abs(last_v.data)) / torch.mean(torch.abs(param.data)))\n",
    "        # print(mult.shape)\n",
    "        last_v.data = last_v.data / mult\n",
    "        param.data = param.data * mult#.transpose(0, 1)\n",
    "        last_v = None\n",
    "        # print(mult)\n",
    "    if \"up_proj\" in name:\n",
    "        last_up = param\n",
    "    elif \"down_proj\" in name:\n",
    "        # print(last_up.data.shape, param.data.shape)\n",
    "        mult = torch.sqrt(torch.mean(torch.abs(last_up.data), dim=1, keepdim=True).transpose(0, 1) / torch.mean(torch.abs(param.data), dim=0))\n",
    "        # mult = torch.sqrt(torch.mean(torch.abs(last_up.data)) / torch.mean(torch.abs(param.data)))\n",
    "        # print(mult.shape)\n",
    "        last_up.data = last_up.data / mult.transpose(0, 1)\n",
    "        param.data = param.data * mult\n",
    "        last_up = None\n",
    "        # print(mult)\n",
    "\n",
    "lora_model = model\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if (\"up_proj\" not in name \n",
    "        and \"down_proj\" not in name \n",
    "        and \"gate_proj\" not in name \n",
    "        # and \"o_proj\" not in name \n",
    "        # and \"k_proj\" not in name \n",
    "        # and \"v_proj\" not in name \n",
    "        # and \"q_proj\" not in name\n",
    "    ):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "# lora_model = PeftModel.from_pretrained(model, model_id=\"Ordovician\", is_trainable=True)\n",
    "# lora_model = PeftModel.from_pretrained(model, model_id=\"Silurian\", is_trainable=True)\n",
    "# lora_model = lora_model.to(\"cuda\")\n",
    "# # lora_model = lora_model.merge_and_unload(progressbar=True)\n",
    "# lora_model = get_peft_model(model, config)\n",
    "# # # lora_model.to(torch.bfloat16)\n",
    "# lora_model.print_trainable_parameters()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", trust_remote_code=False, use_fast=True, cache_dir=\"Models\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=False, use_fast=True, cache_dir=\"Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in lora_model.named_parameters():\n",
    "    if \"lora\" not in name:\n",
    "        print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 181 duplicates from eval\n"
     ]
    }
   ],
   "source": [
    "train_data, eval_data = get_data(train_name=['data/cortex_548_6656.json','data/cortex_547_6656.json',\n",
    "                                            #  'data/cortex_542_6656.json',\n",
    "# 'data/cortex_503_6656.json','data/cortex_506_6656.json','data/cortex_509_6656.json',\n",
    "# 'data/cortex_370_6656.json','data/cortex_369_6656.json','data/cortex_367_6656.json'\n",
    "],\n",
    "                                 train_subset=np.arange(0, (6656*2)-512), eval_subset=512,\n",
    "                                 eval_name='data/cortex_547_6656.json', tokenizer=tokenizer, shuffle=True)\n",
    "# evaluate(lora_model, eval_data, return_to_cpu=True, print_stats=True, base_model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ........ Loss: 0.63273114\n"
     ]
    }
   ],
   "source": [
    "evaluate(lora_model, eval_data, return_to_cpu=True, print_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........Step 512/12288\tLoss: 0.594686\t LR: 3.1873835221031644e-05\n",
      "........Step 1024/11776\tLoss: 0.606488\t LR: 3.14973305780581e-05\n",
      "........Step 1536/11264\tLoss: 0.602072\t LR: 3.087642377421202e-05\n",
      "........Step 2048/10752\tLoss: 0.590941\t LR: 3.0020906880701815e-05\n",
      "Evaluating ........ Loss: 0.63482267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fcd2c3a0250>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alyx/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "train(lora_model, train_data, eval_data, base_model=None, inf_training=False, training_device=\"cuda\",\n",
    "        acc_batch_size=512, instruction_finetuing=True, precalculate_base_loss=True, precalculate_batch_mult=2.1,\n",
    "        lr=3.2e-5, weight_decay=0.0, lr_scheduler=\"cosine\", warmup_steps=0, betas=(0.8, 0.95),\n",
    "        use_sam=False, sam_rho=0.1, opt=\"adam\",\n",
    "        loss_eps = 0.02, overshoot_buffer = -0.01, true_eps=0.01,\n",
    "        manual_grad_clip_norm=0.0, wait_for_full_batch=True, sam_reuse_base_outputs=True,\n",
    "        do_base_gradient=False, add_overshoot_penalty=False, ignore_overshot_samples=True, bad_sample_mult=1.01,\n",
    "        prompt_dropout=0.0,\n",
    "        eval_steps=2048, save_steps=2048, do_save=True, save_name=lora_name,\n",
    "        average_stats=False,\n",
    "        partial_eval_steps=0, partial_eval_size=128, save_n_start=0,\n",
    "        # gradient_checkpointing=False, excessive_cache_clearing=False)\n",
    "        gradient_checkpointing=True, excessive_cache_clearing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ........ Loss: 0.00147147, Base Loss: 1.243601, Lora Diff: -0.01991747, WR: 62.70%, 0epsWR: 97.75%, OShL: 0.00919830\n"
     ]
    }
   ],
   "source": [
    "evaluate(lora_model, eval_data, return_to_cpu=True, print_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(lora_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/alyx/finetuning-subnet/merged_model/tokenizer_config.json',\n",
       " '/home/alyx/finetuning-subnet/merged_model/special_tokens_map.json',\n",
       " '/home/alyx/finetuning-subnet/merged_model/tokenizer.model',\n",
       " '/home/alyx/finetuning-subnet/merged_model/added_tokens.json',\n",
       " '/home/alyx/finetuning-subnet/merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model = lora_model.to(\"cuda\")\n",
    "lora_model = lora_model.merge_and_unload()\n",
    "lora_model.config.name_or_path = \"MesozoicMetallurgist/new_model\"\n",
    "# model_dir = \"Models/merged_model\"\n",
    "model_dir = os.path.expanduser(\"~/finetuning-subnet/merged_model\")\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "else:\n",
    "    # wipe the directory\n",
    "    for f in os.listdir(model_dir):\n",
    "        os.remove(os.path.join(model_dir, f))\n",
    "lora_model.save_pretrained(save_directory=model_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(save_directory=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(lora_name + '_' + \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ........ Loss: 0.00202340, Base Loss: 1.398741, Lora Diff: -0.06646919, WR: 81.64%, 0epsWR: 92.29%, OShL: 0.05393791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.0020233988761901855,\n",
       " 'base_loss': 1.3987407684326172,\n",
       " 'lora_diff': -0.06646919250488281,\n",
       " 'head_to_head': 81.640625,\n",
       " 'eps0_head_to_head': 92.28515625,\n",
       " 'overshoot': 0.05393791198730469}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(lora_model, eval_data, return_to_cpu=True, return_stats=True, print_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ........ Loss: 0.01634118, Base Loss: 0.394893, Lora Diff: 0.00121789, WR: 5.08%, 0epsWR: 38.09%, OShL: 0.00017984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.016341184635450645,\n",
       " 'base_loss': 0.39489315043670103,\n",
       " 'lora_diff': 0.0012178865729310928,\n",
       " 'head_to_head': 5.078125,\n",
       " 'eps0_head_to_head': 38.0859375,\n",
       " 'overshoot': 0.00017984330224862788}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect(); torch.cuda.empty_cache()\n",
    "evaluate(lora_model, eval_data, return_stats=True, base_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ........0.012056129032998797\n",
      " Loss: 0.01205613, Base Loss: 0.365259, Lora Diff: -0.00147909, WR: 13.87%, 0epsWR: 58.01%, OShL: 0.00109561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.012056129032998797,\n",
       " 'base_loss': 0.36525918503184585,\n",
       " 'lora_diff': -0.0014790902009735873,\n",
       " 'head_to_head': 13.8671875,\n",
       " 'eps0_head_to_head': 58.0078125,\n",
       " 'overshoot': 0.0010956073215311335}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(lora_model, eval_data, return_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/alyx/.local/lib/python3.10/site-packages (4.38.1)\n",
      "Requirement already satisfied: filelock in /home/alyx/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/alyx/.local/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/alyx/.local/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alyx/.local/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/alyx/.local/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/alyx/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/alyx/.local/lib/python3.10/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/alyx/.local/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/alyx/.local/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/alyx/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/alyx/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/alyx/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/alyx/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model, lora_model, tokenizer, train_data, eval_data, trainer; gc.collect(); torch.cuda.empty_cache()\n",
    "# gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/alyx/finetuning-subnet/merged_model/tokenizer_config.json',\n",
       " '/home/alyx/finetuning-subnet/merged_model/special_tokens_map.json',\n",
       " '/home/alyx/finetuning-subnet/merged_model/tokenizer.model',\n",
       " '/home/alyx/finetuning-subnet/merged_model/added_tokens.json',\n",
       " '/home/alyx/finetuning-subnet/merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lora_model = lora_model.merge_and_unload()\n",
    "# lora_model.config.name_or_path = \"MesozoicMetallurgist/new_model\"\n",
    "# model_dir = \"Models/merged_model\"\n",
    "# model_dir = os.path.expanduser(\"~/finetuning-subnet/merged_model\")\n",
    "# if not os.path.exists(model_dir):\n",
    "#     os.makedirs(model_dir, exist_ok=True)\n",
    "# else:\n",
    "#     # wipe the directory\n",
    "#     for file in os.listdir(model_dir):\n",
    "#         os.remove(os.path.join(model_dir, file))\n",
    "# lora_model.save_pretrained(save_directory=model_dir, safe_serialization=True)\n",
    "# tokenizer.save_pretrained(save_directory=model_dir)\n",
    "\n",
    "# del lora_model, trainer, model, tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
