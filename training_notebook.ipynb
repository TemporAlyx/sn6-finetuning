{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from utils import *\n",
    "from training import *\n",
    "from validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"stablelm\" # mistral, gemma, stablelm\n",
    "\n",
    "hf_account_name = \"\" # huggingface.co username\n",
    "save_name = \"\" # name to save and upload the model as\n",
    "model_name = \"stabilityai/stablelm-2-zephyr-1_6b\" # hf_repo/model_name\n",
    "model_name_to_beat = model_name # set to the same unless comparing against a different model\n",
    "\n",
    "params = load_local_config()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, **params)\n",
    "model.config.name_or_path = save_name\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "tokenizer = get_tokenizer(model_type) # alternatively can input tokenizer repo and model name\n",
    "\n",
    "model = norm_model_weights(model) # normalize weights to prevent exploding gradients, is not correct or stable for all models, ymmv\n",
    "        \n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name_to_beat, **params)\n",
    "for name, param in base_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "trainer = Trainer(model, tokenizer, base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_model_params(model)\n",
    "\n",
    "print(validate_parameters(model, print_vals=True))\n",
    "print(validate_parameters(base_model, print_vals=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(acc_batch_size=512, opt=\"adamw\", lr=4e-5, lr_schedule=\"cosine\", weight_decay=0.0, betas=(0.9, 0.99), max_batch_steps=None,\n",
    "                warmup_steps=4, warmup_cycle_offset=-1,\n",
    "                grad_clip_norm=1.0, ignore_overshot_samples=True, bad_sample_mult=1.0, ignore_sample_loss_below=0.0, precalc_batch_mult=2.25,\n",
    "                remerging=False, remerge_ratio=0.75,\n",
    "                base_relative_loss=False, loss_eps = 0.02, overshoot_buffer = -0.01, eval_eps=0.01,\n",
    "                eval_n_batches=4, eval_size=64, revert=False, eval_revert_if={\"loss\": 0.004, \"head_to_head\": -12.5, \"eps0_head_to_head\": -22.5},\n",
    "                save_name=\"test\", do_save=True, cortex_steps=5, \n",
    "                gradient_checkpointing=True, excessive_cache_clearing=False, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_improvement(model, base_model, samples=768, tokenizer_name=model_type, dedup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_name = hf_account_name + \"/\" + save_name\n",
    "tokenizer.push_to_hub(repo_id=upload_name, private=True)\n",
    "commit_info = model.push_to_hub(repo_id=upload_name, safe_serialization=True, private=True)\n",
    "print(commit_info.oid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
