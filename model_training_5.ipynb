{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel,\n",
    "    LoHaConfig,\n",
    ")\n",
    "\n",
    "from cortexsubsetloader import CortexSubsetLoader, tokenize\n",
    "from pytorch_optimizer import Ranger21, SAM, DAdaptAdam, SophiaH, ScalableShampoo #, Lamb, DAdaptLion, LOMO\n",
    "from pytorch_optimizer.optimizer.sam import WSAM\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb key \"\"\n",
    "# !pip install -U numpy\n",
    "# !pip install -U torch --force-reinstall\n",
    "# !pip install -U transformers\n",
    "# !pip install -U pytorch-optimizer\n",
    "# !pip install -U flash-attn\n",
    "# !pip show torch transformers peft flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(train, eval):\n",
    "    evalf = []\n",
    "    for f in eval:\n",
    "        if f not in train:\n",
    "            evalf.append(f)\n",
    "    if len(evalf) < len(eval): print(f\"Removed {len(eval)-len(evalf)} duplicates from eval\")\n",
    "    else: print(\"No duplicates found in eval\")\n",
    "    return evalf\n",
    "\n",
    "\n",
    "def data_collator(features):\n",
    "    batches = []\n",
    "    for feature in features:\n",
    "        inputs, prompt_len = feature\n",
    "        data = [inputs]\n",
    "        b_labels = inputs.clone()\n",
    "        b_labels[:, :prompt_len] = -100\n",
    "        labels = [b_labels]\n",
    "            \n",
    "        batch = {}\n",
    "        batch['input_ids'] = torch.concat(data)\n",
    "        batch['labels'] = torch.concat(labels)\n",
    "        batches.append(batch)\n",
    "    return batches\n",
    "\n",
    "\n",
    "def get_data(train_name, eval_name, tokenizer, train_subset=None, eval_subset=None, shuffle=True, extend_train_length=0):\n",
    "    if type(train_name) == str:\n",
    "        with open(train_name, \"r\") as f:\n",
    "            train_data = np.array(json.load(f))\n",
    "    else: # list of str\n",
    "        train_data = []\n",
    "        for name in train_name:\n",
    "            with open(name, \"r\") as f:\n",
    "                train_data = train_data + json.load(f)\n",
    "        train_data = np.array(train_data)\n",
    "    with open(eval_name, \"r\") as f:\n",
    "        eval_data = np.array(json.load(f))\n",
    "\n",
    "    if train_subset is not None:\n",
    "        train_data = train_data[train_subset]\n",
    "    if eval_subset is not None:\n",
    "        eval_data = eval_data[-eval_subset:]\n",
    "    \n",
    "    eval_data = filter_data(train_data, eval_data)\n",
    "\n",
    "    if shuffle:\n",
    "        p = np.random.permutation(len(train_data))\n",
    "        train_data = train_data[p]\n",
    "\n",
    "    train_data = tokenize(tokenizer, train_data, 2048+extend_train_length)\n",
    "    eval_data = tokenize(tokenizer, eval_data, 2048)\n",
    "\n",
    "    train_data = data_collator(train_data)\n",
    "    eval_data = data_collator(eval_data)\n",
    "\n",
    "    return train_data, eval_data\n",
    "\n",
    "\n",
    "def merge(model0, model1, ratio=0.5, embed_ratio=None, norm_ratio=None, fc_ratio=None): # higher ratio means more of model0\n",
    "    if embed_ratio is None:\n",
    "        embed_ratio = ratio\n",
    "    if norm_ratio is None:\n",
    "        norm_ratio = ratio\n",
    "    if fc_ratio is None:\n",
    "        fc_ratio = ratio\n",
    "\n",
    "    params0 = {}\n",
    "    for name, param in model0.named_parameters():\n",
    "        params0[name] = param\n",
    "\n",
    "    for name, param in model1.named_parameters():\n",
    "        if \"embed\" in name:\n",
    "            param.data = ((params0[name].data * embed_ratio) + (param.data * (1 - embed_ratio)))\n",
    "        elif (\"up_proj\" not in name \n",
    "            and \"down_proj\" not in name \n",
    "            and \"gate_proj\" not in name \n",
    "            and \"o_proj\" not in name \n",
    "            and \"k_proj\" not in name \n",
    "            and \"v_proj\" not in name \n",
    "            and \"q_proj\" not in name\n",
    "            and \"embed\" not in name\n",
    "            ):\n",
    "            param.data = ((params0[name].data * norm_ratio) + (param.data * (1 - norm_ratio)))\n",
    "        elif \"up_proj\" in name or \"down_proj\" in name:\n",
    "            param.data = ((params0[name].data * fc_ratio) + (param.data * (1 - fc_ratio)))\n",
    "        else:\n",
    "            param.data = ((params0[name].data * ratio) + (param.data * (1 - ratio)))\n",
    "\n",
    "    return model1\n",
    "\n",
    "def copy_weights_over(model0, model1):\n",
    "    \"\"\"Copies the weights from model0 to model1, returns model1 with the copied weights\"\"\"\n",
    "    params0 = {}\n",
    "    for name, param in model0.named_parameters():\n",
    "        params0[name] = param\n",
    "\n",
    "    for name, param in model1.named_parameters():\n",
    "        if name in params0:\n",
    "            param.data = params0[name].data\n",
    "    return model1\n",
    "\n",
    "\n",
    "# initial_loss_eps = 0.0001\n",
    "intermed_check_step_split = 8\n",
    "\n",
    "def simple_eval(model, eval_d):\n",
    "    print(\"Evaluating\", end=\" \")\n",
    "    model = model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    steps_so_far = 1\n",
    "    for batch in eval_d:\n",
    "        inputs = batch['input_ids'].to(\"cuda\")\n",
    "        labels = batch['labels'].to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            eval_loss += outputs.loss.item() / len(eval_d)\n",
    "        if steps_so_far % (len(eval_d) // intermed_check_step_split) == 0:\n",
    "            print(\".\", end=\"\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "        steps_so_far += 1\n",
    "    model = model.to(\"cpu\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    print(f\" Loss: {eval_loss:.8f}\")\n",
    "\n",
    "def evaluate(model, eval_d, return_to_cpu=False, return_stats=False, print_stats=True, cached_base_loss=None,\n",
    "             base_model=None, precompute_base_loss=True, device=\"cuda\", instruction_finetuning=True, true_eps=0.01):\n",
    "    print(\"Evaluating\", end=\" \")\n",
    "    model = model.to(\"cuda\")\n",
    "    eval_base_loss = 0\n",
    "    lora_diff = 0\n",
    "    eval_loss = 0\n",
    "    head_to_head = 0\n",
    "    eps0_head_to_head = 0\n",
    "    overshoot = 0\n",
    "    model.eval()\n",
    "    steps_so_far = 1\n",
    "\n",
    "    LORA = True\n",
    "    if base_model is not None:\n",
    "        LORA = False\n",
    "\n",
    "    precomputed_base_losses = []\n",
    "    if cached_base_loss is not None:\n",
    "        for x in cached_base_loss:\n",
    "            precomputed_base_losses.append(x)\n",
    "        precompute_base_loss = True\n",
    "    elif precompute_base_loss and cached_base_loss is None:\n",
    "        if LORA:\n",
    "            model.disable_adapter_layers()\n",
    "        else:\n",
    "            model = model.to(\"cpu\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "            base_model = base_model.to(device)\n",
    "\n",
    "        for batch in eval_d:\n",
    "            if instruction_finetuning:\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "            else:\n",
    "                inputs = batch.to(device)\n",
    "                labels = inputs.clone()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if LORA:\n",
    "                    base_outputs_loss = model(inputs, labels=labels).loss\n",
    "                else:\n",
    "                    base_outputs_loss = base_model(inputs, labels=labels).loss\n",
    "                precomputed_base_losses.append(base_outputs_loss)\n",
    "        \n",
    "        if LORA:\n",
    "            model.enable_adapter_layers()\n",
    "        else:\n",
    "            base_model = base_model.to(\"cpu\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "            model = model.to(device)\n",
    "    else:\n",
    "        print(\"WARN: not precomputing base loss will put both models on the same device\")\n",
    "        base_model = base_model.to(device)\n",
    "\n",
    "\n",
    "    for batch in eval_d:\n",
    "        if instruction_finetuning:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "        else:\n",
    "            inputs = batch.to(device)\n",
    "            labels = inputs.clone()\n",
    "        with torch.no_grad():\n",
    "            if precompute_base_loss:\n",
    "                base_outputs_loss = precomputed_base_losses.pop(0)\n",
    "            else:\n",
    "                if LORA:\n",
    "                    model.disable_adapter_layers()\n",
    "                    base_outputs_loss = model(inputs, labels=labels).loss\n",
    "                    model.enable_adapter_layers()\n",
    "                else:\n",
    "                    base_outputs_loss = base_model(inputs, labels=labels).loss\n",
    "            outputs_loss = model(inputs, labels=labels).loss\n",
    "\n",
    "            base_loss = base_outputs_loss\n",
    "            partial_loss = torch.nn.functional.relu(outputs_loss - (base_loss * (1.0 - true_eps)))\n",
    "            overshoot_penalty = torch.nn.functional.relu(-(outputs_loss - (base_loss * (1.0 - true_eps))))\n",
    "            loss = partial_loss / base_loss\n",
    "\n",
    "            if cached_base_loss is None:\n",
    "                base_outputs_loss_item = base_outputs_loss.item()\n",
    "            else:\n",
    "                base_outputs_loss_item = base_outputs_loss\n",
    "            eval_loss += loss.item() / len(eval_d)\n",
    "            eval_base_loss += base_outputs_loss_item / len(eval_d)\n",
    "            lora_diff += (outputs_loss.item() - base_outputs_loss_item) / len(eval_d)\n",
    "            head_to_head += 100.0 / len(eval_d) if outputs_loss < (base_outputs_loss * (1.0 - true_eps)) else 0.0\n",
    "            head_to_head += 50.0 / len(eval_d) if outputs_loss == (base_outputs_loss * (1.0 - true_eps)) else 0.0\n",
    "            eps0_head_to_head += 100.0 / len(eval_d) if outputs_loss < base_outputs_loss else 0.0\n",
    "            eps0_head_to_head += 50.0 / len(eval_d) if outputs_loss == base_outputs_loss else 0.0\n",
    "            overshoot += overshoot_penalty.item() / len(eval_d)\n",
    "\n",
    "        if steps_so_far % (len(eval_d) // intermed_check_step_split) == 0:\n",
    "            print(\".\", end=\"\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "        steps_so_far += 1\n",
    "\n",
    "    if return_to_cpu:\n",
    "        model = model.to(\"cpu\")\n",
    "\n",
    "    if not LORA:\n",
    "        base_model = base_model.to(\"cpu\")\n",
    "\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    data = {\n",
    "        \"loss\": eval_loss,\n",
    "        \"base_loss\": eval_base_loss,\n",
    "        \"lora_diff\": lora_diff,\n",
    "        \"head_to_head\": head_to_head,\n",
    "        \"eps0_head_to_head\": eps0_head_to_head,\n",
    "        \"overshoot\": overshoot\n",
    "    }\n",
    "\n",
    "    if print_stats:\n",
    "        print(f\" Loss: {eval_loss:.8f}, Base Loss: {eval_base_loss:.6f}, Lora Diff: {lora_diff:.8f},\",\n",
    "            f\"WR: {head_to_head:.2f}%, 0epsWR: {eps0_head_to_head:.2f}%, OShL: {overshoot:.8f}\")\n",
    "    if return_stats:\n",
    "        return data\n",
    "\n",
    "old_train_data = []\n",
    "\n",
    "def train(model, tokenizer, train_d=[], eval_d=[], base_model=None, training_device=\"cuda\",\n",
    "          inf_training=False, inf_data_steps=5, inf_data_decay=1, inf_data_purge=0,\n",
    "            acc_batch_size=512, instruction_finetuing=True, precalculate_batch_mult=2.25, precalc_eval_base=True,\n",
    "            lr=1e-5, weight_decay=0.001, lr_scheduler=\"constant\", warmup_steps=4, warmup_end_offset=0, betas=(0.9, 0.99), \n",
    "            use_sam=False, sam_rho=0.05, opt=\"adamw\", adaptive_sam=True, wsam_variant=False, ignore_below=0.0, ignore_auto_percent=2,\n",
    "            manual_grad_clip_norm=1.0, sam_grad_clip_norm=None,\n",
    "            add_overshoot_penalty=False, ignore_overshot_samples=True, bad_sample_mult=1.0, \n",
    "            remerging=False, remerge_eval=True, remerge_ratio=[0.75, 0.6, 0.16],\n",
    "            loss_eps = 0.02, overshoot_buffer = -0.01, true_eps=0.01, simple_loss=False, process_base_loss=True, relative_loss=False,\n",
    "            eval_steps=2048, save_name=\"lora\", do_save=True, eval_revert_if={\"loss\": 0.004, \"head_to_head\": -12.5, \"eps0_head_to_head\": -22.5},\n",
    "            average_stats=False, save_n_start=0, revert=True,\n",
    "            gradient_checkpointing=False, excessive_cache_clearing=False, base_model_switching=True):\n",
    "    global old_train_data\n",
    "    if warmup_steps is None:\n",
    "        warmup_steps = (eval_steps // acc_batch_size) // 2\n",
    "    LORA = True\n",
    "    if base_model is not None:\n",
    "        LORA = False\n",
    "        # base_model = base_model.to(\"cuda\")\n",
    "\n",
    "    if type(remerge_ratio) is not list:\n",
    "        remerge_ratio = [remerge_ratio]\n",
    "    else:\n",
    "        print(\"multiple remerge ratios, note that they are combined in order\")\n",
    "\n",
    "    if len(train_d) == 0:\n",
    "        print(\"WARN: no training data provided, enabling infinite training\")\n",
    "        inf_training = True\n",
    "\n",
    "    params = {\n",
    "        'low_cpu_mem_usage': True,\n",
    "        'trust_remote_code': False,\n",
    "        'torch_dtype': torch.bfloat16,\n",
    "        'use_safetensors': True,\n",
    "        'attn_implementation': \"flash_attention_2\"\n",
    "    }\n",
    "\n",
    "    if remerging and LORA:\n",
    "        print(\"WARN: remerging is enabled but LORA is not enabled, disabling remerging\")\n",
    "        remerging = False\n",
    "    \n",
    "    if LORA and revert:\n",
    "        print(\"WARN: LORA is enabled, model_prev is not implemented yet, disabling revert\")\n",
    "        revert=False\n",
    "    \n",
    "    if revert:\n",
    "        model.save_pretrained(\"model_prev\")\n",
    "        print(\"WARN: model_prev created and may take up extra memory\")\n",
    "        model_prev = AutoModelForCausalLM.from_pretrained(\"model_prev\", **params)\n",
    "    else:\n",
    "        model_prev = None\n",
    "\n",
    "    \n",
    "    def get_new_data(n_samples=2560, dd_eval=True, dd_train=True, steps=1, old_data=old_train_data):\n",
    "        cortex_subset_loader = CortexSubsetLoader(latest=True, random_seed = None, max_samples=n_samples, progress=False, \n",
    "                                        running=True, retry_limit=5, page_size=400, retry_delay=5, silent=True, steps=steps,\n",
    "                                        ignore_list=old_data)\n",
    "        batches = data_collator(cortex_subset_loader.tokenize(tokenizer))\n",
    "        dedup_batches = batches\n",
    "        p = np.random.permutation(len(dedup_batches))# [:min(n_samples, len(dedup_batches))]\n",
    "        return [dedup_batches[i] for i in p]\n",
    "\n",
    "    if len(eval_d) == 0:\n",
    "        print(\"WARN: no evaluation data provided, acquiring new data\")\n",
    "        eval_d = get_new_data(5120) # get more than necessary to avoid similar samples in train\n",
    "        eval_d = eval_d[:512]\n",
    "\n",
    "    add_inf_steps = 0\n",
    "    if inf_training:\n",
    "        if len(train_d) == 0:\n",
    "            print(\"WARN: no training data provided, acquiring new data\")\n",
    "            while len(train_d) < (acc_batch_size * precalculate_batch_mult):\n",
    "                new_data = get_new_data(int(acc_batch_size * precalculate_batch_mult), steps=inf_data_steps+add_inf_steps)\n",
    "                if len(new_data) == 0:\n",
    "                    add_inf_steps += inf_data_steps\n",
    "                else:\n",
    "                    add_inf_steps = add_inf_steps - inf_data_decay\n",
    "                    train_d = train_d + new_data\n",
    "    \n",
    "    if simple_loss:\n",
    "        if process_base_loss:\n",
    "            print(\"WARN: simple loss is enabled, this will disable base model processing\")\n",
    "            process_base_loss = False\n",
    "        \n",
    "    if not process_base_loss:\n",
    "        if ignore_overshot_samples:\n",
    "            print(\"Base loss processing is disabled, disabling ignore overshot samples\")\n",
    "            ignore_overshot_samples = False\n",
    "\n",
    "\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    model = model.to(\"cuda\")\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "    if gradient_checkpointing:\n",
    "        model.config.use_cache = False\n",
    "        grad_check_kwargs = {\"use_reentrant\": False}\n",
    "        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=grad_check_kwargs)\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    if not use_sam:\n",
    "        if opt == \"dadapt_adam\":\n",
    "            optimizer = DAdaptAdam(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay, fixed_decay=True)\n",
    "        elif opt == \"shampoo\":\n",
    "            optimizer = ScalableShampoo(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay, \n",
    "                                        start_preconditioning_step=warmup_steps+1, preconditioning_compute_steps=1)\n",
    "        elif opt == \"sophia\":\n",
    "            optimizer = SophiaH(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        elif opt == \"ranger\":\n",
    "            optimizer = Ranger21(model.parameters(), num_iterations=1, lr=lr, betas=betas, weight_decay=weight_decay,\n",
    "                                 num_warm_up_iterations=0, num_warm_down_iterations=0)\n",
    "        elif opt == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        elif opt == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=betas[0], weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer {opt}\")\n",
    "    else:\n",
    "        base_optimizer_args = {\"lr\": lr, \"weight_decay\": weight_decay, \"betas\": betas, \"eps\": 1e-8}\n",
    "\n",
    "        if opt == \"dadapt_adam\":\n",
    "            base_optimizer = DAdaptAdam\n",
    "        elif opt == \"sophia\":\n",
    "            base_optimizer = SophiaH\n",
    "        elif opt == \"shampoo\":\n",
    "            base_optimizer = ScalableShampoo\n",
    "            base_optimizer_args[\"start_preconditioning_step\"] = warmup_steps+1\n",
    "            base_optimizer_args[\"preconditioning_compute_steps\"] = 1\n",
    "        elif opt == \"ranger\":\n",
    "            base_optimizer = Ranger21\n",
    "            base_optimizer_args[\"num_iterations\"] = 1\n",
    "            base_optimizer_args[\"num_warm_up_iterations\"] = 0\n",
    "            base_optimizer_args[\"num_warm_down_iterations\"] = 0\n",
    "        elif opt == \"adamw\":\n",
    "            base_optimizer = torch.optim.AdamW\n",
    "        elif opt == \"sgd\":\n",
    "            base_optimizer = torch.optim.SGD\n",
    "            del base_optimizer_args[\"betas\"], base_optimizer_args[\"eps\"]\n",
    "            base_optimizer_args[\"momentum\"] = betas[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer {opt}\")\n",
    "\n",
    "        if not wsam_variant:\n",
    "            optimizer = SAM(model.parameters(), base_optimizer=base_optimizer, rho=sam_rho, adaptive=adaptive_sam, **base_optimizer_args)\n",
    "            sam_optimizer = optimizer\n",
    "        else:\n",
    "            optimizer = WSAM(model, params=model.parameters(), base_optimizer=base_optimizer, rho=sam_rho, adaptive=adaptive_sam, \n",
    "                             **base_optimizer_args, max_norm=sam_grad_clip_norm)\n",
    "            sam_optimizer = optimizer\n",
    "\n",
    "\n",
    "\n",
    "    if lr_scheduler == \"cosine\":\n",
    "        lr_scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, warmup_steps, (len(train_d)//acc_batch_size)+warmup_end_offset)\n",
    "    elif lr_scheduler == \"polynomial\":\n",
    "        lr_scheduler = transformers.get_polynomial_decay_schedule_with_warmup(optimizer, warmup_steps, \n",
    "                                                                              (len(train_d)//acc_batch_size)+warmup_end_offset)\n",
    "    elif lr_scheduler == \"constant\":\n",
    "        lr_scheduler = transformers.get_constant_schedule_with_warmup(optimizer, warmup_steps)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown lr_scheduler {lr_scheduler}\")\n",
    "    lr_scheduler.step() # don't want to start at 0\n",
    "    \n",
    "    @torch.jit.script\n",
    "    def combined_loss_os(outputs_loss, base_loss_in, loss_eps:float=loss_eps, overshoot_buffer:float=overshoot_buffer):\n",
    "        base_loss = base_loss_in.item()\n",
    "        partial_loss = outputs_loss - (base_loss * (1.0 - loss_eps))\n",
    "        loss = partial_loss / base_loss\n",
    "        overshoot_penalty = torch.nn.functional.relu(-(loss + overshoot_buffer))\n",
    "        return torch.nn.functional.relu(loss) + overshoot_penalty, overshoot_penalty.item()\n",
    "    \n",
    "    @torch.jit.script\n",
    "    def combined_loss_os_noshot(outputs_loss, base_loss_in, loss_eps:float=loss_eps, overshoot_buffer:float=overshoot_buffer):\n",
    "        base_loss = base_loss_in.item()\n",
    "        partial_loss = outputs_loss - (base_loss * (1.0 - loss_eps))\n",
    "        loss = partial_loss / base_loss\n",
    "        return torch.nn.functional.relu(loss), torch.nn.functional.relu(-(loss + overshoot_buffer)).item()\n",
    "    \n",
    "    @torch.jit.script\n",
    "    def simple_loss_func(outputs_loss, base_loss_in, loss_eps:float=loss_eps, overshoot_buffer:float=overshoot_buffer):\n",
    "        return outputs_loss, 0.0\n",
    "    \n",
    "    @torch.jit.script\n",
    "    def relative_loss_func(outputs_loss, base_loss_in, loss_eps:float=loss_eps, overshoot_buffer:float=overshoot_buffer):\n",
    "        base_loss = base_loss_in.item()\n",
    "        relative_loss = outputs_loss / (base_loss + loss_eps)\n",
    "        osh = torch.nn.functional.relu(-((relative_loss - 1.0) + overshoot_buffer)).item()\n",
    "        loss = outputs_loss * (4.2 * torch.square(torch.sin((torch.clamp(relative_loss, min=0.813, max=1.187) - 0.065) * 4.2)))\n",
    "        return loss, osh\n",
    "\n",
    "    if add_overshoot_penalty:\n",
    "        combined_loss = combined_loss_os\n",
    "    else:\n",
    "        combined_loss = combined_loss_os_noshot\n",
    "\n",
    "    if simple_loss:\n",
    "        combined_loss = simple_loss_func\n",
    "        if relative_loss:\n",
    "            print(\"WARN: simple loss is enabled, disabling relative loss\")\n",
    "            relative_loss = False\n",
    "\n",
    "    if relative_loss:\n",
    "        combined_loss = relative_loss_func\n",
    "\n",
    "\n",
    "    if precalc_eval_base:\n",
    "        print(\"Note: precalced eval base loss does not account for pretrained fine-tuning\")\n",
    "        eval_base_loss = []\n",
    "        steps_so_far = 1\n",
    "        if not LORA:\n",
    "            model = model.to(\"cpu\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "            base_model = base_model.to(training_device)\n",
    "        for batch in eval_d:\n",
    "            if instruction_finetuing:\n",
    "                inputs = batch['input_ids'].to(training_device)\n",
    "                labels = batch['labels'].to(training_device)\n",
    "            else:\n",
    "                inputs = batch.to(training_device)\n",
    "                labels = inputs.clone()\n",
    "            with torch.no_grad():\n",
    "                if LORA:\n",
    "                    base_outputs_loss = model(inputs, labels=labels).loss\n",
    "                else:\n",
    "                    base_outputs_loss = base_model(inputs, labels=labels).loss\n",
    "                eval_base_loss.append(base_outputs_loss.item())\n",
    "            if steps_so_far % (len(eval_d) // intermed_check_step_split) == 0:\n",
    "                print(\".\", end=\"\")\n",
    "                gc.collect(); torch.cuda.empty_cache()\n",
    "            steps_so_far += 1\n",
    "        if not LORA:\n",
    "            base_model = base_model.to(\"cpu\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "            model = model.to(training_device)\n",
    "        print(f\"Eval Base Loss: {sum(eval_base_loss)/len(eval_d):.6f}\")\n",
    "        if ignore_below == \"auto\":\n",
    "            ignore_below = np.percentile(eval_base_loss, ignore_auto_percent)\n",
    "    else:\n",
    "        eval_base_loss = None\n",
    "\n",
    "    if ignore_below == \"auto\":\n",
    "        print(\"eval loss not precalced, ignore_below set to 0.0\")\n",
    "        ignore_below = 0.0\n",
    "\n",
    "    steps_so_far = 1 # start at one to avoid all the modulo checks\n",
    "    epoch_loss = 0; epoch_overshoot = 0; epoch_base_loss = 0; lora_diff = 0\n",
    "    epoch_wr = 0; epoch_0eps_wr = 0\n",
    "    fit_samples = 0; unfit_samples = 0\n",
    "    sam_batch = []\n",
    "    # lomo_batch_loss = []\n",
    "    accum_steps = 0.0\n",
    "    true_steps_taken = 0; prev_dot_step = -1\n",
    "    last_tst = true_steps_taken\n",
    "    add_inf_steps = 0\n",
    "    sam_saved_base_outputs = []\n",
    "    precalculated_base_outputs = []\n",
    "    prev_eval = {\"loss\": 99.99, \"head_to_head\": 0.0, \"eps0_head_to_head\": 0.0}\n",
    "    while len(train_d) > 0:\n",
    "\n",
    "        if (true_steps_taken % (acc_batch_size // intermed_check_step_split) == 0) and accum_steps != prev_dot_step:\n",
    "            prev_dot_step = accum_steps\n",
    "            print(\".\", end=\"\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "        while len(train_d) < (acc_batch_size * precalculate_batch_mult):\n",
    "            new_data = get_new_data(int(acc_batch_size * precalculate_batch_mult), steps=inf_data_steps+add_inf_steps)\n",
    "            if len(new_data) == 0:\n",
    "                add_inf_steps += inf_data_steps\n",
    "            else:\n",
    "                add_inf_steps = add_inf_steps - inf_data_decay\n",
    "            # old_train_data = old_train_data + new_data\n",
    "            train_d = train_d + new_data\n",
    "\n",
    "        bstep = 0\n",
    "        if len(precalculated_base_outputs) == 0 and process_base_loss:\n",
    "            batches = train_d[:int(acc_batch_size * precalculate_batch_mult)]\n",
    "\n",
    "            if LORA:\n",
    "                model.disable_adapter_layers()\n",
    "            else:\n",
    "                if base_model_switching:\n",
    "                    model = model.to(\"cpu\")\n",
    "                    gc.collect(); torch.cuda.empty_cache()\n",
    "                base_model = base_model.to(training_device)\n",
    "            \n",
    "            for batch in batches:\n",
    "                if instruction_finetuing:\n",
    "                    inputs = batch['input_ids'].to(training_device)\n",
    "                    labels = batch['labels'].to(training_device)\n",
    "                else:\n",
    "                    inputs = batch.to(training_device)\n",
    "                    labels = inputs.clone()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if LORA:\n",
    "                        base_outputs = model(inputs, labels=labels)\n",
    "                    else:\n",
    "                        base_outputs = base_model(inputs, labels=labels)\n",
    "                    precalculated_base_outputs.append(torch.tensor(base_outputs.loss.item()))\n",
    "\n",
    "                bstep+=1\n",
    "                if bstep % (intermed_check_step_split) == 0:\n",
    "                    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "            if LORA:\n",
    "                model.enable_adapter_layers()\n",
    "            else:\n",
    "                base_model = base_model.to(\"cpu\")\n",
    "                gc.collect(); torch.cuda.empty_cache()\n",
    "                if base_model_switching:\n",
    "                    model = model.to(training_device)\n",
    "\n",
    "\n",
    "        batch = train_d.pop(0)\n",
    "        if instruction_finetuing:\n",
    "            inputs = batch['input_ids'].to(training_device)\n",
    "            labels = batch['labels'].to(training_device)\n",
    "        else:\n",
    "            inputs = batch.to(training_device)\n",
    "            labels = inputs.clone()\n",
    "\n",
    "        \n",
    "        if process_base_loss:\n",
    "            base_outputs_loss = precalculated_base_outputs.pop(0)\n",
    "        else:\n",
    "            base_outputs_loss = torch.zeros(size=(1,))\n",
    "        outputs_loss = model(inputs, labels=labels).loss\n",
    "\n",
    "        loss, overshoot_penalty = combined_loss(outputs_loss, base_outputs_loss)\n",
    "        loss = loss / acc_batch_size\n",
    "\n",
    "        if (not ignore_overshot_samples or overshoot_penalty <= 0.0) and outputs_loss.item() >= ignore_below:\n",
    "            if not simple_loss and loss.item() > ((loss_eps / acc_batch_size)+1e-8):\n",
    "                unfit_samples += -1\n",
    "                if bad_sample_mult != 1.0:\n",
    "                    loss = loss * bad_sample_mult\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            accum_steps += 1\n",
    "            true_steps_taken += 1\n",
    "            if use_sam:\n",
    "                sam_batch.append((inputs, labels))\n",
    "                sam_saved_base_outputs.append(base_outputs_loss)\n",
    "        else:\n",
    "            fit_samples += 1\n",
    "\n",
    "        outputs_loss_item = outputs_loss.detach().item()\n",
    "        if process_base_loss:\n",
    "            base_loss_item = base_outputs_loss.item()\n",
    "        else:\n",
    "            if eval_base_loss is not None:\n",
    "                base_loss_item = sum(eval_base_loss) / len(eval_d)\n",
    "            else:\n",
    "                base_loss_item = 0.0\n",
    "\n",
    "        epoch_base_loss += base_loss_item\n",
    "        lora_diff += (outputs_loss_item - base_loss_item)\n",
    "        epoch_loss += max(loss.detach().item() * acc_batch_size, 0.0)\n",
    "        epoch_wr += 100.0 if outputs_loss_item < (base_loss_item * (1.0 - true_eps)) else 0.0\n",
    "        epoch_wr += 50.0 if outputs_loss_item == (base_loss_item * (1.0 - true_eps)) else 0.0\n",
    "        epoch_0eps_wr += 100.0 if outputs_loss_item < base_loss_item else 0.0\n",
    "        epoch_0eps_wr += 50.0 if outputs_loss_item == base_loss_item else 0.0\n",
    "        epoch_overshoot += overshoot_penalty\n",
    "\n",
    "        if accum_steps == acc_batch_size:\n",
    "            if not use_sam:\n",
    "                if manual_grad_clip_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), manual_grad_clip_norm)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                sub_steps = 1\n",
    "                sam_optimizer.first_step(zero_grad=True)\n",
    "                \n",
    "                for inputs, labels in sam_batch:\n",
    "                    base_outputs_loss = sam_saved_base_outputs.pop(0)\n",
    "                    outputs = model(inputs, labels=labels)\n",
    "\n",
    "                    loss, overshoot_penalty = combined_loss(outputs.loss, base_outputs_loss)\n",
    "                    if loss.item() > ((loss_eps / acc_batch_size)+1e-8):\n",
    "                        if bad_sample_mult is not None and bad_sample_mult != 1.0:\n",
    "                            loss = loss * bad_sample_mult\n",
    "                    loss = loss / accum_steps\n",
    "                    \n",
    "                    loss.backward()\n",
    "\n",
    "                    if sub_steps % (acc_batch_size // intermed_check_step_split) == 0:\n",
    "                        print(\".\", end=\"\")\n",
    "                    sub_steps += 1\n",
    "                    \n",
    "                    if excessive_cache_clearing:\n",
    "                        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "                sam_optimizer.second_step(zero_grad=True)\n",
    "                sam_batch = []\n",
    "\n",
    "            if average_stats:\n",
    "                stat_steps = steps_so_far\n",
    "            else:\n",
    "                stat_steps = accum_steps\n",
    "                stat_steps += fit_samples\n",
    "            print(f\"Step {steps_so_far}/{len(train_d)}\\tLoss: {epoch_loss/stat_steps:.6f}\",\n",
    "                                                    f\"OShL: {epoch_overshoot/stat_steps:.3e}\"\n",
    "                                                    f\"\\tBase: {epoch_base_loss/stat_steps:.4f}\",\n",
    "                                                    f\"Diff: {lora_diff/stat_steps:.4e}\",\n",
    "                                                    f\"\\tWR: {epoch_wr/stat_steps:2.2f}%\",\n",
    "                                                    f\"0eps: {epoch_0eps_wr/stat_steps:2.2f}% \",\n",
    "                                                    f\"\\tLR: {lr_scheduler.get_last_lr()[0]:.2e}\",\n",
    "                                                    # f\"eps: {loss_eps:.2e}\",\n",
    "                                                    f\"fit: {fit_samples}/{unfit_samples}\"\n",
    "                                                    )\n",
    "\n",
    "            if not average_stats:\n",
    "                epoch_overshoot = 0\n",
    "                epoch_loss = 0\n",
    "                epoch_base_loss = 0\n",
    "                lora_diff = 0\n",
    "                epoch_wr = 0\n",
    "                epoch_0eps_wr = 0\n",
    "            unfit_samples = 0\n",
    "            fit_samples = 0\n",
    "            accum_steps = 0\n",
    "            \n",
    "            lr_scheduler.step()\n",
    "            if lr_scheduler.get_last_lr()[0] == 0.0:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        if true_steps_taken % eval_steps == 0 and len(train_d) > 0 and true_steps_taken != last_tst:\n",
    "            is_better = False\n",
    "            if remerging:\n",
    "                if remerge_eval:\n",
    "                    evaluate(model, eval_d, base_model=base_model, device=training_device, instruction_finetuning=instruction_finetuing, \n",
    "                            true_eps=true_eps, cached_base_loss=eval_base_loss, precompute_base_loss=True)\n",
    "                for ratio in remerge_ratio:\n",
    "                    model = model.to(\"cpu\")\n",
    "                    model = merge(model_prev, model, ratio=(1.0 - ratio))\n",
    "                    model = model.to(training_device)\n",
    "                    new_eval = evaluate(model, eval_d, base_model=base_model, device=training_device, instruction_finetuning=instruction_finetuing, \n",
    "                            true_eps=true_eps, cached_base_loss=eval_base_loss, precompute_base_loss=True, return_stats=True)\n",
    "                    \n",
    "                    if ((prev_eval['loss'] + eval_revert_if['loss']) > new_eval['loss'] and\n",
    "                        (prev_eval['head_to_head'] + eval_revert_if['head_to_head']) < new_eval['head_to_head'] and\n",
    "                        (prev_eval['eps0_head_to_head'] + eval_revert_if['eps0_head_to_head']) < new_eval['eps0_head_to_head']):\n",
    "                        is_better = True\n",
    "                        break\n",
    "            else:\n",
    "                if ((prev_eval['loss'] + eval_revert_if['loss']) > new_eval['loss'] and\n",
    "                    (prev_eval['head_to_head'] + eval_revert_if['head_to_head']) < new_eval['head_to_head'] and\n",
    "                    (prev_eval['eps0_head_to_head'] + eval_revert_if['eps0_head_to_head']) < new_eval['eps0_head_to_head']):\n",
    "                    is_better = True\n",
    "\n",
    "            if revert:\n",
    "                if is_better:\n",
    "                    model.save_pretrained(\"model_prev\")\n",
    "                    model_prev = AutoModelForCausalLM.from_pretrained(\"model_prev\", **params)\n",
    "                    prev_eval = new_eval\n",
    "                else:\n",
    "                    print(\"latest eval was worse, reverting model..\")\n",
    "\n",
    "                    model = copy_weights_over(model_prev, model)\n",
    "                    model = model.to(training_device)\n",
    "\n",
    "                    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "            if do_save:\n",
    "                model.save_pretrained(save_name + '_' + str((true_steps_taken // eval_steps) + save_n_start).format(\"02d\"))\n",
    "            model.train()\n",
    "        \n",
    "        steps_so_far += 1\n",
    "        last_tst = true_steps_taken\n",
    "\n",
    "        if excessive_cache_clearing:\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "            \n",
    "        if time.time() - notebook_start_time > 5.75*60*60:\n",
    "            break\n",
    "\n",
    "    if do_save:\n",
    "        if save_n_start > 0:\n",
    "            model.save_pretrained(save_name+\"_X\"+str(save_n_start))\n",
    "        else:\n",
    "            model.save_pretrained(save_name)\n",
    "\n",
    "    model.eval()\n",
    "    final_eval_stats = evaluate(model, eval_d, return_stats=True, base_model=base_model, device=training_device, \n",
    "                                instruction_finetuning=instruction_finetuing, true_eps=true_eps,\n",
    "                     cached_base_loss=eval_base_loss, precompute_base_loss=True)\n",
    "\n",
    "    model = model.to(\"cpu\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    return final_eval_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76163c0c054449bca6bf5685ae94180e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "mistral=False\n",
    "llama3 = True\n",
    "if not llama3:\n",
    "    if not mistral:\n",
    "        # lora_name \"Carboniferous\"\n",
    "        lora_name = \"Carnian\"\n",
    "        model_name = \"\"\n",
    "        model_name_to_beat = model_name\n",
    "    else:\n",
    "        lora_name = \"Helium\"\n",
    "        model_name = \"\"\n",
    "        model_name_to_beat = model_name\n",
    "else:\n",
    "    lora_name = \"Hadean\"\n",
    "    model_name = \"\"\n",
    "    model_name_to_beat = model_name\n",
    "\n",
    "params = {\n",
    "    'low_cpu_mem_usage': True,\n",
    "    'trust_remote_code': False,\n",
    "    'torch_dtype': torch.bfloat16,\n",
    "    'use_safetensors': True,\n",
    "    # 'attn_implementation': \"flash_attention_2\"\n",
    "}\n",
    "\n",
    "# if not mistral:\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name, **params, token=HF_TOKEN, cache_dir=\"Models\")\n",
    "#     # model = model.to(\"cuda\")\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-2-zephyr-1_6b\", trust_remote_code=False, use_fast=True, cache_dir=\"Models\")\n",
    "# else:\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name, **params, token=HF_TOKEN, cache_dir=\"Models\") # MistralForCausalLM\n",
    "#     # model = model.to(\"cuda\")\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", cache_dir=\"Models\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, **params, token=HF_TOKEN, cache_dir=\"Models\")\n",
    "# model = model.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Meta-Llama-3-8B-Instruct\", cache_dir=\"Models\")\n",
    "\n",
    "neft_noise = 0.0 # bad actually?\n",
    "\n",
    "# rank = 96\n",
    "# config = LoraConfig(\n",
    "#     r=rank, lora_alpha=rank*2,\n",
    "#     target_modules=[\n",
    "#                     'q_proj',\n",
    "#                     'v_proj', \n",
    "#                     \"k_proj\", \n",
    "#                     \"o_proj\", \n",
    "#                     # \"gate_proj\", \n",
    "#                     \"up_proj\", \n",
    "#                     \"down_proj\"\n",
    "#                     ],  #   , \n",
    "#     lora_dropout=0.0,\n",
    "#     bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "#     # use_rslora=True,\n",
    "#     use_dora=True,\n",
    "#     # init_lora_weights=\"gaussian\",\n",
    "# )\n",
    "\n",
    "def norm_model_weights(model):\n",
    "    last_q = None\n",
    "    lqb = None\n",
    "    lqkm = None\n",
    "    last_v = None\n",
    "    lvb = None\n",
    "    lvom = None\n",
    "    last_up = None\n",
    "    bias = False\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"q_proj\" in name:\n",
    "            if \"bias\" in name:\n",
    "                bias = True\n",
    "                lqb = param\n",
    "            else:\n",
    "                last_q = param\n",
    "        if \"k_proj\" in name:\n",
    "            if \"bias\" in name:\n",
    "                if lqkm is not None:\n",
    "                    param.data = param.data * lqkm.flatten()\n",
    "                    pass\n",
    "            else:\n",
    "                if mistral or llama3:\n",
    "                    # print(last_q.data.shape, param.data.shape)\n",
    "                    mult = torch.sqrt(torch.mean(torch.abs(last_q.data), dim=1, keepdim=True) / \n",
    "                                    torch.mean(torch.abs(param.data), dim=1, keepdim=True).repeat(\n",
    "                                                                        int(last_q.data.shape[0] / param.data.shape[0]), 1))\n",
    "                    # print(mult.shape, mult)\n",
    "                    mult = torch.mean(mult)\n",
    "                    # mult = np.sqrt(2.0)\n",
    "                    last_q.data = last_q.data / mult#.transpose(0, 1)\n",
    "                    # if bias: lqb.data = lqb.data / mult#.transpose(0, 1).flatten()\n",
    "                    param.data = param.data * mult#[:param.data.shape[0]] # \n",
    "                    lqkm = mult\n",
    "                    \n",
    "                else:\n",
    "                    # print(last_q.data.shape, param.data.shape)\n",
    "                    # print(name)\n",
    "                    # mult = torch.sqrt(torch.mean(torch.abs(last_q.data), dim=1, keepdim=True) / \n",
    "                    #                 torch.mean(torch.abs(param.data), dim=1, keepdim=True)) # Loss: 0.14939117\n",
    "                    mult = torch.sqrt(torch.mean(torch.abs(last_q.data), dim=1, keepdim=True) / \n",
    "                                    torch.mean(torch.abs(param.data), dim=0, keepdim=True).transpose(0, 1)).transpose(0, 1) # Loss: 0.04619789\n",
    "                    # mult = torch.sqrt(torch.mean(torch.abs(last_q.data), dim=1, keepdim=True).transpose(0, 1) / \n",
    "                    #                 torch.mean(torch.abs(param.data), dim=0, keepdim=True)) # Loss: 0.04619789\n",
    "                    # mult = torch.sqrt(torch.mean(torch.abs(last_q.data), dim=0, keepdim=True).transpose(0, 1) / \n",
    "                    #                 torch.mean(torch.abs(param.data), dim=1, keepdim=True)) # Loss: 0.07823181\n",
    "                    # mult = torch.sqrt(torch.mean(torch.abs(last_q.data), dim=0, keepdim=True) / \n",
    "                    #                 torch.mean(torch.abs(param.data), dim=0, keepdim=True)).transpose(0, 1) # Loss: 0.01167393\n",
    "                    # mult = mult / 2\n",
    "                    # print(mult.shape, mult)\n",
    "                    mult = torch.mean(mult)\n",
    "                    last_q.data = last_q.data / mult#.transpose(0, 1)\n",
    "                    if bias: lqb.data = lqb.data / mult.flatten() # .transpose(0, 1)\n",
    "                    param.data = param.data * mult # \n",
    "                    lqkm = mult\n",
    "                    # print(last_q.data.norm(), param.data.norm())\n",
    "        \n",
    "        if \"v_proj\" in name:\n",
    "            if \"bias\" in name:\n",
    "                lvb = param\n",
    "            else:\n",
    "                last_v = param\n",
    "        if \"o_proj\" in name:\n",
    "            if \"bias\" in name:\n",
    "                param.data = param.data * lvom\n",
    "            else:\n",
    "                # print(last_v.data.shape, param.data.shape)\n",
    "                # print(mult.shape, mult)\n",
    "                if mistral or llama3:\n",
    "                    mult = torch.sqrt(torch.mean(torch.abs(last_v.data), dim=0, keepdim=True) / \n",
    "                            torch.mean(torch.abs(param.data), dim=0, keepdim=True))\n",
    "                    mult = torch.mean(mult)\n",
    "                    # mult = np.sqrt(1.0 / 2.0)\n",
    "                    last_v.data = last_v.data / mult\n",
    "                    if bias: lvb.data = lvb.data / mult\n",
    "                    param.data = param.data * mult\n",
    "                    lvom = mult\n",
    "                else:\n",
    "                    mult = torch.sqrt(torch.mean(torch.abs(last_v.data), dim=1, keepdim=True).transpose(0, 1).repeat(1, \n",
    "                                                                                int(param.data.shape[0] / last_v.data.shape[0])) / \n",
    "                                    torch.mean(torch.abs(param.data), dim=0, keepdim=True))\n",
    "                    last_v.data = last_v.data / mult.transpose(0, 1)[:last_v.data.shape[0]]\n",
    "                    if bias: lvb.data = lvb.data / mult.transpose(0, 1).flatten()\n",
    "                    param.data = param.data * mult\n",
    "                    lvom = mult\n",
    "\n",
    "        if \"up_proj\" in name:\n",
    "            last_up = param\n",
    "        if \"down_proj\" in name:\n",
    "            # print(last_up.data.shape, param.data.shape)\n",
    "            mult = torch.sqrt(torch.mean(torch.abs(last_up.data), dim=1, keepdim=True).transpose(0, 1) / \n",
    "                            torch.mean(torch.abs(param.data), dim=0, keepdim=True))\n",
    "            last_up.data = last_up.data / mult.transpose(0, 1)\n",
    "            param.data = param.data * mult\n",
    "            # print(mult, mult.shape)\n",
    "\n",
    "        # if \"model.norm.weight\" == name:\n",
    "        #     param.data /= 200\n",
    "        # if \"model.norm.bias\" == name:\n",
    "        #     param.data /= 200\n",
    "        # if \"lm_head.weight\" == name:\n",
    "        #     param.data *= 200\n",
    "        \n",
    "    return model\n",
    "\n",
    "model = norm_model_weights(model)\n",
    "model.config.name_or_path = \"MesozoicMetallurgist/\" + lora_name\n",
    "        \n",
    "\n",
    "# model.save_pretrained(\"Models/fixedscaling\")\n",
    "# base_model = None\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(model_name_to_beat, **params, cache_dir=\"Models\")\n",
    "# for name, param in base_model.named_parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "lora_model = model\n",
    "# for name, param in lora_model.named_parameters():\n",
    "#     if not mistral:\n",
    "#         param.requires_grad = True\n",
    "#     else:\n",
    "#         if (\"up_proj\" not in name and \"down_proj\" not in name \n",
    "#             and \"gate_proj\" not in name \n",
    "#             and \"embed_\" not in name\n",
    "#             # and \"q_proj\" not in name\n",
    "#             # and \"k_proj\" not in name \n",
    "#             # and \"v_proj\" not in name \n",
    "#             # and \"o_proj\" not in name \n",
    "#         ):\n",
    "#             param.requires_grad = True\n",
    "#         else:\n",
    "#             param.requires_grad = False\n",
    "\n",
    "# lora_model = PeftModel.from_pretrained(model, model_id=\"Ypresian_\", is_trainable=True)\n",
    "# lora_model = lora_model.merge_and_unload(progressbar=True)\n",
    "# lora_model = get_peft_model(model, config)\n",
    "# lora_model.print_trainable_parameters()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"MesozoicMetallurgist/nous-Hauterivian\", trust_remote_code=False, use_fast=True, cache_dir=\"Models\")\n",
    "# lora_model = lora_model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q_proj': 83.640625, 'k_proj': 43.328125, 'v_proj': 19.912109375, 'o_proj': 39.94921875, 'up_proj': 81.15625, 'down_proj': 81.421875}\n",
      "{'q_proj': 91.0, 'k_proj': 49.5, 'v_proj': 30.5, 'o_proj': 61.0, 'up_proj': 95.5, 'down_proj': 95.5}\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(validate_parameters(lora_model, print_vals=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q_proj': 145.125, 'k_proj': 24.953125, 'v_proj': 180.875, 'o_proj': 4.4208984375, 'up_proj': 204.5, 'down_proj': 32.375}\n",
      "{'q_proj': 158.0, 'k_proj': 26.375, 'v_proj': 322.0, 'o_proj': 5.78125, 'up_proj': 260.0, 'down_proj': 35.75}\n",
      "[0.0, 0.0, 0.25, 0.0, 0.625, 0.0]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(validate_parameters(lora_model, print_vals=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found in eval\n",
      "Evaluating ........ Loss: 0.49611994, Base Loss: 0.403597, Lora Diff: 0.17448707, WR: 0.00%, 0epsWR: 0.00%, OShL: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "train_data, eval_data = get_data(train_name=['data/cortex_686_8704.json',\n",
    "],\n",
    "                                 train_subset=np.arange(0,32),\n",
    "                                 eval_subset=32,\n",
    "                                 eval_name='data/cortex_686_8704.json', tokenizer=tokenizer, shuffle=False)\n",
    "evaluate(lora_model, eval_data, return_to_cpu=True, print_stats=True, base_model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def validate_parameters(base_model, eps_soft=200, eps_soft_percent_threshold=0.15, eps_hard=1000, print_vals=False) -> bool:\n",
    "    \"\"\"\n",
    "    Validate that parameters of a model\n",
    "\n",
    "    Parameters:\n",
    "        base_model (transformers.PreTrainedModel): The base model instance.\n",
    "        num_layers (int): Number of layers in the model to inspect.\n",
    "        eps_soft (float): Calculate the percentage of layers above this norm\n",
    "        eps_soft_percent_threshold (float): Threshold of percentage above eps_soft that will trigger a detection\n",
    "        eps_hard (float): Hard limit for any norm\n",
    "    \"\"\"\n",
    "\n",
    "    exceed_counts = {'q_proj': 0, 'k_proj': 0, 'v_proj': 0, 'o_proj': 0, 'up_proj': 0, 'down_proj': 0}\n",
    "    total_counts = {'q_proj': 0, 'k_proj': 0, 'v_proj': 0, 'o_proj': 0, 'up_proj': 0, 'down_proj': 0}\n",
    "    if print_vals:\n",
    "        avg_norms = {'q_proj': 0.0, 'k_proj': 0.0, 'v_proj': 0.0, 'o_proj': 0.0, 'up_proj': 0.0, 'down_proj': 0.0}\n",
    "        max_norms = {'q_proj': 0.0, 'k_proj': 0.0, 'v_proj': 0.0, 'o_proj': 0.0, 'up_proj': 0.0, 'down_proj': 0.0}\n",
    "\n",
    "    for layer in base_model.model.layers:\n",
    "        for proj in ['q_proj', 'k_proj', 'v_proj', 'o_proj']:\n",
    "            weight_norm = getattr(layer.self_attn, proj).weight.norm().item()\n",
    "            if weight_norm > eps_hard:\n",
    "                return False\n",
    "            elif weight_norm > eps_soft:\n",
    "                exceed_counts[proj] += 1\n",
    "            total_counts[proj] += 1\n",
    "            if print_vals:\n",
    "                avg_norms[proj] += weight_norm\n",
    "                max_norms[proj] = max(max_norms[proj], weight_norm)\n",
    "\n",
    "        # up_proj and down_proj are in the mlp layer\n",
    "        for proj in ['up_proj', 'down_proj']:\n",
    "            weight_norm = getattr(layer.mlp, proj).weight.norm().item()\n",
    "            if weight_norm > eps_hard:\n",
    "                return False\n",
    "            elif weight_norm > eps_soft:\n",
    "                exceed_counts[proj] += 1\n",
    "            total_counts[proj] += 1\n",
    "            if print_vals:\n",
    "                avg_norms[proj] += weight_norm\n",
    "                max_norms[proj] = max(max_norms[proj], weight_norm)\n",
    "\n",
    "    # Calculating and printing percentages\n",
    "    percentages = [exceed_counts[proj] / total_counts[proj] for proj in exceed_counts]\n",
    "\n",
    "    if print_vals:\n",
    "        for key, value in total_counts.items():\n",
    "            avg_norms[key] = avg_norms[key] / value\n",
    "        print(avg_norms)\n",
    "        print(max_norms)\n",
    "        print(percentages)\n",
    "\n",
    "    return statistics.fmean(percentages) <= eps_soft_percent_threshold\n",
    "\n",
    "# print(validate_parameters(lora_model, print_vals=True))\n",
    "# print(validate_parameters(base_model, print_vals=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in lora_model.named_parameters():\n",
    "    if \"lora\" not in name:\n",
    "        print(name, param, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found in eval\n",
      "Evaluating ........ Loss: 0.00985266, Base Loss: 0.403597, Lora Diff: -0.00016907, WR: 0.00%, 0epsWR: 56.25%, OShL: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "train_data, eval_data = get_data(train_name=['data/cortex_686_8704.json',\n",
    "],\n",
    "                                 train_subset=np.arange(0,32),\n",
    "                                 eval_subset=32,\n",
    "                                 eval_name='data/cortex_686_8704.json', tokenizer=tokenizer, shuffle=False)\n",
    "evaluate(lora_model, eval_data, return_to_cpu=True, print_stats=True, base_model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found in eval\n",
      "Evaluating ........ Loss: 0.01000053, Base Loss: 0.665697, Lora Diff: 0.00001144, WR: 2.34%, 0epsWR: 50.29%, OShL: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "train_data, eval_data = get_data(train_name=['data/cortex_686_8704.json',\n",
    "],\n",
    "                                 train_subset=np.arange(0,32),\n",
    "                                 eval_subset=512,\n",
    "                                 eval_name='data/cortex_686_8704.json', tokenizer=tokenizer, shuffle=False)\n",
    "evaluate(lora_model, eval_data, return_to_cpu=True, print_stats=True, base_model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: no training data provided, enabling infinite training\n",
      "WARN: no evaluation data provided, acquiring new data\n",
      "WARN: no training data provided, acquiring new data\n",
      "WARN: simple loss is enabled, this will disable base model processing\n",
      "Base loss processing is disabled, disabling ignore overshot samples\n",
      "Note: precalced eval base loss does not account for pretrained fine-tuning\n",
      "........Eval Base Loss: 0.612937\n",
      "........Step 512/1792\tLoss: 0.827209 OShL: 0.000e+00\tBase: 0.6129 Diff: 2.1427e-01 \tWR: 23.05% 0eps: 23.83%  \tLR: 4.00e-06 fit: 0/0\n",
      "........Step 1024/1280\tLoss: 0.823925 OShL: 0.000e+00\tBase: 0.6129 Diff: 2.1099e-01 \tWR: 19.92% 0eps: 20.70%  \tLR: 8.00e-06 fit: 0/0\n",
      "........Step 1536/1920\tLoss: 0.704217 OShL: 0.000e+00\tBase: 0.6129 Diff: 9.1280e-02 \tWR: 43.75% 0eps: 43.75%  \tLR: 1.20e-05 fit: 0/0\n",
      "........Step 2048/1408\tLoss: 0.679326 OShL: 0.000e+00\tBase: 0.6129 Diff: 6.6389e-02 \tWR: 49.41% 0eps: 50.00%  \tLR: 1.60e-05 fit: 0/0\n",
      "Evaluating ........ Loss: 0.01009590, Base Loss: 0.612937, Lora Diff: -0.00551128, WR: 10.35%, 0epsWR: 49.90%, OShL: 0.00530616\n",
      "........Step 2560/2048\tLoss: 0.693182 OShL: 0.000e+00\tBase: 0.6129 Diff: 8.0245e-02 \tWR: 46.88% 0eps: 47.46%  \tLR: 1.54e-05 fit: 0/0\n",
      "........Step 3072/1536\tLoss: 0.700526 OShL: 0.000e+00\tBase: 0.6129 Diff: 8.7589e-02 \tWR: 41.21% 0eps: 42.77%  \tLR: 1.37e-05 fit: 0/0\n",
      "........Step 3584/2176\tLoss: 0.684402 OShL: 0.000e+00\tBase: 0.6129 Diff: 7.1465e-02 \tWR: 45.51% 0eps: 46.29%  \tLR: 1.11e-05 fit: 0/0\n",
      "........Step 4096/1664\tLoss: 0.648556 OShL: 0.000e+00\tBase: 0.6129 Diff: 3.5619e-02 \tWR: 42.77% 0eps: 43.36%  \tLR: 8.00e-06 fit: 0/0\n",
      "Evaluating ........ Loss: 0.01248724, Base Loss: 0.612937, Lora Diff: -0.01084232, WR: 11.72%, 0epsWR: 42.77%, OShL: 0.01186568\n",
      "........Step 4608/1152\tLoss: 0.646283 OShL: 0.000e+00\tBase: 0.6129 Diff: 3.3346e-02 \tWR: 42.19% 0eps: 42.97%  \tLR: 4.94e-06 fit: 0/0\n",
      "........Step 5120/1792\tLoss: 0.587588 OShL: 0.000e+00\tBase: 0.6129 Diff: -2.5349e-02 \tWR: 48.83% 0eps: 49.80%  \tLR: 2.34e-06 fit: 0/0\n",
      "........Step 5632/1280\tLoss: 0.591800 OShL: 0.000e+00\tBase: 0.6129 Diff: -2.1137e-02 \tWR: 47.27% 0eps: 47.85%  \tLR: 6.09e-07 fit: 0/0\n",
      "........Step 6144/1920\tLoss: 0.628305 OShL: 0.000e+00\tBase: 0.6129 Diff: 1.5368e-02 \tWR: 53.32% 0eps: 53.52%  \tLR: 6.09e-07 fit: 0/0\n",
      "Evaluating ........ Loss: 0.01188055, Base Loss: 0.612937, Lora Diff: -0.01186943, WR: 11.52%, 0epsWR: 44.43%, OShL: 0.01260862\n",
      "........Step 6656/1408\tLoss: 0.595852 OShL: 0.000e+00\tBase: 0.6129 Diff: -1.7085e-02 \tWR: 55.47% 0eps: 56.64%  \tLR: 2.34e-06 fit: 0/0\n",
      "........Step 7168/2048\tLoss: 0.635421 OShL: 0.000e+00\tBase: 0.6129 Diff: 2.2484e-02 \tWR: 46.68% 0eps: 47.46%  \tLR: 4.94e-06 fit: 0/0\n",
      "........Step 7680/1536\tLoss: 0.682914 OShL: 0.000e+00\tBase: 0.6129 Diff: 6.9977e-02 \tWR: 32.03% 0eps: 34.18%  \tLR: 8.00e-06 fit: 0/0\n",
      "........Step 8192/2176\tLoss: 0.651202 OShL: 0.000e+00\tBase: 0.6129 Diff: 3.8265e-02 \tWR: 42.58% 0eps: 43.55%  \tLR: 1.11e-05 fit: 0/0\n",
      "Evaluating ........ Loss: 0.00990951, Base Loss: 0.612937, Lora Diff: -0.01403141, WR: 13.28%, 0epsWR: 51.86%, OShL: 0.01376878\n",
      "........Step 8704/1664\tLoss: 0.547860 OShL: 0.000e+00\tBase: 0.6129 Diff: -6.5077e-02 \tWR: 56.64% 0eps: 57.23%  \tLR: 1.37e-05 fit: 0/0\n",
      "........Step 9216/1152\tLoss: 0.549648 OShL: 0.000e+00\tBase: 0.6129 Diff: -6.3289e-02 \tWR: 55.47% 0eps: 56.25%  \tLR: 1.54e-05 fit: 0/0\n",
      "........Step 9728/1792\tLoss: 0.745390 OShL: 0.000e+00\tBase: 0.6129 Diff: 1.3245e-01 \tWR: 42.38% 0eps: 43.36%  \tLR: 1.60e-05 fit: 0/0\n",
      "........Step 10240/1280\tLoss: 0.741829 OShL: 0.000e+00\tBase: 0.6129 Diff: 1.2889e-01 \tWR: 43.95% 0eps: 43.95%  \tLR: 1.54e-05 fit: 0/0\n",
      "Evaluating ........ Loss: 0.00808356, Base Loss: 0.612937, Lora Diff: -0.01649380, WR: 16.60%, 0epsWR: 61.52%, OShL: 0.01531944\n",
      "........Step 10752/1920\tLoss: 0.655207 OShL: 0.000e+00\tBase: 0.6129 Diff: 4.2270e-02 \tWR: 47.85% 0eps: 48.44%  \tLR: 1.37e-05 fit: 0/0\n",
      "........Step 11264/1408\tLoss: 0.665123 OShL: 0.000e+00\tBase: 0.6129 Diff: 5.2186e-02 \tWR: 49.02% 0eps: 50.00%  \tLR: 1.11e-05 fit: 0/0\n",
      "........Step 11776/2048\tLoss: 0.606694 OShL: 0.000e+00\tBase: 0.6129 Diff: -6.2432e-03 \tWR: 54.10% 0eps: 55.27%  \tLR: 8.00e-06 fit: 0/0\n",
      "........Step 12288/1536\tLoss: 0.586838 OShL: 0.000e+00\tBase: 0.6129 Diff: -2.6099e-02 \tWR: 60.16% 0eps: 61.13%  \tLR: 4.94e-06 fit: 0/0\n",
      "Evaluating ........ Loss: 0.00906870, Base Loss: 0.612937, Lora Diff: -0.01590538, WR: 14.06%, 0epsWR: 55.27%, OShL: 0.01526577\n",
      "........Step 12800/2176\tLoss: 0.591562 OShL: 0.000e+00\tBase: 0.6129 Diff: -2.1375e-02 \tWR: 57.81% 0eps: 58.40%  \tLR: 2.34e-06 fit: 0/0\n",
      "........Step 13312/1664\tLoss: 0.589174 OShL: 0.000e+00\tBase: 0.6129 Diff: -2.3763e-02 \tWR: 55.66% 0eps: 56.25%  \tLR: 6.09e-07 fit: 0/0\n",
      "........Step 13824/1152\tLoss: 0.559789 OShL: 0.000e+00\tBase: 0.6129 Diff: -5.3148e-02 \tWR: 63.28% 0eps: 63.87%  \tLR: 6.09e-07 fit: 0/0\n",
      "........Step 14336/1792\tLoss: 0.580971 OShL: 0.000e+00\tBase: 0.6129 Diff: -3.1965e-02 \tWR: 55.27% 0eps: 56.45%  \tLR: 2.34e-06 fit: 0/0\n",
      "Evaluating ........ Loss: 0.00879765, Base Loss: 0.612937, Lora Diff: -0.01613903, WR: 14.26%, 0epsWR: 56.93%, OShL: 0.01536123\n",
      "........Step 14848/1280\tLoss: 0.596830 OShL: 0.000e+00\tBase: 0.6129 Diff: -1.6107e-02 \tWR: 52.73% 0eps: 52.93%  \tLR: 4.94e-06 fit: 0/0\n",
      "........Step 15360/1920\tLoss: 0.689488 OShL: 0.000e+00\tBase: 0.6129 Diff: 7.6551e-02 \tWR: 38.87% 0eps: 40.04%  \tLR: 8.00e-06 fit: 0/0\n",
      "........Step 15872/1408\tLoss: 0.701324 OShL: 0.000e+00\tBase: 0.6129 Diff: 8.8387e-02 \tWR: 38.28% 0eps: 39.06%  \tLR: 1.11e-05 fit: 0/0\n",
      "........Step 16384/2048\tLoss: 0.744110 OShL: 0.000e+00\tBase: 0.6129 Diff: 1.3117e-01 \tWR: 33.20% 0eps: 33.79%  \tLR: 1.37e-05 fit: 0/0\n",
      "Evaluating ........ Loss: 0.00805564, Base Loss: 0.612937, Lora Diff: -0.01684570, WR: 16.41%, 0epsWR: 62.21%, OShL: 0.01565410\n",
      "...."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m eval_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minf_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43macc_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction_finetuing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecalculate_batch_mult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.6e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_end_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msam_rho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madaptive_sam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwsam_variant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madamw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_eps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43movershoot_buffer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_below\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_auto_percent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremerging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# remerging=True, revert=True,\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremerge_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremerge_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_revert_if\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhead_to_head\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m6.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps0_head_to_head\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m12.5\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimple_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_base_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecalc_eval_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmanual_grad_clip_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msam_grad_clip_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_overshoot_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_overshot_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbad_sample_mult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_n_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexcessive_cache_clearing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 575\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, tokenizer, train_d, eval_d, base_model, inf_training, inf_data_steps, inf_data_decay, training_device, acc_batch_size, instruction_finetuing, precalculate_batch_mult, precalc_eval_base, lr, weight_decay, lr_scheduler, warmup_steps, warmup_end_offset, betas, use_sam, sam_rho, opt, adaptive_sam, wsam_variant, ignore_below, ignore_auto_percent, manual_grad_clip_norm, sam_grad_clip_norm, add_overshoot_penalty, ignore_overshot_samples, bad_sample_mult, remerging, remerge_eval, remerge_ratio, loss_eps, overshoot_buffer, true_eps, simple_loss, process_base_loss, relative_loss, eval_steps, save_name, do_save, eval_revert_if, average_stats, save_n_start, revert, gradient_checkpointing, excessive_cache_clearing, base_model_switching)\u001b[0m\n\u001b[1;32m    572\u001b[0m loss, overshoot_penalty \u001b[38;5;241m=\u001b[39m combined_loss(outputs_loss, base_outputs_loss)\n\u001b[1;32m    573\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m acc_batch_size\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m ignore_overshot_samples \u001b[38;5;129;01mor\u001b[39;00m overshoot_penalty \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43moutputs_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m ignore_below:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m simple_loss \u001b[38;5;129;01mand\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m>\u001b[39m ((loss_eps \u001b[38;5;241m/\u001b[39m acc_batch_size)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-8\u001b[39m):\n\u001b[1;32m    577\u001b[0m         unfit_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "eval_data = []\n",
    "train(lora_model, tokenizer, train_data, eval_data, base_model=base_model, inf_training=False, training_device=\"cuda\",\n",
    "        acc_batch_size=512, instruction_finetuing=True, precalculate_batch_mult=2.25,\n",
    "        lr=1.6e-5, weight_decay=0.0, lr_scheduler=\"cosine\", warmup_steps=4, warmup_end_offset=10, betas=(0.8, 0.95),\n",
    "        use_sam=False, sam_rho=0.05, adaptive_sam=False, wsam_variant=True,\n",
    "        opt=\"adamw\",\n",
    "        loss_eps = 0.02, overshoot_buffer = -0.01, true_eps=0.01, ignore_below=0.0, ignore_auto_percent=1,\n",
    "        remerging=False, revert=False,\n",
    "        # remerging=True, revert=True,\n",
    "        remerge_eval=False, remerge_ratio=0.3, eval_revert_if={\"loss\": 0.001, \"head_to_head\": -6.25, \"eps0_head_to_head\": -12.5},\n",
    "        simple_loss=True, process_base_loss=True, precalc_eval_base=True, relative_loss=False,\n",
    "        manual_grad_clip_norm=1.0, sam_grad_clip_norm=None,\n",
    "        add_overshoot_penalty=False, ignore_overshot_samples=True, bad_sample_mult=1.0,\n",
    "        eval_steps=2048, do_save=False, save_name=lora_name, save_n_start=0,\n",
    "        average_stats=False,\n",
    "        gradient_checkpointing=False, excessive_cache_clearing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da03a29fad2475dba74f703152e69cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Save the model\n",
    "lora_model.save_pretrained(lora_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = lora_model.to(\"cuda\")\n",
    "lora_model = lora_model.merge_and_unload()\n",
    "# lora_model = norm_model_weights(lora_model)\n",
    "# simple_eval(lora_model, eval_data)\n",
    "lora_model = lora_model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.config.name_or_path = \"MesozoicMetallurgist/new_model\"\n",
    "# model_dir = \"Models/merged_model\"\n",
    "model_dir = os.path.expanduser(\"~/finetuning-subnet/merged_model_l3\")\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "else:\n",
    "    # wipe the directory\n",
    "    for file in os.listdir(model_dir):\n",
    "        os.remove(os.path.join(model_dir, file))\n",
    "lora_model.save_pretrained(save_directory=model_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(save_directory=model_dir)\n",
    "lora_model = lora_model.to(\"cpu\")\n",
    "gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ........ Loss: 0.00798187, Base Loss: 0.557989, Lora Diff: -0.00123047, WR: 18.10%, 0epsWR: 66.80%, OShL: 0.00037500\n"
     ]
    }
   ],
   "source": [
    "evaluate(lora_model, eval_data, return_to_cpu=True, print_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_model = AutoModelForCausalLM.from_pretrained(model_name, **params, cache_dir=\"Models\")\n",
    "\n",
    "lora_model = lora_model.to(\"cuda\")\n",
    "# lora_model = lora_model.merge_and_unload()\n",
    "# model_dir = \"Models/merged_model\"\n",
    "# lora_model.save_pretrained(save_directory=model_dir, safe_serialization=True)\n",
    "# tokenizer.save_pretrained(save_directory=model_dir)\n",
    "\n",
    "last_q = None\n",
    "last_v = None\n",
    "last_up = None\n",
    "for name, param  in lora_model.named_parameters():\n",
    "    # if \"q_proj\" in name:\n",
    "    #     last_q = param\n",
    "    # elif \"k_proj\" in name:\n",
    "    #     # print(last_q.data.shape, param.data.shape)\n",
    "    #     # mult = (torch.mean(torch.abs(last_q.data), dim=0, keepdim=True) / torch.mean(torch.abs(param.data), dim=0)) ** 0.5\n",
    "    #     mult = (torch.mean(torch.abs(last_q.data)) / torch.mean(torch.abs(param.data))) ** 0.5\n",
    "    #     mult = 100.0\n",
    "    #     # print(mult.shape)\n",
    "    #     last_q.data = last_q.data / mult#.transpose(0, 1)\n",
    "    #     param.data = param.data * mult\n",
    "    #     last_q = None\n",
    "    #     # print(mult)\n",
    "    # if \"v_proj\" in name:\n",
    "    #     last_v = param\n",
    "    # elif \"o_proj\" in name:\n",
    "    #     # print(last_v.data.shape, param.data.shape)\n",
    "    #     # mult = (torch.mean(torch.abs(last_v.data), dim=0, keepdim=True) / torch.mean(torch.abs(param.data), dim=1)) ** 0.5\n",
    "    #     mult = (torch.mean(torch.abs(last_v.data)) / torch.mean(torch.abs(param.data))) ** 0.5\n",
    "    #     mult = 100.0\n",
    "    #     # print(mult.shape)\n",
    "    #     last_v.data = last_v.data / mult\n",
    "    #     param.data = param.data * mult#.transpose(0, 1)\n",
    "    #     last_v = None\n",
    "        # print(mult)\n",
    "    # get current random seed from torch\n",
    "    iseed = torch.initial_seed()\n",
    "    # set the random seed to a fixed value\n",
    "    torch.manual_seed(42)\n",
    "    if \"up_proj\" in name:\n",
    "        last_up = param\n",
    "    elif \"down_proj\" in name:\n",
    "        # print(last_up.data.shape, param.data.shape)\n",
    "        mult = (torch.mean(torch.abs(last_up.data), dim=1, keepdim=True).transpose(0, 1) / torch.mean(torch.abs(param.data), dim=0)) ** 0.5\n",
    "        mult = torch.randint_like(mult, 1, 128, dtype=torch.bfloat16)\n",
    "        # print(mult.shape)\n",
    "        last_up.data = last_up.data / mult.transpose(0, 1)\n",
    "        param.data = param.data * mult\n",
    "        last_up = None\n",
    "        # print(mult)\n",
    "    # set the random seed back to the original value\n",
    "    torch.manual_seed(iseed)\n",
    "\n",
    "model_dir = \"Models/merged_model_fuckkery\"\n",
    "lora_model.save_pretrained(save_directory=model_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(save_directory=model_dir)\n",
    "lora_model = lora_model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ........ Loss: 0.01010798, Base Loss: 0.707643, Lora Diff: 0.00000000, WR: 0.00%, 0epsWR: 50.00%, OShL: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "lora_model = get_peft_model(model, config)\n",
    "evaluate(lora_model, eval_data, return_to_cpu=True, print_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = lora_model.to(\"cuda\")\n",
    "lora_model = lora_model.merge_and_unload()\n",
    "lora_model.save_pretrained(lora_name+'fuckkery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
