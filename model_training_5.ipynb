{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "from cortexsubsetloader import CortexSubsetLoader\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76163c0c054449bca6bf5685ae94180e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "mistral=False\n",
    "llama3 = True\n",
    "if not llama3:\n",
    "    if not mistral:\n",
    "        # lora_name \"Carboniferous\"\n",
    "        lora_name = \"Carnian\"\n",
    "        model_name = \"\"\n",
    "        model_name_to_beat = model_name\n",
    "    else:\n",
    "        lora_name = \"Helium\"\n",
    "        model_name = \"\"\n",
    "        model_name_to_beat = model_name\n",
    "else:\n",
    "    lora_name = \"Hadean\"\n",
    "    model_name = \"\"\n",
    "    model_name_to_beat = model_name\n",
    "\n",
    "params = {\n",
    "    'low_cpu_mem_usage': True,\n",
    "    'trust_remote_code': False,\n",
    "    'torch_dtype': torch.bfloat16,\n",
    "    'use_safetensors': True,\n",
    "    # 'attn_implementation': \"flash_attention_2\"\n",
    "}\n",
    "\n",
    "# if not mistral:\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name, **params, token=HF_TOKEN, cache_dir=\"Models\")\n",
    "#     # model = model.to(\"cuda\")\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-2-zephyr-1_6b\", trust_remote_code=False, use_fast=True, cache_dir=\"Models\")\n",
    "# else:\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name, **params, token=HF_TOKEN, cache_dir=\"Models\") # MistralForCausalLM\n",
    "#     # model = model.to(\"cuda\")\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", cache_dir=\"Models\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, **params, token=HF_TOKEN, cache_dir=\"Models\")\n",
    "# model = model.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Meta-Llama-3-8B-Instruct\", cache_dir=\"Models\")\n",
    "\n",
    "# rank = 96\n",
    "# config = LoraConfig(\n",
    "#     r=rank, lora_alpha=rank*2,\n",
    "#     target_modules=[\n",
    "#                     'q_proj',\n",
    "#                     'v_proj', \n",
    "#                     \"k_proj\", \n",
    "#                     \"o_proj\", \n",
    "#                     # \"gate_proj\", \n",
    "#                     \"up_proj\", \n",
    "#                     \"down_proj\"\n",
    "#                     ],  #   , \n",
    "#     lora_dropout=0.0,\n",
    "#     bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "#     # use_rslora=True,\n",
    "#     use_dora=True,\n",
    "#     # init_lora_weights=\"gaussian\",\n",
    "# )\n",
    "\n",
    "from utils import norm_model_weights\n",
    "\n",
    "model = norm_model_weights(model)\n",
    "model.config.name_or_path = \"MesozoicMetallurgist/\" + lora_name\n",
    "        \n",
    "\n",
    "# model.save_pretrained(\"Models/fixedscaling\")\n",
    "# base_model = None\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(model_name_to_beat, **params, cache_dir=\"Models\")\n",
    "# for name, param in base_model.named_parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "lora_model = model\n",
    "# for name, param in lora_model.named_parameters():\n",
    "#     if not mistral:\n",
    "#         param.requires_grad = True\n",
    "#     else:\n",
    "#         if (\"up_proj\" not in name and \"down_proj\" not in name \n",
    "#             and \"gate_proj\" not in name \n",
    "#             and \"embed_\" not in name\n",
    "#             # and \"q_proj\" not in name\n",
    "#             # and \"k_proj\" not in name \n",
    "#             # and \"v_proj\" not in name \n",
    "#             # and \"o_proj\" not in name \n",
    "#         ):\n",
    "#             param.requires_grad = True\n",
    "#         else:\n",
    "#             param.requires_grad = False\n",
    "\n",
    "# lora_model = PeftModel.from_pretrained(model, model_id=\"Ypresian_\", is_trainable=True)\n",
    "# lora_model = lora_model.merge_and_unload(progressbar=True)\n",
    "# lora_model = get_peft_model(model, config)\n",
    "# lora_model.print_trainable_parameters()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"MesozoicMetallurgist/nous-Hauterivian\", trust_remote_code=False, use_fast=True, cache_dir=\"Models\")\n",
    "# lora_model = lora_model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q_proj': 83.640625, 'k_proj': 43.328125, 'v_proj': 19.912109375, 'o_proj': 39.94921875, 'up_proj': 81.15625, 'down_proj': 81.421875}\n",
      "{'q_proj': 91.0, 'k_proj': 49.5, 'v_proj': 30.5, 'o_proj': 61.0, 'up_proj': 95.5, 'down_proj': 95.5}\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(validate_parameters(lora_model, print_vals=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q_proj': 145.125, 'k_proj': 24.953125, 'v_proj': 180.875, 'o_proj': 4.4208984375, 'up_proj': 204.5, 'down_proj': 32.375}\n",
      "{'q_proj': 158.0, 'k_proj': 26.375, 'v_proj': 322.0, 'o_proj': 5.78125, 'up_proj': 260.0, 'down_proj': 35.75}\n",
      "[0.0, 0.0, 0.25, 0.0, 0.625, 0.0]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(validate_parameters(lora_model, print_vals=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found in eval\n",
      "Evaluating ........ Loss: 0.49611994, Base Loss: 0.403597, Lora Diff: 0.17448707, WR: 0.00%, 0epsWR: 0.00%, OShL: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "train_data, eval_data = get_data(train_name=['data/cortex_686_8704.json',\n",
    "],\n",
    "                                 train_subset=np.arange(0,32),\n",
    "                                 eval_subset=32,\n",
    "                                 eval_name='data/cortex_686_8704.json', tokenizer=tokenizer, shuffle=False)\n",
    "evaluate(lora_model, eval_data, return_to_cpu=True, print_stats=True, base_model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validation import validate_parameters\n",
    "\n",
    "# print(validate_parameters(lora_model, print_vals=True))\n",
    "# print(validate_parameters(base_model, print_vals=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.name_or_path = upload_name\n",
    "tokenizer.push_to_hub(repo_id=upload_name, private=True)\n",
    "commit_info = model.push_to_hub(repo_id=upload_name, safe_serialization=True, private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in lora_model.named_parameters():\n",
    "    if \"lora\" not in name:\n",
    "        print(name, param, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found in eval\n",
      "Evaluating ........ Loss: 0.00985266, Base Loss: 0.403597, Lora Diff: -0.00016907, WR: 0.00%, 0epsWR: 56.25%, OShL: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "train_data, eval_data = get_data(train_name=['data/cortex_686_8704.json',\n",
    "],\n",
    "                                 train_subset=np.arange(0,32),\n",
    "                                 eval_subset=32,\n",
    "                                 eval_name='data/cortex_686_8704.json', tokenizer=tokenizer, shuffle=False)\n",
    "evaluate(lora_model, eval_data, return_to_cpu=True, print_stats=True, base_model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found in eval\n",
      "Evaluating ........ Loss: 0.01000053, Base Loss: 0.665697, Lora Diff: 0.00001144, WR: 2.34%, 0epsWR: 50.29%, OShL: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "train_data, eval_data = get_data(train_name=['data/cortex_686_8704.json',\n",
    "],\n",
    "                                 train_subset=np.arange(0,32),\n",
    "                                 eval_subset=512,\n",
    "                                 eval_name='data/cortex_686_8704.json', tokenizer=tokenizer, shuffle=False)\n",
    "evaluate(lora_model, eval_data, return_to_cpu=True, print_stats=True, base_model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: no training data provided, enabling infinite training\n",
      "WARN: no evaluation data provided, acquiring new data\n",
      "WARN: no training data provided, acquiring new data\n",
      "WARN: simple loss is enabled, this will disable base model processing\n",
      "Base loss processing is disabled, disabling ignore overshot samples\n",
      "Note: precalced eval base loss does not account for pretrained fine-tuning\n",
      "........Eval Base Loss: 0.612937\n",
      "........Step 512/1792\tLoss: 0.827209 OShL: 0.000e+00\tBase: 0.6129 Diff: 2.1427e-01 \tWR: 23.05% 0eps: 23.83%  \tLR: 4.00e-06 fit: 0/0\n",
      "........Step 1024/1280\tLoss: 0.823925 OShL: 0.000e+00\tBase: 0.6129 Diff: 2.1099e-01 \tWR: 19.92% 0eps: 20.70%  \tLR: 8.00e-06 fit: 0/0\n",
      "........Step 1536/1920\tLoss: 0.704217 OShL: 0.000e+00\tBase: 0.6129 Diff: 9.1280e-02 \tWR: 43.75% 0eps: 43.75%  \tLR: 1.20e-05 fit: 0/0\n",
      "........Step 2048/1408\tLoss: 0.679326 OShL: 0.000e+00\tBase: 0.6129 Diff: 6.6389e-02 \tWR: 49.41% 0eps: 50.00%  \tLR: 1.60e-05 fit: 0/0\n",
      "Evaluating ........ Loss: 0.01009590, Base Loss: 0.612937, Lora Diff: -0.00551128, WR: 10.35%, 0epsWR: 49.90%, OShL: 0.00530616\n",
      "........Step 2560/2048\tLoss: 0.693182 OShL: 0.000e+00\tBase: 0.6129 Diff: 8.0245e-02 \tWR: 46.88% 0eps: 47.46%  \tLR: 1.54e-05 fit: 0/0\n",
      "........Step 3072/1536\tLoss: 0.700526 OShL: 0.000e+00\tBase: 0.6129 Diff: 8.7589e-02 \tWR: 41.21% 0eps: 42.77%  \tLR: 1.37e-05 fit: 0/0\n",
      "........Step 3584/2176\tLoss: 0.684402 OShL: 0.000e+00\tBase: 0.6129 Diff: 7.1465e-02 \tWR: 45.51% 0eps: 46.29%  \tLR: 1.11e-05 fit: 0/0\n",
      "........Step 4096/1664\tLoss: 0.648556 OShL: 0.000e+00\tBase: 0.6129 Diff: 3.5619e-02 \tWR: 42.77% 0eps: 43.36%  \tLR: 8.00e-06 fit: 0/0\n",
      "Evaluating ........ Loss: 0.01248724, Base Loss: 0.612937, Lora Diff: -0.01084232, WR: 11.72%, 0epsWR: 42.77%, OShL: 0.01186568\n",
      "........Step 4608/1152\tLoss: 0.646283 OShL: 0.000e+00\tBase: 0.6129 Diff: 3.3346e-02 \tWR: 42.19% 0eps: 42.97%  \tLR: 4.94e-06 fit: 0/0\n",
      "........Step 5120/1792\tLoss: 0.587588 OShL: 0.000e+00\tBase: 0.6129 Diff: -2.5349e-02 \tWR: 48.83% 0eps: 49.80%  \tLR: 2.34e-06 fit: 0/0\n",
      "........Step 5632/1280\tLoss: 0.591800 OShL: 0.000e+00\tBase: 0.6129 Diff: -2.1137e-02 \tWR: 47.27% 0eps: 47.85%  \tLR: 6.09e-07 fit: 0/0\n",
      "........Step 6144/1920\tLoss: 0.628305 OShL: 0.000e+00\tBase: 0.6129 Diff: 1.5368e-02 \tWR: 53.32% 0eps: 53.52%  \tLR: 6.09e-07 fit: 0/0\n",
      "Evaluating ........ Loss: 0.01188055, Base Loss: 0.612937, Lora Diff: -0.01186943, WR: 11.52%, 0epsWR: 44.43%, OShL: 0.01260862\n",
      "........Step 6656/1408\tLoss: 0.595852 OShL: 0.000e+00\tBase: 0.6129 Diff: -1.7085e-02 \tWR: 55.47% 0eps: 56.64%  \tLR: 2.34e-06 fit: 0/0\n",
      "........Step 7168/2048\tLoss: 0.635421 OShL: 0.000e+00\tBase: 0.6129 Diff: 2.2484e-02 \tWR: 46.68% 0eps: 47.46%  \tLR: 4.94e-06 fit: 0/0\n",
      "........Step 7680/1536\tLoss: 0.682914 OShL: 0.000e+00\tBase: 0.6129 Diff: 6.9977e-02 \tWR: 32.03% 0eps: 34.18%  \tLR: 8.00e-06 fit: 0/0\n",
      "........Step 8192/2176\tLoss: 0.651202 OShL: 0.000e+00\tBase: 0.6129 Diff: 3.8265e-02 \tWR: 42.58% 0eps: 43.55%  \tLR: 1.11e-05 fit: 0/0\n",
      "Evaluating ........ Loss: 0.00990951, Base Loss: 0.612937, Lora Diff: -0.01403141, WR: 13.28%, 0epsWR: 51.86%, OShL: 0.01376878\n",
      "........Step 8704/1664\tLoss: 0.547860 OShL: 0.000e+00\tBase: 0.6129 Diff: -6.5077e-02 \tWR: 56.64% 0eps: 57.23%  \tLR: 1.37e-05 fit: 0/0\n",
      "........Step 9216/1152\tLoss: 0.549648 OShL: 0.000e+00\tBase: 0.6129 Diff: -6.3289e-02 \tWR: 55.47% 0eps: 56.25%  \tLR: 1.54e-05 fit: 0/0\n",
      "........Step 9728/1792\tLoss: 0.745390 OShL: 0.000e+00\tBase: 0.6129 Diff: 1.3245e-01 \tWR: 42.38% 0eps: 43.36%  \tLR: 1.60e-05 fit: 0/0\n",
      "........Step 10240/1280\tLoss: 0.741829 OShL: 0.000e+00\tBase: 0.6129 Diff: 1.2889e-01 \tWR: 43.95% 0eps: 43.95%  \tLR: 1.54e-05 fit: 0/0\n",
      "Evaluating ........ Loss: 0.00808356, Base Loss: 0.612937, Lora Diff: -0.01649380, WR: 16.60%, 0epsWR: 61.52%, OShL: 0.01531944\n",
      "........Step 10752/1920\tLoss: 0.655207 OShL: 0.000e+00\tBase: 0.6129 Diff: 4.2270e-02 \tWR: 47.85% 0eps: 48.44%  \tLR: 1.37e-05 fit: 0/0\n",
      "........Step 11264/1408\tLoss: 0.665123 OShL: 0.000e+00\tBase: 0.6129 Diff: 5.2186e-02 \tWR: 49.02% 0eps: 50.00%  \tLR: 1.11e-05 fit: 0/0\n",
      "........Step 11776/2048\tLoss: 0.606694 OShL: 0.000e+00\tBase: 0.6129 Diff: -6.2432e-03 \tWR: 54.10% 0eps: 55.27%  \tLR: 8.00e-06 fit: 0/0\n",
      "........Step 12288/1536\tLoss: 0.586838 OShL: 0.000e+00\tBase: 0.6129 Diff: -2.6099e-02 \tWR: 60.16% 0eps: 61.13%  \tLR: 4.94e-06 fit: 0/0\n",
      "Evaluating ........ Loss: 0.00906870, Base Loss: 0.612937, Lora Diff: -0.01590538, WR: 14.06%, 0epsWR: 55.27%, OShL: 0.01526577\n",
      "........Step 12800/2176\tLoss: 0.591562 OShL: 0.000e+00\tBase: 0.6129 Diff: -2.1375e-02 \tWR: 57.81% 0eps: 58.40%  \tLR: 2.34e-06 fit: 0/0\n",
      "........Step 13312/1664\tLoss: 0.589174 OShL: 0.000e+00\tBase: 0.6129 Diff: -2.3763e-02 \tWR: 55.66% 0eps: 56.25%  \tLR: 6.09e-07 fit: 0/0\n",
      "........Step 13824/1152\tLoss: 0.559789 OShL: 0.000e+00\tBase: 0.6129 Diff: -5.3148e-02 \tWR: 63.28% 0eps: 63.87%  \tLR: 6.09e-07 fit: 0/0\n",
      "........Step 14336/1792\tLoss: 0.580971 OShL: 0.000e+00\tBase: 0.6129 Diff: -3.1965e-02 \tWR: 55.27% 0eps: 56.45%  \tLR: 2.34e-06 fit: 0/0\n",
      "Evaluating ........ Loss: 0.00879765, Base Loss: 0.612937, Lora Diff: -0.01613903, WR: 14.26%, 0epsWR: 56.93%, OShL: 0.01536123\n",
      "........Step 14848/1280\tLoss: 0.596830 OShL: 0.000e+00\tBase: 0.6129 Diff: -1.6107e-02 \tWR: 52.73% 0eps: 52.93%  \tLR: 4.94e-06 fit: 0/0\n",
      "........Step 15360/1920\tLoss: 0.689488 OShL: 0.000e+00\tBase: 0.6129 Diff: 7.6551e-02 \tWR: 38.87% 0eps: 40.04%  \tLR: 8.00e-06 fit: 0/0\n",
      "........Step 15872/1408\tLoss: 0.701324 OShL: 0.000e+00\tBase: 0.6129 Diff: 8.8387e-02 \tWR: 38.28% 0eps: 39.06%  \tLR: 1.11e-05 fit: 0/0\n",
      "........Step 16384/2048\tLoss: 0.744110 OShL: 0.000e+00\tBase: 0.6129 Diff: 1.3117e-01 \tWR: 33.20% 0eps: 33.79%  \tLR: 1.37e-05 fit: 0/0\n",
      "Evaluating ........ Loss: 0.00805564, Base Loss: 0.612937, Lora Diff: -0.01684570, WR: 16.41%, 0epsWR: 62.21%, OShL: 0.01565410\n",
      "...."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m eval_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minf_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43macc_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction_finetuing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecalculate_batch_mult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.6e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_end_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msam_rho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madaptive_sam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwsam_variant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madamw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_eps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43movershoot_buffer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_below\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_auto_percent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremerging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# remerging=True, revert=True,\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremerge_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremerge_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_revert_if\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhead_to_head\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m6.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps0_head_to_head\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m12.5\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimple_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_base_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecalc_eval_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmanual_grad_clip_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msam_grad_clip_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_overshoot_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_overshot_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbad_sample_mult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_n_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexcessive_cache_clearing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 575\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, tokenizer, train_d, eval_d, base_model, inf_training, inf_data_steps, inf_data_decay, training_device, acc_batch_size, instruction_finetuing, precalculate_batch_mult, precalc_eval_base, lr, weight_decay, lr_scheduler, warmup_steps, warmup_end_offset, betas, use_sam, sam_rho, opt, adaptive_sam, wsam_variant, ignore_below, ignore_auto_percent, manual_grad_clip_norm, sam_grad_clip_norm, add_overshoot_penalty, ignore_overshot_samples, bad_sample_mult, remerging, remerge_eval, remerge_ratio, loss_eps, overshoot_buffer, true_eps, simple_loss, process_base_loss, relative_loss, eval_steps, save_name, do_save, eval_revert_if, average_stats, save_n_start, revert, gradient_checkpointing, excessive_cache_clearing, base_model_switching)\u001b[0m\n\u001b[1;32m    572\u001b[0m loss, overshoot_penalty \u001b[38;5;241m=\u001b[39m combined_loss(outputs_loss, base_outputs_loss)\n\u001b[1;32m    573\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m acc_batch_size\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m ignore_overshot_samples \u001b[38;5;129;01mor\u001b[39;00m overshoot_penalty \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43moutputs_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m ignore_below:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m simple_loss \u001b[38;5;129;01mand\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m>\u001b[39m ((loss_eps \u001b[38;5;241m/\u001b[39m acc_batch_size)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-8\u001b[39m):\n\u001b[1;32m    577\u001b[0m         unfit_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "eval_data = []\n",
    "train(lora_model, tokenizer, train_data, eval_data, base_model=base_model, inf_training=False, training_device=\"cuda\",\n",
    "        acc_batch_size=512, instruction_finetuing=True, precalculate_batch_mult=2.25,\n",
    "        lr=1.6e-5, weight_decay=0.0, lr_scheduler=\"cosine\", warmup_steps=4, warmup_end_offset=10, betas=(0.8, 0.95),\n",
    "        use_sam=False, sam_rho=0.05, adaptive_sam=False, wsam_variant=True,\n",
    "        opt=\"adamw\",\n",
    "        loss_eps = 0.02, overshoot_buffer = -0.01, true_eps=0.01, ignore_below=0.0, ignore_auto_percent=1,\n",
    "        remerging=False, revert=False,\n",
    "        # remerging=True, revert=True,\n",
    "        remerge_eval=False, remerge_ratio=0.3, eval_revert_if={\"loss\": 0.001, \"head_to_head\": -6.25, \"eps0_head_to_head\": -12.5},\n",
    "        simple_loss=True, process_base_loss=True, precalc_eval_base=True, relative_loss=False,\n",
    "        manual_grad_clip_norm=1.0, sam_grad_clip_norm=None,\n",
    "        add_overshoot_penalty=False, ignore_overshot_samples=True, bad_sample_mult=1.0,\n",
    "        eval_steps=2048, do_save=False, save_name=lora_name, save_n_start=0,\n",
    "        average_stats=False,\n",
    "        gradient_checkpointing=False, excessive_cache_clearing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_improvement(\"tomaszki/stablelm-1\", \"MesozoicMetallurgist/zeta-Ladinian\", False, n_runs=1, samples=768, dedup=False)\n",
    "validate_improvement(\"MesozoicMetallurgist/zeta-Anisian\", \"0x0dad0/beta_s03\", False, n_runs=1, samples=768, dedup=False)\n",
    "validate_improvement(\"MesozoicMetallurgist/zeta-Anisian\", \"tomaszki/stablelm-0\", False, n_runs=1, samples=768, dedup=False)\n",
    "validate_improvement(\"MesozoicMetallurgist/zeta-Induan\", \"MesozoicMetallurgist/zeta-Anisian\", False, n_runs=1, samples=768, dedup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da03a29fad2475dba74f703152e69cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Save the model\n",
    "lora_model.save_pretrained(lora_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = lora_model.to(\"cuda\")\n",
    "lora_model = lora_model.merge_and_unload()\n",
    "# lora_model = norm_model_weights(lora_model)\n",
    "# simple_eval(lora_model, eval_data)\n",
    "lora_model = lora_model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.config.name_or_path = \"MesozoicMetallurgist/new_model\"\n",
    "# model_dir = \"Models/merged_model\"\n",
    "model_dir = os.path.expanduser(\"~/finetuning-subnet/merged_model_l3\")\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "else:\n",
    "    # wipe the directory\n",
    "    for file in os.listdir(model_dir):\n",
    "        os.remove(os.path.join(model_dir, file))\n",
    "lora_model.save_pretrained(save_directory=model_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(save_directory=model_dir)\n",
    "lora_model = lora_model.to(\"cpu\")\n",
    "gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ........ Loss: 0.00798187, Base Loss: 0.557989, Lora Diff: -0.00123047, WR: 18.10%, 0epsWR: 66.80%, OShL: 0.00037500\n"
     ]
    }
   ],
   "source": [
    "evaluate(lora_model, eval_data, return_to_cpu=True, print_stats=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
