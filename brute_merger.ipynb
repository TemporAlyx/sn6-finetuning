{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "params = {\n",
    "    'low_cpu_mem_usage': True,\n",
    "    'trust_remote_code': False,\n",
    "    'torch_dtype': torch.bfloat16,\n",
    "    'use_safetensors': True,\n",
    "    'attn_implementation': \"flash_attention_2\"\n",
    "}\n",
    "\n",
    "\n",
    "def norm_model_weights(model):\n",
    "    last_q = None\n",
    "    lqb = None\n",
    "    lqkm = None\n",
    "    last_v = None\n",
    "    lvb = None\n",
    "    lvom = None\n",
    "    last_up = None\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"q_proj\" in name:\n",
    "            if \"bias\" in name:\n",
    "                lqb = param\n",
    "            else:\n",
    "                last_q = param\n",
    "        if \"k_proj\" in name:\n",
    "            if \"bias\" in name:\n",
    "                param.data = param.data * lqkm\n",
    "            else:\n",
    "                # print(last_q.data.shape, param.data.shape)\n",
    "\n",
    "                # safe but not full solution\n",
    "                # last_q.data = last_q.data.to(torch.float64)\n",
    "                # param.data = param.data.to(torch.float64)\n",
    "                mult = torch.sqrt(torch.mean(torch.abs(last_q.data), dim=0, keepdim=True).transpose(0, 1) / \n",
    "                                torch.mean(torch.abs(param.data), dim=1, keepdim=True))\n",
    "                mult = torch.mean(mult)\n",
    "                last_q.data = last_q.data / mult\n",
    "                lqb.data = lqb.data / mult\n",
    "                param.data = param.data * mult\n",
    "                lqkm = mult\n",
    "                # last_q.data = last_q.data.to(torch.bfloat16)\n",
    "                # param.data = param.data.to(torch.bfloat16)\n",
    "                #\n",
    "\n",
    "                # THIS ONE STAYS\n",
    "                # last_q.data = last_q.data.to(torch.float64)\n",
    "                # param.data = param.data.to(torch.float64)\n",
    "                # mult = torch.sqrt(torch.mean(torch.abs(last_q.data), dim=1, keepdim=True) / \n",
    "                #                   torch.mean(torch.abs(param.data.repeat(8, 1)), dim=1, keepdim=True))\n",
    "                # # print(mult.shape, mult)\n",
    "                # last_q.data = last_q.data / mult\n",
    "                # param.data = param.data * ((mult[:256] + mult[256:512] + mult[512:768] + mult[768:1024] +\n",
    "                #                            mult[1024:1280] + mult[1280:1536] + mult[1536:1792] + mult[1792:2048]) / 8)\n",
    "                # last_q.data = last_q.data.to(torch.bfloat16)\n",
    "                # param.data = param.data.to(torch.bfloat16)\n",
    "\n",
    "        if \"v_proj\" in name:\n",
    "            if \"bias\" in name:\n",
    "                lvb = param\n",
    "            else:\n",
    "                last_v = param\n",
    "        if \"o_proj\" in name:\n",
    "            if \"bias\" in name:\n",
    "                param.data = param.data * lvom\n",
    "            else:\n",
    "                # print(last_v.data.shape, param.data.shape)\n",
    "\n",
    "                # safe but not full solution\n",
    "                # last_v.data = last_v.data.to(torch.float64)\n",
    "                # param.data = param.data.to(torch.float64)\n",
    "                mult = torch.sqrt(torch.mean(torch.abs(last_v.data), dim=0, keepdim=True) / \n",
    "                                torch.mean(torch.abs(param.data), dim=0, keepdim=True))\n",
    "                mult = torch.mean(mult) * -1\n",
    "                last_v.data = last_v.data / mult\n",
    "                lvb.data = lvb.data / mult\n",
    "                param.data = param.data * mult\n",
    "                lvom = mult\n",
    "                # last_v.data = last_v.data.to(torch.bfloat16)\n",
    "                # param.data = param.data.to(torch.bfloat16)\n",
    "                # \n",
    "\n",
    "                # mult = torch.sqrt(torch.mean(torch.abs(last_v.data.repeat(8, 1)), dim=0, keepdim=True) / \n",
    "                #                   torch.mean(torch.abs(param.data), dim=0, keepdim=True)) / 2\n",
    "                # print(mult.shape, mult)\n",
    "                # last_v.data = last_v.data / mult # ((mult[:256] + mult[256:512] + mult[512:768] + mult[768:1024] +\n",
    "                #                            #mult[1024:1280] + mult[1280:1536] + mult[1536:1792] + mult[1792:2048]) / 8)\n",
    "                # param.data = param.data * mult\n",
    "\n",
    "        if \"up_proj\" in name:\n",
    "            last_up = param\n",
    "        if \"down_proj\" in name:\n",
    "            # print(last_up.data.shape, param.data.shape)\n",
    "            # last_up.data = last_up.data.to(torch.float64)\n",
    "            # param.data = param.data.to(torch.float64)\n",
    "            mult = torch.sqrt(torch.mean(torch.abs(last_up.data), dim=1, keepdim=True).transpose(0, 1) / \n",
    "                            torch.mean(torch.abs(param.data), dim=0, keepdim=True))\n",
    "            # print(mult, mult.shape)\n",
    "            last_up.data = last_up.data / mult.transpose(0, 1)\n",
    "            param.data = param.data * mult\n",
    "            # last_up.data = last_up.data.to(torch.bfloat16)\n",
    "            # param.data = param.data.to(torch.bfloat16)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def merge(model_name0, model_name1, merge_name=\"merged\", ratio=0.5, \n",
    "          embed_ratio=None, norm_ratio=None, fc_ratio=None, norm0=False, norm1=False, return_model=False): # higher ratio means more of model0\n",
    "    \n",
    "    if embed_ratio is None:\n",
    "        embed_ratio = ratio\n",
    "    if norm_ratio is None:\n",
    "        norm_ratio = ratio\n",
    "    if fc_ratio is None:\n",
    "        fc_ratio = ratio\n",
    "\n",
    "    tokenizer = None\n",
    "    if type(model_name0) is str:\n",
    "        model0 = AutoModelForCausalLM.from_pretrained(model_name0, **params, cache_dir=\"Models\")\n",
    "        # tokenizer = AutoTokenizer.from_pretrained(model_name0, cache_dir=\"Models\")\n",
    "    else:\n",
    "        model0 = model_name0\n",
    "    if type(model_name1) is str:\n",
    "        model1 = AutoModelForCausalLM.from_pretrained(model_name1, **params, cache_dir=\"Models\")\n",
    "        # if tokenizer is None:\n",
    "        #     tokenizer = AutoTokenizer.from_pretrained(model_name1, cache_dir=\"Models\")\n",
    "    else:\n",
    "        model1 = model_name1\n",
    "\n",
    "    if norm0:\n",
    "        model0 = norm_model_weights(model0)\n",
    "    if norm1:\n",
    "        model1 = norm_model_weights(model1)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-2-zephyr-1_6b\", trust_remote_code=False, use_fast=True, cache_dir=\"Models\")\n",
    "\n",
    "    params0 = {}\n",
    "    for name, param in model0.named_parameters():\n",
    "        params0[name] = param\n",
    "\n",
    "    for name, param in model1.named_parameters():\n",
    "        if \"embed\" in name:\n",
    "            param.data = ((params0[name].data * embed_ratio) + (param.data * (1 - embed_ratio)))\n",
    "        elif (\"up_proj\" not in name \n",
    "            and \"down_proj\" not in name \n",
    "            and \"gate_proj\" not in name \n",
    "            and \"o_proj\" not in name \n",
    "            and \"k_proj\" not in name \n",
    "            and \"v_proj\" not in name \n",
    "            and \"q_proj\" not in name\n",
    "            and \"embed\" not in name\n",
    "            ):\n",
    "            param.data = ((params0[name].data * norm_ratio) + (param.data * (1 - norm_ratio)))\n",
    "        elif \"up_proj\" in name or \"down_proj\" in name:\n",
    "            param.data = ((params0[name].data * fc_ratio) + (param.data * (1 - fc_ratio)))\n",
    "        else:\n",
    "            param.data = ((params0[name].data * ratio) + (param.data * (1 - ratio)))\n",
    "\n",
    "    model1.config.bos_token_id = 2\n",
    "    model1.config.eos_token_id = 1\n",
    "    model1.generation_config.bos_token_id = 2\n",
    "    model1.generation_config.eos_token_id = 1\n",
    "\n",
    "    if return_model:\n",
    "        del model0; gc.collect()\n",
    "        return model1, tokenizer\n",
    "    else:\n",
    "        model1.save_pretrained(\"Models/\"+merge_name)\n",
    "        tokenizer.save_pretrained(\"Models/\"+merge_name)\n",
    "        del model0, model1, tokenizer; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge(\"MesozoicMetallurgist/zeta-Anisian\", \"0x0dad0/beta_s03\", \n",
    "      merge_name=\"merged20\", norm0=True, norm1=True, ratio=0.5, embed_ratio=0.5, norm_ratio=0.5, fc_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge(\"MesozoicMetallurgist/nous-Burdigalian\", \"MesozoicMetallurgist/nous-Langhian\", \n",
    "      merge_name=\"merged17\", norm0=True, norm1=True, ratio=0.5, embed_ratio=0.5, norm_ratio=0.5, fc_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
