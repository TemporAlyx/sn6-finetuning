{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel,\n",
    "    LoHaConfig,\n",
    ")\n",
    "\n",
    "from cortexsubsetloader import CortexSubsetLoader, tokenize\n",
    "from pytorch_optimizer import Ranger21, SAM, DAdaptAdam, SophiaH, ScalableShampoo #, Lamb, DAdaptLion, LOMO\n",
    "from pytorch_optimizer.optimizer.sam import WSAM\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.37.2\n",
    "# !pip install -U transformers\n",
    "# !pip install -U peft\n",
    "# !pip show torch transformers peft flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(train, eval):\n",
    "    evalf = []\n",
    "    for f in eval:\n",
    "        if f not in train:\n",
    "            evalf.append(f)\n",
    "    if len(evalf) < len(eval): print(f\"Removed {len(eval)-len(evalf)} duplicates from eval\")\n",
    "    else: print(\"No duplicates found in eval\")\n",
    "    return evalf\n",
    "\n",
    "\n",
    "def data_collator(features):\n",
    "    batches = []\n",
    "    for feature in features:\n",
    "        inputs, prompt_len = feature\n",
    "        data = [inputs]\n",
    "        b_labels = inputs.clone()\n",
    "        b_labels[:, :prompt_len] = -100\n",
    "        labels = [b_labels]\n",
    "            \n",
    "        batch = {}\n",
    "        batch['input_ids'] = torch.concat(data)\n",
    "        batch['labels'] = torch.concat(labels)\n",
    "        batches.append(batch)\n",
    "    return batches\n",
    "\n",
    "\n",
    "def get_data(train_name, eval_name, tokenizer, train_subset=None, eval_subset=None, shuffle=True, extend_train_length=0):\n",
    "    if type(train_name) == str:\n",
    "        with open(train_name, \"r\") as f:\n",
    "            train_data = np.array(json.load(f))\n",
    "    else: # list of str\n",
    "        train_data = []\n",
    "        for name in train_name:\n",
    "            with open(name, \"r\") as f:\n",
    "                train_data = train_data + json.load(f)\n",
    "        train_data = np.array(train_data)\n",
    "    with open(eval_name, \"r\") as f:\n",
    "        eval_data = np.array(json.load(f))\n",
    "\n",
    "    if train_subset is not None:\n",
    "        train_data = train_data[train_subset]\n",
    "    if eval_subset is not None:\n",
    "        eval_data = eval_data[-eval_subset:]\n",
    "    \n",
    "    eval_data = filter_data(train_data, eval_data)\n",
    "\n",
    "    if shuffle:\n",
    "        p = np.random.permutation(len(train_data))\n",
    "        train_data = train_data[p]\n",
    "\n",
    "    train_data = tokenize(tokenizer, train_data, 2048+extend_train_length)\n",
    "    eval_data = tokenize(tokenizer, eval_data, 2048)\n",
    "\n",
    "    train_data = data_collator(train_data)\n",
    "    eval_data = data_collator(eval_data)\n",
    "\n",
    "    return train_data, eval_data\n",
    "\n",
    "\n",
    "def merge(model0, model1, ratio=0.5, embed_ratio=None, norm_ratio=None, fc_ratio=None): # higher ratio means more of model0\n",
    "    if embed_ratio is None:\n",
    "        embed_ratio = ratio\n",
    "    if norm_ratio is None:\n",
    "        norm_ratio = ratio\n",
    "    if fc_ratio is None:\n",
    "        fc_ratio = ratio\n",
    "\n",
    "    params0 = {}\n",
    "    for name, param in model0.named_parameters():\n",
    "        params0[name] = param\n",
    "\n",
    "    for name, param in model1.named_parameters():\n",
    "        if \"embed\" in name:\n",
    "            param.data = ((params0[name].data * embed_ratio) + (param.data * (1 - embed_ratio)))\n",
    "        elif (\"up_proj\" not in name \n",
    "            and \"down_proj\" not in name \n",
    "            and \"gate_proj\" not in name \n",
    "            and \"o_proj\" not in name \n",
    "            and \"k_proj\" not in name \n",
    "            and \"v_proj\" not in name \n",
    "            and \"q_proj\" not in name\n",
    "            and \"embed\" not in name\n",
    "            ):\n",
    "            param.data = ((params0[name].data * norm_ratio) + (param.data * (1 - norm_ratio)))\n",
    "        elif \"up_proj\" in name or \"down_proj\" in name:\n",
    "            param.data = ((params0[name].data * fc_ratio) + (param.data * (1 - fc_ratio)))\n",
    "        else:\n",
    "            param.data = ((params0[name].data * ratio) + (param.data * (1 - ratio)))\n",
    "\n",
    "    return model1\n",
    "\n",
    "def copy_weights_over(model0, model1):\n",
    "    params0 = {}\n",
    "    for name, param in model0.named_parameters():\n",
    "        params0[name] = param\n",
    "\n",
    "    for name, param in model1.named_parameters():\n",
    "        if name in params0:\n",
    "            param.data = params0[name].data\n",
    "    return model1\n",
    "\n",
    "\n",
    "# initial_loss_eps = 0.0001\n",
    "intermed_check_step_split = 8\n",
    "\n",
    "def simple_eval(model, eval_d):\n",
    "    print(\"Evaluating\", end=\" \")\n",
    "    model = model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    steps_so_far = 1\n",
    "    for batch in eval_d:\n",
    "        inputs = batch['input_ids'].to(\"cuda\")\n",
    "        labels = batch['labels'].to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            eval_loss += outputs.loss.item() / len(eval_d)\n",
    "        if steps_so_far % (len(eval_d) // intermed_check_step_split) == 0:\n",
    "            print(\".\", end=\"\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "        steps_so_far += 1\n",
    "    model = model.to(\"cpu\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    print(f\" Loss: {eval_loss:.8f}\")\n",
    "\n",
    "def evaluate(model, eval_d, return_to_cpu=False, return_stats=False, print_stats=True, cached_base_loss=None,\n",
    "             base_model=None, precompute_base_loss=True, device=\"cuda\", instruction_finetuning=True, true_eps=0.01):\n",
    "    print(\"Evaluating\", end=\" \")\n",
    "    model = model.to(\"cuda\")\n",
    "    eval_base_loss = 0\n",
    "    lora_diff = 0\n",
    "    eval_loss = 0\n",
    "    head_to_head = 0\n",
    "    eps0_head_to_head = 0\n",
    "    overshoot = 0\n",
    "    model.eval()\n",
    "    steps_so_far = 1\n",
    "\n",
    "    LORA = True\n",
    "    if base_model is not None:\n",
    "        LORA = False\n",
    "\n",
    "    precomputed_base_losses = []\n",
    "    if cached_base_loss is not None:\n",
    "        for x in cached_base_loss:\n",
    "            precomputed_base_losses.append(x)\n",
    "        precompute_base_loss = True\n",
    "    elif precompute_base_loss and cached_base_loss is None:\n",
    "        if LORA:\n",
    "            model.disable_adapter_layers()\n",
    "        else:\n",
    "            model = model.to(\"cpu\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "            base_model = base_model.to(device)\n",
    "\n",
    "        for batch in eval_d:\n",
    "            if instruction_finetuning:\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "            else:\n",
    "                inputs = batch.to(device)\n",
    "                labels = inputs.clone()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if LORA:\n",
    "                    base_outputs_loss = model(inputs, labels=labels).loss\n",
    "                else:\n",
    "                    base_outputs_loss = base_model(inputs, labels=labels).loss\n",
    "                precomputed_base_losses.append(base_outputs_loss)\n",
    "        \n",
    "        if LORA:\n",
    "            model.enable_adapter_layers()\n",
    "        else:\n",
    "            base_model = base_model.to(\"cpu\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "            model = model.to(device)\n",
    "    else:\n",
    "        print(\"WARN: not precomputing base loss will put both models on the same device\")\n",
    "        base_model = base_model.to(device)\n",
    "\n",
    "\n",
    "    for batch in eval_d:\n",
    "        if instruction_finetuning:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "        else:\n",
    "            inputs = batch.to(device)\n",
    "            labels = inputs.clone()\n",
    "        with torch.no_grad():\n",
    "            if precompute_base_loss:\n",
    "                base_outputs_loss = precomputed_base_losses.pop(0)\n",
    "            else:\n",
    "                if LORA:\n",
    "                    model.disable_adapter_layers()\n",
    "                    base_outputs_loss = model(inputs, labels=labels).loss\n",
    "                    model.enable_adapter_layers()\n",
    "                else:\n",
    "                    base_outputs_loss = base_model(inputs, labels=labels).loss\n",
    "            outputs_loss = model(inputs, labels=labels).loss\n",
    "\n",
    "            base_loss = base_outputs_loss\n",
    "            partial_loss = torch.nn.functional.relu(outputs_loss - (base_loss * (1.0 - true_eps)))\n",
    "            overshoot_penalty = torch.nn.functional.relu(-(outputs_loss - (base_loss * (1.0 - true_eps))))\n",
    "            loss = partial_loss / base_loss\n",
    "\n",
    "            if cached_base_loss is None:\n",
    "                base_outputs_loss_item = base_outputs_loss.item()\n",
    "            else:\n",
    "                base_outputs_loss_item = base_outputs_loss\n",
    "            eval_loss += loss.item() / len(eval_d)\n",
    "            eval_base_loss += base_outputs_loss_item / len(eval_d)\n",
    "            lora_diff += (outputs_loss.item() - base_outputs_loss_item) / len(eval_d)\n",
    "            head_to_head += 100.0 / len(eval_d) if outputs_loss < (base_outputs_loss * (1.0 - true_eps)) else 0.0\n",
    "            head_to_head += 50.0 / len(eval_d) if outputs_loss == (base_outputs_loss * (1.0 - true_eps)) else 0.0\n",
    "            eps0_head_to_head += 100.0 / len(eval_d) if outputs_loss < base_outputs_loss else 0.0\n",
    "            eps0_head_to_head += 50.0 / len(eval_d) if outputs_loss == base_outputs_loss else 0.0\n",
    "            overshoot += overshoot_penalty.item() / len(eval_d)\n",
    "\n",
    "        if steps_so_far % (len(eval_d) // intermed_check_step_split) == 0:\n",
    "            print(\".\", end=\"\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "        steps_so_far += 1\n",
    "\n",
    "    if return_to_cpu:\n",
    "        model = model.to(\"cpu\")\n",
    "\n",
    "    if not LORA:\n",
    "        base_model = base_model.to(\"cpu\")\n",
    "\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    data = {\n",
    "        \"loss\": eval_loss,\n",
    "        \"base_loss\": eval_base_loss,\n",
    "        \"lora_diff\": lora_diff,\n",
    "        \"head_to_head\": head_to_head,\n",
    "        \"eps0_head_to_head\": eps0_head_to_head,\n",
    "        \"overshoot\": overshoot\n",
    "    }\n",
    "\n",
    "    if print_stats:\n",
    "        print(f\" Loss: {eval_loss:.8f}, Base Loss: {eval_base_loss:.6f}, Lora Diff: {lora_diff:.8f},\",\n",
    "            f\"WR: {head_to_head:.2f}%, 0epsWR: {eps0_head_to_head:.2f}%, OShL: {overshoot:.8f}\")\n",
    "    if return_stats:\n",
    "        return data\n",
    "\n",
    "old_train_data = []\n",
    "\n",
    "def train(model, tokenizer, train_d=[], eval_d=[], base_model=None, inf_training=False, inf_data_steps=5, inf_data_decay=1, training_device=\"cuda\",\n",
    "            acc_batch_size=512, instruction_finetuing=True, precalculate_batch_mult=2.25, precalc_eval_base=True,\n",
    "            lr=1e-5, weight_decay=0.001, lr_scheduler=\"constant\", warmup_steps=4, warmup_end_offset=0, betas=(0.9, 0.99), \n",
    "            use_sam=False, sam_rho=0.05, opt=\"adamw\", adaptive_sam=True, wsam_variant=False, ignore_below=0.0, ignore_auto_percent=2,\n",
    "            manual_grad_clip_norm=1.0, sam_grad_clip_norm=None,\n",
    "            add_overshoot_penalty=False, ignore_overshot_samples=True, bad_sample_mult=1.0, remerging=False, remerge_eval=True, remerge_ratio=0.5,\n",
    "            loss_eps = 0.02, overshoot_buffer = -0.01, true_eps=0.01, simple_loss=False, process_base_loss=True, relative_loss=False,\n",
    "            eval_steps=2048, save_name=\"lora\", do_save=True,\n",
    "            average_stats=False, save_n_start=0,\n",
    "            gradient_checkpointing=False, excessive_cache_clearing=False, base_model_switching=True):\n",
    "    global old_train_data\n",
    "    if warmup_steps is None:\n",
    "        warmup_steps = (eval_steps // acc_batch_size) // 2\n",
    "    LORA = True\n",
    "    if base_model is not None:\n",
    "        LORA = False\n",
    "        # base_model = base_model.to(\"cuda\")\n",
    "\n",
    "    if len(train_d) == 0:\n",
    "        print(\"WARN: no training data provided, enabling infinite training\")\n",
    "        inf_training = True\n",
    "\n",
    "    if remerging:\n",
    "        params = {\n",
    "            'low_cpu_mem_usage': True,\n",
    "            'trust_remote_code': False,\n",
    "            'torch_dtype': torch.bfloat16,\n",
    "            'use_safetensors': True,\n",
    "            'attn_implementation': \"flash_attention_2\"\n",
    "        }\n",
    "        # create a copy of the model to merge with\n",
    "        model.save_pretrained(\"model_prev\")\n",
    "        model_prev = AutoModelForCausalLM.from_pretrained(\"model_prev\", **params)\n",
    "        print(\"WARN: remerging is enabled, model_prev created and may take up extra memory\")\n",
    "\n",
    "    \n",
    "    def get_new_data(n_samples=2560, dd_eval=True, dd_train=True, steps=1, old_data=old_train_data):\n",
    "        cortex_subset_loader = CortexSubsetLoader(latest=True, random_seed = None, max_samples=n_samples, progress=False, \n",
    "                                        running=True, retry_limit=5, page_size=400, retry_delay=5, silent=True, steps=steps,\n",
    "                                        ignore_list=old_data)\n",
    "        batches = data_collator(cortex_subset_loader.tokenize(tokenizer))\n",
    "        dedup_batches = batches\n",
    "        p = np.random.permutation(len(dedup_batches))# [:min(n_samples, len(dedup_batches))]\n",
    "        return [dedup_batches[i] for i in p]\n",
    "\n",
    "    if len(eval_d) == 0:\n",
    "        print(\"WARN: no evaluation data provided, acquiring new data\")\n",
    "        eval_d = get_new_data(512)\n",
    "        eval_d = eval_d[:512]\n",
    "\n",
    "    add_inf_steps = 0\n",
    "    if inf_training:\n",
    "        if len(train_d) == 0:\n",
    "            print(\"WARN: no training data provided, acquiring new data\")\n",
    "            while len(train_d) < (acc_batch_size * precalculate_batch_mult):\n",
    "                new_data = get_new_data(int(acc_batch_size * precalculate_batch_mult), steps=inf_data_steps+add_inf_steps)\n",
    "                if len(new_data) == 0:\n",
    "                    add_inf_steps += inf_data_steps\n",
    "                else:\n",
    "                    add_inf_steps = add_inf_steps - inf_data_decay\n",
    "                    train_d = train_d + new_data\n",
    "    \n",
    "    if simple_loss:\n",
    "        if process_base_loss:\n",
    "            print(\"WARN: simple loss is enabled, this will disable base model processing\")\n",
    "            process_base_loss = False\n",
    "        \n",
    "    if not process_base_loss:\n",
    "        if ignore_overshot_samples:\n",
    "            print(\"Base loss processing is disabled, disabling ignore overshot samples\")\n",
    "            ignore_overshot_samples = False\n",
    "\n",
    "\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    model = model.to(\"cuda\")\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "    if gradient_checkpointing:\n",
    "        model.config.use_cache = False\n",
    "        grad_check_kwargs = {\"use_reentrant\": False}\n",
    "        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=grad_check_kwargs)\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    if not use_sam:\n",
    "        if opt == \"dadapt_adam\":\n",
    "            optimizer = DAdaptAdam(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay, fixed_decay=True)\n",
    "        elif opt == \"shampoo\":\n",
    "            optimizer = ScalableShampoo(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay, \n",
    "                                        start_preconditioning_step=warmup_steps+1, preconditioning_compute_steps=1)\n",
    "        elif opt == \"sophia\":\n",
    "            optimizer = SophiaH(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        elif opt == \"ranger\":\n",
    "            optimizer = Ranger21(model.parameters(), num_iterations=1, lr=lr, betas=betas, weight_decay=weight_decay,\n",
    "                                 num_warm_up_iterations=0, num_warm_down_iterations=0)\n",
    "        elif opt == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        elif opt == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=betas[0], weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer {opt}\")\n",
    "    else:\n",
    "        base_optimizer_args = {\"lr\": lr, \"weight_decay\": weight_decay, \"betas\": betas, \"eps\": 1e-8}\n",
    "\n",
    "        if opt == \"dadapt_adam\":\n",
    "            base_optimizer = DAdaptAdam\n",
    "        elif opt == \"sophia\":\n",
    "            base_optimizer = SophiaH\n",
    "        elif opt == \"shampoo\":\n",
    "            base_optimizer = ScalableShampoo\n",
    "            base_optimizer_args[\"start_preconditioning_step\"] = warmup_steps+1\n",
    "            base_optimizer_args[\"preconditioning_compute_steps\"] = 1\n",
    "        elif opt == \"ranger\":\n",
    "            base_optimizer = Ranger21\n",
    "            base_optimizer_args[\"num_iterations\"] = 1\n",
    "            base_optimizer_args[\"num_warm_up_iterations\"] = 0\n",
    "            base_optimizer_args[\"num_warm_down_iterations\"] = 0\n",
    "        elif opt == \"adamw\":\n",
    "            base_optimizer = torch.optim.AdamW\n",
    "        elif opt == \"sgd\":\n",
    "            base_optimizer = torch.optim.SGD\n",
    "            del base_optimizer_args[\"betas\"], base_optimizer_args[\"eps\"]\n",
    "            base_optimizer_args[\"momentum\"] = betas[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer {opt}\")\n",
    "\n",
    "        if not wsam_variant:\n",
    "            optimizer = SAM(model.parameters(), base_optimizer=base_optimizer, rho=sam_rho, adaptive=adaptive_sam, **base_optimizer_args)\n",
    "            sam_optimizer = optimizer\n",
    "        else:\n",
    "            optimizer = WSAM(model, params=model.parameters(), base_optimizer=base_optimizer, rho=sam_rho, adaptive=adaptive_sam, \n",
    "                             **base_optimizer_args, max_norm=sam_grad_clip_norm)\n",
    "            sam_optimizer = optimizer\n",
    "\n",
    "\n",
    "\n",
    "    if lr_scheduler == \"cosine\":\n",
    "        lr_scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, warmup_steps, (len(train_d)//acc_batch_size)+warmup_end_offset)\n",
    "    elif lr_scheduler == \"polynomial\":\n",
    "        lr_scheduler = transformers.get_polynomial_decay_schedule_with_warmup(optimizer, warmup_steps, \n",
    "                                                                              (len(train_d)//acc_batch_size)+warmup_end_offset)\n",
    "    elif lr_scheduler == \"constant\":\n",
    "        lr_scheduler = transformers.get_constant_schedule_with_warmup(optimizer, warmup_steps)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown lr_scheduler {lr_scheduler}\")\n",
    "    lr_scheduler.step() # don't want to start at 0\n",
    "    \n",
    "    @torch.jit.script\n",
    "    def combined_loss_os(outputs_loss, base_loss_in, loss_eps:float=loss_eps, overshoot_buffer:float=overshoot_buffer):\n",
    "        base_loss = base_loss_in.item()\n",
    "        partial_loss = outputs_loss - (base_loss * (1.0 - loss_eps))\n",
    "        loss = partial_loss / base_loss\n",
    "        overshoot_penalty = torch.nn.functional.relu(-(loss + overshoot_buffer))\n",
    "        return torch.nn.functional.relu(loss) + overshoot_penalty, overshoot_penalty.item()\n",
    "    \n",
    "    @torch.jit.script\n",
    "    def combined_loss_os_noshot(outputs_loss, base_loss_in, loss_eps:float=loss_eps, overshoot_buffer:float=overshoot_buffer):\n",
    "        base_loss = base_loss_in.item()\n",
    "        partial_loss = outputs_loss - (base_loss * (1.0 - loss_eps))\n",
    "        loss = partial_loss / base_loss\n",
    "        return torch.nn.functional.relu(loss), torch.nn.functional.relu(-(loss + overshoot_buffer)).item()\n",
    "    \n",
    "    @torch.jit.script\n",
    "    def simple_loss_func(outputs_loss, base_loss_in, loss_eps:float=loss_eps, overshoot_buffer:float=overshoot_buffer):\n",
    "        return outputs_loss, 0.0\n",
    "    \n",
    "    @torch.jit.script\n",
    "    def relative_loss_func(outputs_loss, base_loss_in, loss_eps:float=loss_eps, overshoot_buffer:float=overshoot_buffer):\n",
    "        base_loss = base_loss_in.item()\n",
    "        relative_loss = outputs_loss / (base_loss + loss_eps)\n",
    "        osh = torch.nn.functional.relu(-((relative_loss - 1.0) + overshoot_buffer)).item()\n",
    "        loss = outputs_loss * (4.2 * torch.square(torch.sin((torch.clamp(relative_loss, min=0.813, max=1.187) - 0.065) * 4.2)))\n",
    "        return loss, osh\n",
    "\n",
    "    if add_overshoot_penalty:\n",
    "        combined_loss = combined_loss_os\n",
    "    else:\n",
    "        combined_loss = combined_loss_os_noshot\n",
    "\n",
    "    if simple_loss:\n",
    "        combined_loss = simple_loss_func\n",
    "        if relative_loss:\n",
    "            print(\"WARN: simple loss is enabled, disabling relative loss\")\n",
    "            relative_loss = False\n",
    "\n",
    "    if relative_loss:\n",
    "        combined_loss = relative_loss_func\n",
    "\n",
    "\n",
    "    if precalc_eval_base:\n",
    "        print(\"Note: precalced eval base loss does not account for pretrained fine-tuning\")\n",
    "        eval_base_loss = []\n",
    "        steps_so_far = 1\n",
    "        if not LORA:\n",
    "            model = model.to(\"cpu\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "            base_model = base_model.to(training_device)\n",
    "        for batch in eval_d:\n",
    "            if instruction_finetuing:\n",
    "                inputs = batch['input_ids'].to(training_device)\n",
    "                labels = batch['labels'].to(training_device)\n",
    "            else:\n",
    "                inputs = batch.to(training_device)\n",
    "                labels = inputs.clone()\n",
    "            with torch.no_grad():\n",
    "                if LORA:\n",
    "                    base_outputs_loss = model(inputs, labels=labels).loss\n",
    "                else:\n",
    "                    base_outputs_loss = base_model(inputs, labels=labels).loss\n",
    "                eval_base_loss.append(base_outputs_loss.item())\n",
    "            if steps_so_far % (len(eval_d) // intermed_check_step_split) == 0:\n",
    "                print(\".\", end=\"\")\n",
    "                gc.collect(); torch.cuda.empty_cache()\n",
    "            steps_so_far += 1\n",
    "        if not LORA:\n",
    "            base_model = base_model.to(\"cpu\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "            model = model.to(training_device)\n",
    "        print(f\"Eval Base Loss: {sum(eval_base_loss)/len(eval_d):.6f}\")\n",
    "        if ignore_below == \"auto\":\n",
    "            ignore_below = np.percentile(eval_base_loss, ignore_auto_percent)\n",
    "    else:\n",
    "        eval_base_loss = None\n",
    "\n",
    "    if ignore_below == \"auto\":\n",
    "        print(\"eval loss not precalced, ignore_below set to 0.0\")\n",
    "        ignore_below = 0.0\n",
    "\n",
    "    steps_so_far = 1 # start at one to avoid all the modulo checks\n",
    "    epoch_loss = 0; epoch_overshoot = 0; epoch_base_loss = 0; lora_diff = 0\n",
    "    epoch_wr = 0; epoch_0eps_wr = 0\n",
    "    fit_samples = 0; unfit_samples = 0\n",
    "    sam_batch = []\n",
    "    # lomo_batch_loss = []\n",
    "    accum_steps = 0.0\n",
    "    true_steps_taken = 0; prev_dot_step = -1\n",
    "    last_tst = true_steps_taken\n",
    "    add_inf_steps = 0\n",
    "    sam_saved_base_outputs = []\n",
    "    precalculated_base_outputs = []\n",
    "    while len(train_d) > 0:\n",
    "\n",
    "        if (true_steps_taken % (acc_batch_size // intermed_check_step_split) == 0) and accum_steps != prev_dot_step:\n",
    "            prev_dot_step = accum_steps\n",
    "            print(\".\", end=\"\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "        while len(train_d) < (acc_batch_size * precalculate_batch_mult):\n",
    "            new_data = get_new_data(int(acc_batch_size * precalculate_batch_mult), steps=inf_data_steps+add_inf_steps)\n",
    "            if len(new_data) == 0:\n",
    "                add_inf_steps += inf_data_steps\n",
    "            else:\n",
    "                add_inf_steps = add_inf_steps - inf_data_decay\n",
    "            # old_train_data = old_train_data + new_data\n",
    "            train_d = train_d + new_data\n",
    "\n",
    "        bstep = 0\n",
    "        if len(precalculated_base_outputs) == 0 and process_base_loss:\n",
    "            batches = train_d[:int(acc_batch_size * precalculate_batch_mult)]\n",
    "\n",
    "            if LORA:\n",
    "                model.disable_adapter_layers()\n",
    "            else:\n",
    "                if base_model_switching:\n",
    "                    model = model.to(\"cpu\")\n",
    "                    gc.collect(); torch.cuda.empty_cache()\n",
    "                base_model = base_model.to(training_device)\n",
    "            \n",
    "            for batch in batches:\n",
    "                if instruction_finetuing:\n",
    "                    inputs = batch['input_ids'].to(training_device)\n",
    "                    labels = batch['labels'].to(training_device)\n",
    "                else:\n",
    "                    inputs = batch.to(training_device)\n",
    "                    labels = inputs.clone()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if LORA:\n",
    "                        base_outputs = model(inputs, labels=labels)\n",
    "                    else:\n",
    "                        base_outputs = base_model(inputs, labels=labels)\n",
    "                    precalculated_base_outputs.append(torch.tensor(base_outputs.loss.item()))\n",
    "\n",
    "                bstep+=1\n",
    "                if bstep % (intermed_check_step_split) == 0:\n",
    "                    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "            if LORA:\n",
    "                model.enable_adapter_layers()\n",
    "            else:\n",
    "                base_model = base_model.to(\"cpu\")\n",
    "                gc.collect(); torch.cuda.empty_cache()\n",
    "                if base_model_switching:\n",
    "                    model = model.to(training_device)\n",
    "\n",
    "\n",
    "        batch = train_d.pop(0)\n",
    "        if instruction_finetuing:\n",
    "            inputs = batch['input_ids'].to(training_device)\n",
    "            labels = batch['labels'].to(training_device)\n",
    "        else:\n",
    "            inputs = batch.to(training_device)\n",
    "            labels = inputs.clone()\n",
    "\n",
    "        \n",
    "        if process_base_loss:\n",
    "            base_outputs_loss = precalculated_base_outputs.pop(0)\n",
    "        else:\n",
    "            base_outputs_loss = torch.zeros(size=(1,))\n",
    "        outputs_loss = model(inputs, labels=labels).loss\n",
    "\n",
    "        loss, overshoot_penalty = combined_loss(outputs_loss, base_outputs_loss)\n",
    "        loss = loss / acc_batch_size\n",
    "\n",
    "        if (not ignore_overshot_samples or overshoot_penalty <= 0.0) and outputs_loss.item() >= ignore_below:\n",
    "            if not simple_loss and loss.item() > ((loss_eps / acc_batch_size)+1e-8):\n",
    "                unfit_samples += -1\n",
    "                if bad_sample_mult != 1.0:\n",
    "                    loss = loss * bad_sample_mult\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            accum_steps += 1\n",
    "            true_steps_taken += 1\n",
    "            if use_sam:\n",
    "                sam_batch.append((inputs, labels))\n",
    "                sam_saved_base_outputs.append(base_outputs_loss)\n",
    "        else:\n",
    "            fit_samples += 1\n",
    "\n",
    "        outputs_loss_item = outputs_loss.detach().item()\n",
    "        if process_base_loss:\n",
    "            base_loss_item = base_outputs_loss.item()\n",
    "        else:\n",
    "            if eval_base_loss is not None:\n",
    "                base_loss_item = sum(eval_base_loss) / len(eval_d)\n",
    "            else:\n",
    "                base_loss_item = 0.0\n",
    "\n",
    "        epoch_base_loss += base_loss_item\n",
    "        lora_diff += (outputs_loss_item - base_loss_item)\n",
    "        epoch_loss += max(loss.detach().item() * acc_batch_size, 0.0)\n",
    "        epoch_wr += 100.0 if outputs_loss_item < (base_loss_item * (1.0 - true_eps)) else 0.0\n",
    "        epoch_wr += 50.0 if outputs_loss_item == (base_loss_item * (1.0 - true_eps)) else 0.0\n",
    "        epoch_0eps_wr += 100.0 if outputs_loss_item < base_loss_item else 0.0\n",
    "        epoch_0eps_wr += 50.0 if outputs_loss_item == base_loss_item else 0.0\n",
    "        epoch_overshoot += overshoot_penalty\n",
    "\n",
    "        if accum_steps == acc_batch_size:\n",
    "            if not use_sam:\n",
    "                if manual_grad_clip_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), manual_grad_clip_norm)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                sub_steps = 1\n",
    "                sam_optimizer.first_step(zero_grad=True)\n",
    "                \n",
    "                for inputs, labels in sam_batch:\n",
    "                    base_outputs_loss = sam_saved_base_outputs.pop(0)\n",
    "                    outputs = model(inputs, labels=labels)\n",
    "\n",
    "                    loss, overshoot_penalty = combined_loss(outputs.loss, base_outputs_loss)\n",
    "                    if loss.item() > ((loss_eps / acc_batch_size)+1e-8):\n",
    "                        if bad_sample_mult is not None and bad_sample_mult != 1.0:\n",
    "                            loss = loss * bad_sample_mult\n",
    "                    loss = loss / accum_steps\n",
    "                    \n",
    "                    loss.backward()\n",
    "\n",
    "                    if sub_steps % (acc_batch_size // intermed_check_step_split) == 0:\n",
    "                        print(\".\", end=\"\")\n",
    "                    sub_steps += 1\n",
    "                    \n",
    "                    if excessive_cache_clearing:\n",
    "                        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "                sam_optimizer.second_step(zero_grad=True)\n",
    "                sam_batch = []\n",
    "\n",
    "            if average_stats:\n",
    "                stat_steps = steps_so_far\n",
    "            else:\n",
    "                stat_steps = accum_steps\n",
    "                stat_steps += fit_samples\n",
    "            print(f\"Step {steps_so_far}/{len(train_d)}\\tLoss: {epoch_loss/stat_steps:.6f}\",\n",
    "                                                    f\"OShL: {epoch_overshoot/stat_steps:.3e}\"\n",
    "                                                    f\"\\tBase: {epoch_base_loss/stat_steps:.4f}\",\n",
    "                                                    f\"Diff: {lora_diff/stat_steps:.4e}\",\n",
    "                                                    f\"\\tWR: {epoch_wr/stat_steps:2.2f}%\",\n",
    "                                                    f\"0eps: {epoch_0eps_wr/stat_steps:2.2f}% \",\n",
    "                                                    f\"\\tLR: {lr_scheduler.get_last_lr()[0]:.2e}\",\n",
    "                                                    # f\"eps: {loss_eps:.2e}\",\n",
    "                                                    f\"fit: {fit_samples}/{unfit_samples}\"\n",
    "                                                    )\n",
    "\n",
    "            if not average_stats:\n",
    "                epoch_overshoot = 0\n",
    "                epoch_loss = 0\n",
    "                epoch_base_loss = 0\n",
    "                lora_diff = 0\n",
    "                epoch_wr = 0\n",
    "                epoch_0eps_wr = 0\n",
    "            unfit_samples = 0\n",
    "            fit_samples = 0\n",
    "            accum_steps = 0\n",
    "            \n",
    "            lr_scheduler.step()\n",
    "            if lr_scheduler.get_last_lr()[0] == 0.0:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        if true_steps_taken % eval_steps == 0 and len(train_d) > 0 and true_steps_taken != last_tst:\n",
    "            if remerging:\n",
    "                if remerge_eval:\n",
    "                    evaluate(model, eval_d, base_model=base_model, device=training_device, instruction_finetuning=instruction_finetuing, \n",
    "                            true_eps=true_eps, cached_base_loss=eval_base_loss, precompute_base_loss=True)\n",
    "                model = model.to(\"cpu\")\n",
    "                model = merge(model_prev, model, ratio=remerge_ratio)\n",
    "                model.save_pretrained(\"model_prev\")\n",
    "                model_prev = AutoModelForCausalLM.from_pretrained(\"model_prev\", **params)\n",
    "                model = model.to(training_device)\n",
    "            evaluate(model, eval_d, base_model=base_model, device=training_device, instruction_finetuning=instruction_finetuing, \n",
    "                    true_eps=true_eps, cached_base_loss=eval_base_loss, precompute_base_loss=True)\n",
    "            if do_save:\n",
    "                model.save_pretrained(save_name + '_' + str((true_steps_taken // eval_steps) + save_n_start).format(\"02d\"))\n",
    "            model.train()\n",
    "        \n",
    "        steps_so_far += 1\n",
    "        last_tst = true_steps_taken\n",
    "\n",
    "        if excessive_cache_clearing:\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "            \n",
    "        if time.time() - notebook_start_time > 5.75*60*60:\n",
    "            break\n",
    "\n",
    "    if do_save:\n",
    "        if save_n_start > 0:\n",
    "            model.save_pretrained(save_name+\"_X\"+str(save_n_start))\n",
    "        else:\n",
    "            model.save_pretrained(save_name)\n",
    "\n",
    "    model.eval()\n",
    "    final_eval_stats = evaluate(model, eval_d, return_stats=True, base_model=base_model, device=training_device, \n",
    "                                instruction_finetuning=instruction_finetuing, true_eps=true_eps,\n",
    "                     cached_base_loss=eval_base_loss, precompute_base_loss=True)\n",
    "\n",
    "    model = model.to(\"cpu\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    return final_eval_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "# lora_name \"Carboniferous\"\n",
    "lora_name = \"Burdigalian\"\n",
    "model_name = \"MesozoicMetallurgist/nous-Aquitanian\" #\n",
    "# model_name = \"fulltune\"\n",
    "# lora_name = \"Helium\"\n",
    "model_name_to_beat = \"MesozoicMetallurgist/nous-Aquitanian\"\n",
    "\n",
    "neft_noise = 0.0 # bad actually?\n",
    "\n",
    "# rank = 512\n",
    "# config = LoraConfig(\n",
    "#     r=rank, lora_alpha=rank*2,\n",
    "#     target_modules=[\n",
    "#                     'q_proj',\n",
    "#                     'v_proj', \n",
    "#                     \"k_proj\", \n",
    "#                     \"o_proj\", \n",
    "#                     # \"gate_proj\", \n",
    "#                     # \"up_proj\", \n",
    "#                     # \"down_proj\"\n",
    "#                     ],  #   , \n",
    "#     lora_dropout=0.0,\n",
    "#     bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "#     # use_rslora=True,\n",
    "#     use_dora=True,\n",
    "#     # init_lora_weights=\"gaussian\",\n",
    "# )\n",
    "\n",
    "params = {\n",
    "    'low_cpu_mem_usage': True,\n",
    "    'trust_remote_code': False,\n",
    "    'torch_dtype': torch.bfloat16,\n",
    "    'use_safetensors': True,\n",
    "    'attn_implementation': \"flash_attention_2\"\n",
    "}\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, **params, cache_dir=\"Models\")\n",
    "# model.config.bos_token_id = 2\n",
    "# model.config.eos_token_id = 1\n",
    "# model.generation_config.bos_token_id = 2\n",
    "# model.generation_config.eos_token_id = 1\n",
    "model = model.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-2-zephyr-1_6b\", trust_remote_code=False, use_fast=True, cache_dir=\"Models\")\n",
    "\n",
    "\n",
    "def norm_model_weights(model):\n",
    "    last_q = None\n",
    "    lqb = None\n",
    "    lqkm = None\n",
    "    last_v = None\n",
    "    lvb = None\n",
    "    lvom = None\n",
    "    last_up = None\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"q_proj\" in name:\n",
    "            if \"bias\" in name:\n",
    "                lqb = param\n",
    "            else:\n",
    "                last_q = param\n",
    "        if \"k_proj\" in name:\n",
    "            if \"bias\" in name:\n",
    "                param.data = param.data * lqkm\n",
    "            else:\n",
    "                # print(last_q.data.shape, param.data.shape)\n",
    "\n",
    "                # safe but not full solution\n",
    "                # last_q.data = last_q.data.to(torch.float64)\n",
    "                # param.data = param.data.to(torch.float64)\n",
    "                mult = torch.sqrt(torch.mean(torch.abs(last_q.data), dim=0, keepdim=True).transpose(0, 1) / \n",
    "                                torch.mean(torch.abs(param.data), dim=1, keepdim=True))\n",
    "                mult = torch.mean(mult)\n",
    "                last_q.data = last_q.data / mult\n",
    "                lqb.data = lqb.data / mult\n",
    "                param.data = param.data * mult\n",
    "                lqkm = mult\n",
    "                # last_q.data = last_q.data.to(torch.bfloat16)\n",
    "                # param.data = param.data.to(torch.bfloat16)\n",
    "                #\n",
    "\n",
    "                # THIS ONE STAYS\n",
    "                # last_q.data = last_q.data.to(torch.float64)\n",
    "                # param.data = param.data.to(torch.float64)\n",
    "                # mult = torch.sqrt(torch.mean(torch.abs(last_q.data), dim=1, keepdim=True) / \n",
    "                #                   torch.mean(torch.abs(param.data.repeat(8, 1)), dim=1, keepdim=True))\n",
    "                # # print(mult.shape, mult)\n",
    "                # last_q.data = last_q.data / mult\n",
    "                # param.data = param.data * ((mult[:256] + mult[256:512] + mult[512:768] + mult[768:1024] +\n",
    "                #                            mult[1024:1280] + mult[1280:1536] + mult[1536:1792] + mult[1792:2048]) / 8)\n",
    "                # last_q.data = last_q.data.to(torch.bfloat16)\n",
    "                # param.data = param.data.to(torch.bfloat16)\n",
    "\n",
    "        if \"v_proj\" in name:\n",
    "            if \"bias\" in name:\n",
    "                lvb = param\n",
    "            else:\n",
    "                last_v = param\n",
    "        if \"o_proj\" in name:\n",
    "            if \"bias\" in name:\n",
    "                param.data = param.data * lvom\n",
    "            else:\n",
    "                # print(last_v.data.shape, param.data.shape)\n",
    "\n",
    "                # safe but not full solution\n",
    "                # last_v.data = last_v.data.to(torch.float64)\n",
    "                # param.data = param.data.to(torch.float64)\n",
    "                mult = torch.sqrt(torch.mean(torch.abs(last_v.data), dim=0, keepdim=True) / \n",
    "                                torch.mean(torch.abs(param.data), dim=0, keepdim=True))\n",
    "                mult = torch.mean(mult) * -1\n",
    "                last_v.data = last_v.data / mult\n",
    "                lvb.data = lvb.data / mult\n",
    "                param.data = param.data * mult\n",
    "                lvom = mult\n",
    "                # last_v.data = last_v.data.to(torch.bfloat16)\n",
    "                # param.data = param.data.to(torch.bfloat16)\n",
    "                # \n",
    "\n",
    "                # mult = torch.sqrt(torch.mean(torch.abs(last_v.data.repeat(8, 1)), dim=0, keepdim=True) / \n",
    "                #                   torch.mean(torch.abs(param.data), dim=0, keepdim=True)) / 2\n",
    "                # print(mult.shape, mult)\n",
    "                # last_v.data = last_v.data / mult # ((mult[:256] + mult[256:512] + mult[512:768] + mult[768:1024] +\n",
    "                #                            #mult[1024:1280] + mult[1280:1536] + mult[1536:1792] + mult[1792:2048]) / 8)\n",
    "                # param.data = param.data * mult\n",
    "\n",
    "        if \"up_proj\" in name:\n",
    "            last_up = param\n",
    "        if \"down_proj\" in name:\n",
    "            # print(last_up.data.shape, param.data.shape)\n",
    "            # last_up.data = last_up.data.to(torch.float64)\n",
    "            # param.data = param.data.to(torch.float64)\n",
    "            mult = torch.sqrt(torch.mean(torch.abs(last_up.data), dim=1, keepdim=True).transpose(0, 1) / \n",
    "                            torch.mean(torch.abs(param.data), dim=0, keepdim=True))\n",
    "            # print(mult, mult.shape)\n",
    "            last_up.data = last_up.data / mult.transpose(0, 1)\n",
    "            param.data = param.data * mult\n",
    "            # last_up.data = last_up.data.to(torch.bfloat16)\n",
    "            # param.data = param.data.to(torch.bfloat16)\n",
    "    return model\n",
    "\n",
    "model = norm_model_weights(model)\n",
    "        \n",
    "\n",
    "# model.save_pretrained(\"Models/fixedscaling\")\n",
    "base_model = None\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name_to_beat, **params, cache_dir=\"Models\")\n",
    "for name, param in base_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "lora_model = model\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if (\n",
    "        \"embed_\" not in name\n",
    "          and \"up_proj\" not in name and \"down_proj\" not in name\n",
    "          and \"gate_proj\" not in name \n",
    "        # and \"q_proj\" not in name and \"k_proj\" not in name \n",
    "        # and \"v_proj\" not in name and \"o_proj\" not in name \n",
    "    ):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# lora_model = PeftModel.from_pretrained(model, model_id=\"Ypresian_\", is_trainable=True)\n",
    "# lora_model = lora_model.merge_and_unload(progressbar=True)\n",
    "# lora_model = get_peft_model(model, config)\n",
    "# lora_model.print_trainable_parameters()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"MesozoicMetallurgist/nous-Hauterivian\", trust_remote_code=False, use_fast=True, cache_dir=\"Models\")\n",
    "lora_model = lora_model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in lora_model.named_parameters():\n",
    "    if \"lora\" not in name:\n",
    "        print(name, param, param.data.shape)\n",
    "# for name, param in lora_model.named_parameters():\n",
    "#     if \"lora\" in name:\n",
    "#         print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, eval_data = get_data(train_name=['data/cortex_686_8704.json',\n",
    "# ],\n",
    "#                                  train_subset=np.arange(0,32),\n",
    "#                                  eval_subset=32,\n",
    "#                                  eval_name='data/cortex_686_8704.json', tokenizer=tokenizer, shuffle=False)\n",
    "# evaluate(lora_model, eval_data, return_to_cpu=True, print_stats=True, base_model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: no training data provided, enabling infinite training\n",
      "WARN: remerging is enabled, model_prev created and may take up extra memory\n",
      "WARN: no evaluation data provided, acquiring new data\n",
      "WARN: no training data provided, acquiring new data\n",
      "WARN: simple loss is enabled, this will disable base model processing\n",
      "Base loss processing is disabled, disabling ignore overshot samples\n",
      "Note: precalced eval base loss does not account for pretrained fine-tuning\n",
      "........Eval Base Loss: 0.933087\n",
      "........Step 512/1792\tLoss: 0.763006 OShL: 0.000e+00\tBase: 0.9331 Diff: -1.7008e-01 \tWR: 72.46% 0eps: 74.41%  \tLR: 3.13e-05 fit: 0/0\n",
      "........Step 1024/1280\tLoss: 0.779480 OShL: 0.000e+00\tBase: 0.9331 Diff: -1.5361e-01 \tWR: 75.20% 0eps: 76.17%  \tLR: 6.25e-05 fit: 0/0\n",
      "........Step 1536/1920\tLoss: 0.859848 OShL: 0.000e+00\tBase: 0.9331 Diff: -7.3240e-02 \tWR: 63.28% 0eps: 64.26%  \tLR: 9.38e-05 fit: 0/0\n",
      "........Step 2048/1408\tLoss: 0.910329 OShL: 0.000e+00\tBase: 0.9331 Diff: -2.2758e-02 \tWR: 59.57% 0eps: 61.33%  \tLR: 1.25e-04 fit: 0/0\n",
      "Evaluating ........ Loss: 0.00968488, Base Loss: 0.933087, Lora Diff: -0.00030613, WR: 1.76%, 0epsWR: 53.12%, OShL: 0.00000000\n",
      "Evaluating ........ Loss: 0.00968790, Base Loss: 0.933087, Lora Diff: -0.00027275, WR: 1.76%, 0epsWR: 52.73%, OShL: 0.00000089\n",
      "........Step 2560/2048\tLoss: 0.905489 OShL: 0.000e+00\tBase: 0.9331 Diff: -2.7598e-02 \tWR: 58.79% 0eps: 60.16%  \tLR: 1.17e-04 fit: 0/0\n",
      "........Step 3072/1536\tLoss: 0.882179 OShL: 0.000e+00\tBase: 0.9331 Diff: -5.0908e-02 \tWR: 56.64% 0eps: 57.03%  \tLR: 9.38e-05 fit: 0/0\n",
      "........Step 3584/2176\tLoss: 0.891813 OShL: 0.000e+00\tBase: 0.9331 Diff: -4.1275e-02 \tWR: 50.98% 0eps: 52.54%  \tLR: 6.25e-05 fit: 0/0\n",
      "........Step 4096/1664\tLoss: 0.932430 OShL: 0.000e+00\tBase: 0.9331 Diff: -6.5756e-04 \tWR: 55.08% 0eps: 57.81%  \tLR: 3.13e-05 fit: 0/0\n",
      "Evaluating ........ Loss: 0.00970328, Base Loss: 0.933087, Lora Diff: -0.00025749, WR: 1.27%, 0epsWR: 52.93%, OShL: 0.00000061\n",
      "Evaluating ........ Loss: 0.00983059, Base Loss: 0.933087, Lora Diff: -0.00016975, WR: 1.95%, 0epsWR: 51.27%, OShL: 0.00000214\n",
      "........Step 4608/1152\tLoss: 0.917467 OShL: 0.000e+00\tBase: 0.9331 Diff: -1.5621e-02 \tWR: 58.98% 0eps: 61.33%  \tLR: 8.37e-06 fit: 0/0\n",
      "........Step 5120/1792\tLoss: 1.000952 OShL: 0.000e+00\tBase: 0.9331 Diff: 6.7864e-02 \tWR: 41.41% 0eps: 42.19%  \tLR: 8.37e-06 fit: 0/0\n",
      "........Step 5632/1280\tLoss: 0.987280 OShL: 0.000e+00\tBase: 0.9331 Diff: 5.4193e-02 \tWR: 39.45% 0eps: 41.02%  \tLR: 3.12e-05 fit: 0/0\n",
      "........Step 6144/1920\tLoss: 0.936499 OShL: 0.000e+00\tBase: 0.9331 Diff: 3.4120e-03 \tWR: 50.98% 0eps: 52.15%  \tLR: 6.25e-05 fit: 0/0\n",
      "Evaluating ........ Loss: 0.00968288, Base Loss: 0.933087, Lora Diff: -0.00034618, WR: 1.76%, 0epsWR: 53.03%, OShL: 0.00000480\n",
      "Evaluating ........ Loss: 0.00968559, Base Loss: 0.933087, Lora Diff: -0.00028515, WR: 2.05%, 0epsWR: 52.93%, OShL: 0.00000061\n",
      "........Step 6656/1408\tLoss: 0.907795 OShL: 0.000e+00\tBase: 0.9331 Diff: -2.5292e-02 \tWR: 52.15% 0eps: 52.54%  \tLR: 9.38e-05 fit: 0/0\n",
      "........Step 7168/2048\tLoss: 0.870692 OShL: 0.000e+00\tBase: 0.9331 Diff: -6.2395e-02 \tWR: 56.45% 0eps: 58.40%  \tLR: 1.17e-04 fit: 0/0\n",
      "........Step 7680/1536\tLoss: 0.876373 OShL: 0.000e+00\tBase: 0.9331 Diff: -5.6714e-02 \tWR: 56.25% 0eps: 57.81%  \tLR: 1.25e-04 fit: 0/0\n",
      "........Step 8192/2176\tLoss: 0.894803 OShL: 0.000e+00\tBase: 0.9331 Diff: -3.8285e-02 \tWR: 54.30% 0eps: 55.66%  \tLR: 1.17e-04 fit: 0/0\n",
      "Evaluating ........ Loss: 0.00973165, Base Loss: 0.933087, Lora Diff: -0.00024891, WR: 1.86%, 0epsWR: 52.05%, OShL: 0.00000914\n",
      "Evaluating ........ Loss: 0.00982379, Base Loss: 0.933087, Lora Diff: -0.00015926, WR: 2.05%, 0epsWR: 51.07%, OShL: 0.00000176\n",
      "........Step 8704/1664\tLoss: 0.936885 OShL: 0.000e+00\tBase: 0.9331 Diff: 3.7975e-03 \tWR: 48.05% 0eps: 48.63%  \tLR: 9.37e-05 fit: 0/0\n",
      "........Step 9216/1152\tLoss: 0.957331 OShL: 0.000e+00\tBase: 0.9331 Diff: 2.4243e-02 \tWR: 45.12% 0eps: 46.68%  \tLR: 6.25e-05 fit: 0/0\n",
      "."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m eval_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minf_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43macc_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction_finetuing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecalculate_batch_mult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.25e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_end_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msam_rho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madaptive_sam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwsam_variant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshampoo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_eps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43movershoot_buffer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_below\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_auto_percent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremerging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremerge_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremerge_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimple_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_base_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecalc_eval_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmanual_grad_clip_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msam_grad_clip_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_overshoot_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_overshot_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbad_sample_mult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_n_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexcessive_cache_clearing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# gradient_checkpointing=True, excessive_cache_clearing=True)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 563\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, tokenizer, train_d, eval_d, base_model, inf_training, inf_data_steps, inf_data_decay, training_device, acc_batch_size, instruction_finetuing, precalculate_batch_mult, precalc_eval_base, lr, weight_decay, lr_scheduler, warmup_steps, warmup_end_offset, betas, use_sam, sam_rho, opt, adaptive_sam, wsam_variant, ignore_below, ignore_auto_percent, manual_grad_clip_norm, sam_grad_clip_norm, add_overshoot_penalty, ignore_overshot_samples, bad_sample_mult, remerging, remerge_eval, remerge_ratio, loss_eps, overshoot_buffer, true_eps, simple_loss, process_base_loss, relative_loss, eval_steps, save_name, do_save, average_stats, save_n_start, gradient_checkpointing, excessive_cache_clearing, base_model_switching)\u001b[0m\n\u001b[1;32m    560\u001b[0m loss, overshoot_penalty \u001b[38;5;241m=\u001b[39m combined_loss(outputs_loss, base_outputs_loss)\n\u001b[1;32m    561\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m acc_batch_size\n\u001b[0;32m--> 563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m ignore_overshot_samples \u001b[38;5;129;01mor\u001b[39;00m overshoot_penalty \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43moutputs_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m ignore_below:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m simple_loss \u001b[38;5;129;01mand\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m>\u001b[39m ((loss_eps \u001b[38;5;241m/\u001b[39m acc_batch_size)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-8\u001b[39m):\n\u001b[1;32m    565\u001b[0m         unfit_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "eval_data = []\n",
    "train(lora_model, tokenizer, train_data, eval_data, base_model=base_model, inf_training=False, training_device=\"cuda\",\n",
    "        acc_batch_size=512, instruction_finetuing=True, precalculate_batch_mult=2.25,\n",
    "        lr=3.2e-5, weight_decay=0.0, lr_scheduler=\"cosine\", warmup_steps=4, warmup_end_offset=8, betas=(0.8, 0.95),\n",
    "        use_sam=False, sam_rho=0.05, adaptive_sam=False, wsam_variant=True,\n",
    "        opt=\"adamw\",\n",
    "        loss_eps = 0.02, overshoot_buffer = -0.01, true_eps=0.01, ignore_below=0.0, ignore_auto_percent=1,\n",
    "        remerging=True, remerge_eval=True, remerge_ratio=0.3,\n",
    "        simple_loss=True, process_base_loss=True, precalc_eval_base=True, relative_loss=False,\n",
    "        manual_grad_clip_norm=1.0, sam_grad_clip_norm=None,\n",
    "        add_overshoot_penalty=False, ignore_overshot_samples=True, bad_sample_mult=1.0,\n",
    "        eval_steps=2048, do_save=False, save_name=lora_name, save_n_start=0,\n",
    "        average_stats=False,\n",
    "        gradient_checkpointing=False, excessive_cache_clearing=False)\n",
    "        # gradient_checkpointing=True, excessive_cache_clearing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda2aa5a1ffb4880a9beb2da9eb59cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/678 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lora_model.save_pretrained(lora_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ........ Loss: 0.00578544, Base Loss: 0.864540, Lora Diff: -0.00556602, WR: 33.86%, 0epsWR: 80.78%, OShL: 0.00163308\n"
     ]
    }
   ],
   "source": [
    "evaluate(lora_model, eval_data, return_to_cpu=True, print_stats=True, base_model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11173"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5392832bcf1445049428f4b4eee6d61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lora_model.save_pretrained(lora_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = lora_model.to(\"cuda\")\n",
    "lora_model = lora_model.merge_and_unload()\n",
    "# lora_model = norm_model_weights(lora_model)\n",
    "# simple_eval(lora_model, eval_data)\n",
    "lora_model = lora_model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.config.name_or_path = \"MesozoicMetallurgist/new_model\"\n",
    "model_dir = \"Models/merged_model\"\n",
    "# model_dir = os.path.expanduser(\"~/finetuning-subnet/merged_model\")\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "else:\n",
    "    # wipe the directory\n",
    "    for file in os.listdir(model_dir):\n",
    "        os.remove(os.path.join(model_dir, file))\n",
    "lora_model.save_pretrained(save_directory=model_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(save_directory=model_dir)\n",
    "lora_model = lora_model.to(\"cpu\")\n",
    "gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ........ Loss: 0.00798187, Base Loss: 0.557989, Lora Diff: -0.00123047, WR: 18.10%, 0epsWR: 66.80%, OShL: 0.00037500\n"
     ]
    }
   ],
   "source": [
    "evaluate(lora_model, eval_data, return_to_cpu=True, print_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_model = AutoModelForCausalLM.from_pretrained(model_name, **params, cache_dir=\"Models\")\n",
    "\n",
    "lora_model = lora_model.to(\"cuda\")\n",
    "# lora_model = lora_model.merge_and_unload()\n",
    "# model_dir = \"Models/merged_model\"\n",
    "# lora_model.save_pretrained(save_directory=model_dir, safe_serialization=True)\n",
    "# tokenizer.save_pretrained(save_directory=model_dir)\n",
    "\n",
    "last_q = None\n",
    "last_v = None\n",
    "last_up = None\n",
    "for name, param  in lora_model.named_parameters():\n",
    "    # if \"q_proj\" in name:\n",
    "    #     last_q = param\n",
    "    # elif \"k_proj\" in name:\n",
    "    #     # print(last_q.data.shape, param.data.shape)\n",
    "    #     # mult = (torch.mean(torch.abs(last_q.data), dim=0, keepdim=True) / torch.mean(torch.abs(param.data), dim=0)) ** 0.5\n",
    "    #     mult = (torch.mean(torch.abs(last_q.data)) / torch.mean(torch.abs(param.data))) ** 0.5\n",
    "    #     mult = 100.0\n",
    "    #     # print(mult.shape)\n",
    "    #     last_q.data = last_q.data / mult#.transpose(0, 1)\n",
    "    #     param.data = param.data * mult\n",
    "    #     last_q = None\n",
    "    #     # print(mult)\n",
    "    # if \"v_proj\" in name:\n",
    "    #     last_v = param\n",
    "    # elif \"o_proj\" in name:\n",
    "    #     # print(last_v.data.shape, param.data.shape)\n",
    "    #     # mult = (torch.mean(torch.abs(last_v.data), dim=0, keepdim=True) / torch.mean(torch.abs(param.data), dim=1)) ** 0.5\n",
    "    #     mult = (torch.mean(torch.abs(last_v.data)) / torch.mean(torch.abs(param.data))) ** 0.5\n",
    "    #     mult = 100.0\n",
    "    #     # print(mult.shape)\n",
    "    #     last_v.data = last_v.data / mult\n",
    "    #     param.data = param.data * mult#.transpose(0, 1)\n",
    "    #     last_v = None\n",
    "        # print(mult)\n",
    "    # get current random seed from torch\n",
    "    iseed = torch.initial_seed()\n",
    "    # set the random seed to a fixed value\n",
    "    torch.manual_seed(42)\n",
    "    if \"up_proj\" in name:\n",
    "        last_up = param\n",
    "    elif \"down_proj\" in name:\n",
    "        # print(last_up.data.shape, param.data.shape)\n",
    "        mult = (torch.mean(torch.abs(last_up.data), dim=1, keepdim=True).transpose(0, 1) / torch.mean(torch.abs(param.data), dim=0)) ** 0.5\n",
    "        mult = torch.randint_like(mult, 1, 128, dtype=torch.bfloat16)\n",
    "        # print(mult.shape)\n",
    "        last_up.data = last_up.data / mult.transpose(0, 1)\n",
    "        param.data = param.data * mult\n",
    "        last_up = None\n",
    "        # print(mult)\n",
    "    # set the random seed back to the original value\n",
    "    torch.manual_seed(iseed)\n",
    "\n",
    "model_dir = \"Models/merged_model_fuckkery\"\n",
    "lora_model.save_pretrained(save_directory=model_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(save_directory=model_dir)\n",
    "lora_model = lora_model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ........ Loss: 0.01010798, Base Loss: 0.707643, Lora Diff: 0.00000000, WR: 0.00%, 0epsWR: 50.00%, OShL: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "lora_model = get_peft_model(model, config)\n",
    "evaluate(lora_model, eval_data, return_to_cpu=True, print_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = lora_model.to(\"cuda\")\n",
    "lora_model = lora_model.merge_and_unload()\n",
    "lora_model.save_pretrained(lora_name+'fuckkery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
